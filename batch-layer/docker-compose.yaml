services:
    airflow:
        image: airflow:v0
        ports:
          - 8080:8080
        environment:
          - AIRFLOW__CORE__LOAD_EXAMPLES=False
        volumes:
          - ./airflow:/workspace/airflow
    
    namenode:
        image: loind31/spark-hdfs-base-img:v0
        hostname: namenode
        container_name: namenode
        command: >
            /bin/bash -c "
            if [ ! -d /workspace/hadoop/dfs/name/current ]; then
                echo 'Formatting namenode...';
                hdfs namenode -format -force;
            fi;

            echo 'Starting namenode...';
            hdfs namenode &

            echo 'Starting ResourceManager...';
            yarn --daemon start resourcemanager;

            echo 'Starting MapReduce HistoryServer...';
            mapred --daemon start historyserver;

            echo 'Waiting for HDFS to be ready...';
            until hdfs dfsadmin -report > /dev/null 2>&1; do
                echo 'HDFS not ready yet. Retrying in 3s...';
                sleep 3;
            done;

            echo 'HDFS is ready. Creating warehouse and spark-events directories...';
            hdfs dfs -mkdir -p /workspace/hive/data/warehouse || echo 'Warehouse already exists.';
            hdfs dfs -chmod 777 /workspace/hive/data/warehouse;

            mkdir -p /workspace/spark-events;
            echo 'spark.eventLog.enabled true' > /workspace/spark/conf/spark-defaults.conf;
            echo 'spark.eventLog.dir file:/workspace/spark-events' >> /workspace/spark/conf/spark-defaults.conf;
            echo 'spark.history.fs.logDirectory file:/workspace/spark-events' >> /workspace/spark/conf/spark-defaults.conf;

            echo 'Starting Spark History Server...';
            start-history-server.sh &

            echo 'Starting Jupyter Notebook...';
            nohup jupyter lab --ip=0.0.0.0 --no-browser --port=8888 > /workspace/jupyter.log 2>&1 &

            echo 'All services started. Keeping container alive...';
            tail -f /dev/null
            "

        ports:
            - 8088:8088
            - 19888:19888
            - 9870:9870
            - 8888:8888
            - 18080:18080
            - 15002:15002
        volumes:
            # - ./hive-hadoop/data/name-node:/workspace/hadoop/dfs/name
            - ./hdfs-spark/config/spark-defaults.conf:/workspace/spark/conf/spark-defaults.conf
            - ./hdfs-spark/config/common/hdfs-site.xml:/workspace/hadoop/etc/hadoop/hdfs-site.xml
            - ./hdfs-spark/config/common/core-site.xml:/workspace/hadoop/etc/hadoop/core-site.xml
            - ./hdfs-spark/config/common/mapred-site.xml:/workspace/hadoop/etc/hadoop/mapred-site.xml
            - ./hdfs-spark/config/common/yarn-site.xml:/workspace/hadoop/etc/hadoop/yarn-site.xml
            - script-src:/workspace/src
        depends_on:
            - datanode-nodemanager-1
            - datanode-nodemanager-2
    datanode-nodemanager-1:
        image: loind31/hdfs-base-img:v0
        hostname: datanode-nodemanager-1
        container_name: datanode-nodemanager-1
        command: ["/bin/bash", "-c", "hdfs --daemon start datanode; yarn --daemon start nodemanager; sleep 3; tail -f /dev/null"]
        volumes:
            - ./hdfs-spark/config/common/hdfs-site.xml:/workspace/hadoop/etc/hadoop/hdfs-site.xml
            - ./hdfs-spark/config/common/core-site.xml:/workspace/hadoop/etc/hadoop/core-site.xml
            - ./hdfs-spark/config/common/mapred-site.xml:/workspace/hadoop/etc/hadoop/mapred-site.xml
            - ./hdfs-spark/config/common/yarn-site.xml:/workspace/hadoop/etc/hadoop/yarn-site.xml
    datanode-nodemanager-2:
        image: loind31/hdfs-base-img:v0
        hostname: datanode-nodemanager-2
        container_name: datanode-nodemanager-2
        command: ["/bin/bash", "-c", "hdfs --daemon start datanode; yarn --daemon start nodemanager; sleep 3; tail -f /dev/null"]
        volumes:
            - ./hdfs-spark/config/common/hdfs-site.xml:/workspace/hadoop/etc/hadoop/hdfs-site.xml
            - ./hdfs-spark/config/common/core-site.xml:/workspace/hadoop/etc/hadoop/core-site.xml
            - ./hdfs-spark/config/common/mapred-site.xml:/workspace/hadoop/etc/hadoop/mapred-site.xml
            - ./hdfs-spark/config/common/yarn-site.xml:/workspace/hadoop/etc/hadoop/yarn-site.xml
    postgres:
        hostname: postgres
        image: postgres:16.11
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=test
        ports:
            - 5433:5432
        volumes:
            - postgres_data:/var/lib/postgresql/data
volumes:
    postgres_data:
    script-src: