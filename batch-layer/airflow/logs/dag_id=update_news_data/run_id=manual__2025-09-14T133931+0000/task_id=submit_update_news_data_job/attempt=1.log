{"timestamp":"2025-12-19T13:39:48.808830Z","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager","filename":"manager.py","lineno":179}
{"timestamp":"2025-12-19T13:39:48.811835Z","level":"info","event":"Filling up the DagBag from /workspace/airflow/dags/update_news_data.py","logger":"airflow.models.dagbag.DagBag","filename":"dagbag.py","lineno":593}
{"timestamp":"2025-12-19T13:39:46.540007Z","level":"info","event":"Spark-Submit cmd: spark-submit --master yarn --jars /workspace/airflow/connector/mssql-jdbc-12.2.0.jre11.jar --name arrow-spark --verbose --deploy-mode client /workspace/airflow/spark-jobs/update_news_data.py 2025-09-07 2025-09-14","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":474}
{"timestamp":"2025-12-19T13:40:03.643765Z","level":"info","event":"Using properties file: null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.818285Z","level":"info","event":"Parsed arguments:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.818435Z","level":"info","event":"master                  yarn","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.818482Z","level":"info","event":"remote                  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.818509Z","level":"info","event":"deployMode              client","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.818531Z","level":"info","event":"executorMemory          null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.818552Z","level":"info","event":"executorCores           null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.818572Z","level":"info","event":"totalExecutorCores      null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.818592Z","level":"info","event":"propertiesFile          null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.818612Z","level":"info","event":"driverMemory            null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.818633Z","level":"info","event":"driverCores             null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.818653Z","level":"info","event":"driverExtraClassPath    null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.818673Z","level":"info","event":"driverExtraLibraryPath  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.818692Z","level":"info","event":"driverExtraJavaOptions  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.818711Z","level":"info","event":"supervise               false","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.818730Z","level":"info","event":"queue                   null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.818756Z","level":"info","event":"numExecutors            null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.818777Z","level":"info","event":"files                   null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.818796Z","level":"info","event":"pyFiles                 null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.818866Z","level":"info","event":"archives                null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.818908Z","level":"info","event":"mainClass               null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.818945Z","level":"info","event":"primaryResource         file:/workspace/airflow/spark-jobs/update_news_data.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.818970Z","level":"info","event":"name                    arrow-spark","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.818992Z","level":"info","event":"childArgs               [2025-09-07 2025-09-14]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.819014Z","level":"info","event":"jars                    file:/workspace/airflow/connector/mssql-jdbc-12.2.0.jre11.jar","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.819035Z","level":"info","event":"packages                null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.819055Z","level":"info","event":"packagesExclusions      null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.819074Z","level":"info","event":"repositories            null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.819093Z","level":"info","event":"verbose                 true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.819118Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.819139Z","level":"info","event":"Spark properties used, including those specified through","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.819291Z","level":"info","event":"--conf and those from the properties file null:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.819352Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.819384Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:03.819419Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:04.747715Z","level":"info","event":"25/12/19 13:40:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:05.196038Z","level":"info","event":"Main class:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:05.196196Z","level":"info","event":"org.apache.spark.deploy.PythonRunner","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:05.196287Z","level":"info","event":"Arguments:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:05.196325Z","level":"info","event":"file:/workspace/airflow/spark-jobs/update_news_data.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:05.196353Z","level":"info","event":"null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:05.196392Z","level":"info","event":"2025-09-07","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:05.196444Z","level":"info","event":"2025-09-14","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:05.203838Z","level":"info","event":"Spark config:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:05.203992Z","level":"info","event":"(spark.app.name,arrow-spark)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:05.204038Z","level":"info","event":"(spark.app.submitTime,1766151605174)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:05.204066Z","level":"info","event":"(spark.master,yarn)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:05.204092Z","level":"info","event":"(spark.repl.local.jars,file:///workspace/airflow/connector/mssql-jdbc-12.2.0.jre11.jar)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:05.204118Z","level":"info","event":"(spark.submit.deployMode,client)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:05.204141Z","level":"info","event":"(spark.submit.pyFiles,)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:05.204205Z","level":"info","event":"(spark.yarn.dist.jars,file:///workspace/airflow/connector/mssql-jdbc-12.2.0.jre11.jar)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:05.204234Z","level":"info","event":"(spark.yarn.isPython,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:05.204266Z","level":"info","event":"Classpath elements:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:05.204291Z","level":"info","event":"file:///workspace/airflow/connector/mssql-jdbc-12.2.0.jre11.jar","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:05.204319Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:05.204344Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:07.594717Z","level":"info","event":"25/12/19 13:40:07 INFO SparkContext: Running Spark version 3.5.7","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:07.595288Z","level":"info","event":"25/12/19 13:40:07 INFO SparkContext: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:07.595797Z","level":"info","event":"25/12/19 13:40:07 INFO SparkContext: Java version 11.0.23","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:07.671255Z","level":"info","event":"25/12/19 13:40:07 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:07.671833Z","level":"info","event":"25/12/19 13:40:07 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:07.672449Z","level":"info","event":"25/12/19 13:40:07 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:07.673017Z","level":"info","event":"25/12/19 13:40:07 INFO SparkContext: Submitted application: Checking news data","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:07.731507Z","level":"info","event":"25/12/19 13:40:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:07.753414Z","level":"info","event":"25/12/19 13:40:07 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:07.757535Z","level":"info","event":"25/12/19 13:40:07 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:07.998878Z","level":"info","event":"25/12/19 13:40:07 INFO SecurityManager: Changing view acls to: root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:07.999780Z","level":"info","event":"25/12/19 13:40:07 INFO SecurityManager: Changing modify acls to: root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:08.001654Z","level":"info","event":"25/12/19 13:40:08 INFO SecurityManager: Changing view acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:08.003420Z","level":"info","event":"25/12/19 13:40:08 INFO SecurityManager: Changing modify acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:08.004802Z","level":"info","event":"25/12/19 13:40:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:08.867289Z","level":"info","event":"25/12/19 13:40:08 INFO Utils: Successfully started service 'sparkDriver' on port 38389.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:08.950101Z","level":"info","event":"25/12/19 13:40:08 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:09.052914Z","level":"info","event":"25/12/19 13:40:09 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:09.125718Z","level":"info","event":"25/12/19 13:40:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:09.126455Z","level":"info","event":"25/12/19 13:40:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:09.204957Z","level":"info","event":"25/12/19 13:40:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:09.269396Z","level":"info","event":"25/12/19 13:40:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-53d9e067-2c73-4d3e-b1f6-dad836c89e31","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:09.308795Z","level":"info","event":"25/12/19 13:40:09 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:09.409734Z","level":"info","event":"25/12/19 13:40:09 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:09.735162Z","level":"info","event":"25/12/19 13:40:09 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:09.898570Z","level":"info","event":"25/12/19 13:40:09 INFO Utils: Successfully started service 'SparkUI' on port 4040.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:10.368820Z","level":"info","event":"25/12/19 13:40:10 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at namenode/172.18.0.5:8032","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:11.944619Z","level":"info","event":"25/12/19 13:40:11 INFO Configuration: resource-types.xml not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:11.945413Z","level":"info","event":"25/12/19 13:40:11 INFO ResourceUtils: Unable to find 'resource-types.xml'.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:11.976187Z","level":"info","event":"25/12/19 13:40:11 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:11.976862Z","level":"info","event":"25/12/19 13:40:11 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:11.977218Z","level":"info","event":"25/12/19 13:40:11 INFO Client: Setting up container launch context for our AM","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:11.984630Z","level":"info","event":"25/12/19 13:40:11 INFO Client: Setting up the launch environment for our AM container","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:12.005528Z","level":"info","event":"25/12/19 13:40:12 INFO Client: Preparing resources for our AM container","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:12.067641Z","level":"info","event":"25/12/19 13:40:12 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:30.683072Z","level":"info","event":"25/12/19 13:40:30 INFO Client: Uploading resource file:/tmp/spark-157c2819-f7fd-4cf2-932a-2932b8fd775a/__spark_libs__14773309117665205122.zip -> hdfs://namenode:8020/user/root/.sparkStaging/application_1766137997195_0016/__spark_libs__14773309117665205122.zip","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:34.267800Z","level":"info","event":"25/12/19 13:40:34 INFO Client: Uploading resource file:/workspace/airflow/connector/mssql-jdbc-12.2.0.jre11.jar -> hdfs://namenode:8020/user/root/.sparkStaging/application_1766137997195_0016/mssql-jdbc-12.2.0.jre11.jar","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:34.345466Z","level":"info","event":"25/12/19 13:40:34 INFO Client: Uploading resource file:/workspace/airflow/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip -> hdfs://namenode:8020/user/root/.sparkStaging/application_1766137997195_0016/pyspark.zip","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:34.465018Z","level":"info","event":"25/12/19 13:40:34 INFO Client: Uploading resource file:/workspace/airflow/.venv/lib/python3.12/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://namenode:8020/user/root/.sparkStaging/application_1766137997195_0016/py4j-0.10.9.7-src.zip","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:34.931097Z","level":"info","event":"25/12/19 13:40:34 INFO Client: Uploading resource file:/tmp/spark-157c2819-f7fd-4cf2-932a-2932b8fd775a/__spark_conf__16202282722171611936.zip -> hdfs://namenode:8020/user/root/.sparkStaging/application_1766137997195_0016/__spark_conf__.zip","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:35.031595Z","level":"info","event":"25/12/19 13:40:35 INFO SecurityManager: Changing view acls to: root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:35.031866Z","level":"info","event":"25/12/19 13:40:35 INFO SecurityManager: Changing modify acls to: root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:35.031954Z","level":"info","event":"25/12/19 13:40:35 INFO SecurityManager: Changing view acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:35.032009Z","level":"info","event":"25/12/19 13:40:35 INFO SecurityManager: Changing modify acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:35.032061Z","level":"info","event":"25/12/19 13:40:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:35.108281Z","level":"info","event":"25/12/19 13:40:35 INFO Client: Submitting application application_1766137997195_0016 to ResourceManager","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:35.242311Z","level":"info","event":"25/12/19 13:40:35 INFO YarnClientImpl: Submitted application application_1766137997195_0016","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:36.252206Z","level":"info","event":"25/12/19 13:40:36 INFO Client: Application report for application_1766137997195_0016 (state: ACCEPTED)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:36.264280Z","level":"info","event":"25/12/19 13:40:36 INFO Client:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:36.264503Z","level":"info","event":"client token: N/A","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:36.264562Z","level":"info","event":"diagnostics: AM container is launched, waiting for AM container to Register with RM","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:36.264610Z","level":"info","event":"ApplicationMaster host: N/A","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:36.264648Z","level":"info","event":"ApplicationMaster RPC port: -1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:36.264689Z","level":"info","event":"queue: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:36.264740Z","level":"info","event":"start time: 1766151635159","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:36.264787Z","level":"info","event":"final status: UNDEFINED","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:36.264847Z","level":"info","event":"tracking URL: http://namenode:8088/proxy/application_1766137997195_0016/","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:36.264892Z","level":"info","event":"user: root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.276584Z","level":"info","event":"25/12/19 13:40:42 INFO Client: Application report for application_1766137997195_0016 (state: RUNNING)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.277091Z","level":"info","event":"25/12/19 13:40:42 INFO Client:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.277235Z","level":"info","event":"client token: N/A","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.277278Z","level":"info","event":"diagnostics: N/A","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.277304Z","level":"info","event":"ApplicationMaster host: 172.18.0.3","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.277329Z","level":"info","event":"ApplicationMaster RPC port: -1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.277354Z","level":"info","event":"queue: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.277377Z","level":"info","event":"start time: 1766151635159","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.277400Z","level":"info","event":"final status: UNDEFINED","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.277423Z","level":"info","event":"tracking URL: http://namenode:8088/proxy/application_1766137997195_0016/","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.277450Z","level":"info","event":"user: root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.280319Z","level":"info","event":"25/12/19 13:40:42 INFO YarnClientSchedulerBackend: Application application_1766137997195_0016 has started running.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.320683Z","level":"info","event":"25/12/19 13:40:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34603.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.321540Z","level":"info","event":"25/12/19 13:40:42 INFO NettyBlockTransferService: Server created on 12d706d757b9:34603","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.324919Z","level":"info","event":"25/12/19 13:40:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.356308Z","level":"info","event":"25/12/19 13:40:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 12d706d757b9, 34603, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.369977Z","level":"info","event":"25/12/19 13:40:42 INFO BlockManagerMasterEndpoint: Registering block manager 12d706d757b9:34603 with 434.4 MiB RAM, BlockManagerId(driver, 12d706d757b9, 34603, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.388320Z","level":"info","event":"25/12/19 13:40:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 12d706d757b9, 34603, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.392166Z","level":"info","event":"25/12/19 13:40:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 12d706d757b9, 34603, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.573042Z","level":"info","event":"25/12/19 13:40:42 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> namenode, PROXY_URI_BASES -> http://namenode:8088/proxy/application_1766137997195_0016), /proxy/application_1766137997195_0016","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.937963Z","level":"info","event":"25/12/19 13:40:42 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.958144Z","level":"info","event":"25/12/19 13:40:42 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.960674Z","level":"info","event":"25/12/19 13:40:42 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.966480Z","level":"info","event":"25/12/19 13:40:42 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.971197Z","level":"info","event":"25/12/19 13:40:42 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.973949Z","level":"info","event":"25/12/19 13:40:42 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.976674Z","level":"info","event":"25/12/19 13:40:42 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.980056Z","level":"info","event":"25/12/19 13:40:42 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.986876Z","level":"info","event":"25/12/19 13:40:42 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.990074Z","level":"info","event":"25/12/19 13:40:42 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.992512Z","level":"info","event":"25/12/19 13:40:42 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:42.995247Z","level":"info","event":"25/12/19 13:40:42 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:43.001664Z","level":"info","event":"25/12/19 13:40:43 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:43.004445Z","level":"info","event":"25/12/19 13:40:43 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:43.008115Z","level":"info","event":"25/12/19 13:40:43 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:43.015860Z","level":"info","event":"25/12/19 13:40:43 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:43.018501Z","level":"info","event":"25/12/19 13:40:43 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:43.022930Z","level":"info","event":"25/12/19 13:40:43 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:43.030496Z","level":"info","event":"25/12/19 13:40:43 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:43.033954Z","level":"info","event":"25/12/19 13:40:43 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:43.036828Z","level":"info","event":"25/12/19 13:40:43 INFO ServerInfo: Adding filter to /executors/heapHistogram: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:43.040318Z","level":"info","event":"25/12/19 13:40:43 INFO ServerInfo: Adding filter to /executors/heapHistogram/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:43.043832Z","level":"info","event":"25/12/19 13:40:43 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:43.097515Z","level":"info","event":"25/12/19 13:40:43 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:43.099869Z","level":"info","event":"25/12/19 13:40:43 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:43.112460Z","level":"info","event":"25/12/19 13:40:43 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:43.116372Z","level":"info","event":"25/12/19 13:40:43 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:43.137761Z","level":"info","event":"25/12/19 13:40:43 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:43.137979Z","level":"info","event":"25/12/19 13:40:43 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:43.445352Z","level":"info","event":"25/12/19 13:40:43 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:44.048490Z","level":"info","event":"25/12/19 13:40:44 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:44.052535Z","level":"info","event":"25/12/19 13:40:44 INFO SharedState: Warehouse path is 'file:/workspace/airflow/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:44.127880Z","level":"info","event":"25/12/19 13:40:44 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:44.132171Z","level":"info","event":"25/12/19 13:40:44 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:44.135185Z","level":"info","event":"25/12/19 13:40:44 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:44.144008Z","level":"info","event":"25/12/19 13:40:44 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:44.150606Z","level":"info","event":"25/12/19 13:40:44 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:49.436800Z","level":"info","event":"25/12/19 13:40:49 INFO InMemoryFileIndex: It took 900 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:51.340434Z","level":"info","event":"25/12/19 13:40:51 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:38246) with ID 1,  ResourceProfileId 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:51.569654Z","level":"info","event":"25/12/19 13:40:51 INFO BlockManagerMasterEndpoint: Registering block manager datanode-nodemanager-2:45875 with 434.4 MiB RAM, BlockManagerId(1, datanode-nodemanager-2, 45875, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:51.974145Z","level":"info","event":"25/12/19 13:40:51 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:52.039905Z","level":"info","event":"25/12/19 13:40:52 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:52.042162Z","level":"info","event":"25/12/19 13:40:52 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:52.045311Z","level":"info","event":"25/12/19 13:40:52 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:52.052906Z","level":"info","event":"25/12/19 13:40:52 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:52.068177Z","level":"info","event":"25/12/19 13:40:52 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:52.355510Z","level":"info","event":"25/12/19 13:40:52 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.9 KiB, free 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:52.585410Z","level":"info","event":"25/12/19 13:40:52 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 38.1 KiB, free 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:52.592167Z","level":"info","event":"25/12/19 13:40:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 12d706d757b9:34603 (size: 38.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:52.606030Z","level":"info","event":"25/12/19 13:40:52 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:52.686778Z","level":"info","event":"25/12/19 13:40:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:52.690895Z","level":"info","event":"25/12/19 13:40:52 INFO YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:52.810133Z","level":"info","event":"25/12/19 13:40:52 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (datanode-nodemanager-2, executor 1, partition 0, PROCESS_LOCAL, 9195 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:40:53.515037Z","level":"info","event":"25/12/19 13:40:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on datanode-nodemanager-2:45875 (size: 38.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:01.298949Z","level":"info","event":"25/12/19 13:41:01 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 8539 ms on datanode-nodemanager-2 (executor 1) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:01.307472Z","level":"info","event":"25/12/19 13:41:01 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:01.309284Z","level":"info","event":"25/12/19 13:41:01 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:50404) with ID 2,  ResourceProfileId 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:01.332790Z","level":"info","event":"25/12/19 13:41:01 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 9.198 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:01.339810Z","level":"info","event":"25/12/19 13:41:01 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:01.340803Z","level":"info","event":"25/12/19 13:41:01 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:01.363559Z","level":"info","event":"25/12/19 13:41:01 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 6.077763 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:01.482998Z","level":"info","event":"25/12/19 13:41:01 INFO BlockManagerMasterEndpoint: Registering block manager datanode-nodemanager-1:34777 with 434.4 MiB RAM, BlockManagerId(2, datanode-nodemanager-1, 34777, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:02.546866Z","level":"info","event":"25/12/19 13:41:02 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 12d706d757b9:34603 in memory (size: 38.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:02.598881Z","level":"info","event":"25/12/19 13:41:02 INFO BlockManagerInfo: Removed broadcast_0_piece0 on datanode-nodemanager-2:45875 in memory (size: 38.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:07.499177Z","level":"info","event":"25/12/19 13:41:07 INFO DataSourceStrategy: Pruning directories with: isnotnull(ingest_date#21),(ingest_date#21 >= 2025-09-01),(ingest_date#21 < 2025-09-08)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:07.512339Z","level":"info","event":"25/12/19 13:41:07 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:07.515604Z","level":"info","event":"25/12/19 13:41:07 INFO FileSourceStrategy: Post-Scan Filters: (size(from_json(ArrayType(StructType(StructField(topic,StringType,true)),true), topics_json#19, Some(GMT)).topic, true) > 0),isnotnull(from_json(ArrayType(StructType(StructField(topic,StringType,true)),true), topics_json#19, Some(GMT)).topic)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:09.487888Z","level":"info","event":"25/12/19 13:41:09 INFO CodeGenerator: Code generated in 1083.207516 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:09.571057Z","level":"info","event":"25/12/19 13:41:09 INFO CodeGenerator: Code generated in 49.201349 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:09.659646Z","level":"info","event":"25/12/19 13:41:09 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 202.6 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:09.689137Z","level":"info","event":"25/12/19 13:41:09 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:09.692697Z","level":"info","event":"25/12/19 13:41:09 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 12d706d757b9:34603 (size: 35.9 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:09.704321Z","level":"info","event":"25/12/19 13:41:09 INFO SparkContext: Created broadcast 1 from showString at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:09.770355Z","level":"info","event":"25/12/19 13:41:09 INFO InMemoryFileIndex: Selected 7 partitions out of 16, pruned 56.25% partitions.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:09.805806Z","level":"info","event":"25/12/19 13:41:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:10.224323Z","level":"info","event":"25/12/19 13:41:10 INFO DAGScheduler: Registering RDD 8 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:10.235795Z","level":"info","event":"25/12/19 13:41:10 INFO DAGScheduler: Got map stage job 1 (showString at NativeMethodAccessorImpl.java:0) with 14 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:10.236985Z","level":"info","event":"25/12/19 13:41:10 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (showString at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:10.237865Z","level":"info","event":"25/12/19 13:41:10 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:10.242271Z","level":"info","event":"25/12/19 13:41:10 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:10.247806Z","level":"info","event":"25/12/19 13:41:10 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:10.311021Z","level":"info","event":"25/12/19 13:41:10 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 46.5 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:10.342580Z","level":"info","event":"25/12/19 13:41:10 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.3 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:10.343638Z","level":"info","event":"25/12/19 13:41:10 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 12d706d757b9:34603 (size: 20.3 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:10.345942Z","level":"info","event":"25/12/19 13:41:10 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:10.349997Z","level":"info","event":"25/12/19 13:41:10 INFO DAGScheduler: Submitting 14 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:10.352314Z","level":"info","event":"25/12/19 13:41:10 INFO YarnScheduler: Adding task set 1.0 with 14 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:10.382484Z","level":"info","event":"25/12/19 13:41:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (datanode-nodemanager-1, executor 2, partition 0, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:10.383515Z","level":"info","event":"25/12/19 13:41:10 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (datanode-nodemanager-2, executor 1, partition 1, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:10.500099Z","level":"info","event":"25/12/19 13:41:10 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on datanode-nodemanager-2:45875 (size: 20.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:11.095253Z","level":"info","event":"25/12/19 13:41:11 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on datanode-nodemanager-1:34777 (size: 20.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:13.806392Z","level":"info","event":"25/12/19 13:41:13 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on datanode-nodemanager-2:45875 (size: 35.9 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:15.509040Z","level":"info","event":"25/12/19 13:41:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on datanode-nodemanager-1:34777 (size: 35.9 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:17.100400Z","level":"info","event":"25/12/19 13:41:17 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (datanode-nodemanager-2, executor 1, partition 2, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:17.107721Z","level":"info","event":"25/12/19 13:41:17 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 6725 ms on datanode-nodemanager-2 (executor 1) (1/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:18.457471Z","level":"info","event":"25/12/19 13:41:18 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (datanode-nodemanager-2, executor 1, partition 3, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:18.460459Z","level":"info","event":"25/12/19 13:41:18 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 1363 ms on datanode-nodemanager-2 (executor 1) (2/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:19.528152Z","level":"info","event":"25/12/19 13:41:19 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (datanode-nodemanager-2, executor 1, partition 4, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:19.538196Z","level":"info","event":"25/12/19 13:41:19 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 1075 ms on datanode-nodemanager-2 (executor 1) (3/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:20.714634Z","level":"info","event":"25/12/19 13:41:20 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (datanode-nodemanager-2, executor 1, partition 5, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:20.716063Z","level":"info","event":"25/12/19 13:41:20 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 1188 ms on datanode-nodemanager-2 (executor 1) (4/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:21.977587Z","level":"info","event":"25/12/19 13:41:21 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (datanode-nodemanager-2, executor 1, partition 6, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:21.977830Z","level":"info","event":"25/12/19 13:41:21 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 1265 ms on datanode-nodemanager-2 (executor 1) (5/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:22.096549Z","level":"info","event":"25/12/19 13:41:22 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (datanode-nodemanager-1, executor 2, partition 7, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:22.099292Z","level":"info","event":"25/12/19 13:41:22 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 11725 ms on datanode-nodemanager-1 (executor 2) (6/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:22.940548Z","level":"info","event":"25/12/19 13:41:22 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9) (datanode-nodemanager-2, executor 1, partition 8, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:22.945505Z","level":"info","event":"25/12/19 13:41:22 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 967 ms on datanode-nodemanager-2 (executor 1) (7/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:23.338584Z","level":"info","event":"25/12/19 13:41:23 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 10) (datanode-nodemanager-1, executor 2, partition 9, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:23.341277Z","level":"info","event":"25/12/19 13:41:23 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 1246 ms on datanode-nodemanager-1 (executor 2) (8/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:23.899888Z","level":"info","event":"25/12/19 13:41:23 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 11) (datanode-nodemanager-2, executor 1, partition 10, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:23.900110Z","level":"info","event":"25/12/19 13:41:23 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 954 ms on datanode-nodemanager-2 (executor 1) (9/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:24.338465Z","level":"info","event":"25/12/19 13:41:24 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 12) (datanode-nodemanager-1, executor 2, partition 11, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:24.341250Z","level":"info","event":"25/12/19 13:41:24 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 10) in 1003 ms on datanode-nodemanager-1 (executor 2) (10/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:24.672706Z","level":"info","event":"25/12/19 13:41:24 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 13) (datanode-nodemanager-2, executor 1, partition 12, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:24.674358Z","level":"info","event":"25/12/19 13:41:24 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 11) in 781 ms on datanode-nodemanager-2 (executor 1) (11/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:25.345547Z","level":"info","event":"25/12/19 13:41:25 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 14) (datanode-nodemanager-1, executor 2, partition 13, NODE_LOCAL, 13862 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:25.346660Z","level":"info","event":"25/12/19 13:41:25 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 12) in 1009 ms on datanode-nodemanager-1 (executor 2) (12/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:25.459009Z","level":"info","event":"25/12/19 13:41:25 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 13) in 786 ms on datanode-nodemanager-2 (executor 1) (13/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:26.045842Z","level":"info","event":"25/12/19 13:41:26 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 14) in 702 ms on datanode-nodemanager-1 (executor 2) (14/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:26.047374Z","level":"info","event":"25/12/19 13:41:26 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:26.051120Z","level":"info","event":"25/12/19 13:41:26 INFO DAGScheduler: ShuffleMapStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 15.774 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:26.052677Z","level":"info","event":"25/12/19 13:41:26 INFO DAGScheduler: looking for newly runnable stages","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:26.053193Z","level":"info","event":"25/12/19 13:41:26 INFO DAGScheduler: running: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:26.053389Z","level":"info","event":"25/12/19 13:41:26 INFO DAGScheduler: waiting: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:26.053544Z","level":"info","event":"25/12/19 13:41:26 INFO DAGScheduler: failed: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:26.140363Z","level":"info","event":"25/12/19 13:41:26 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:26.298887Z","level":"info","event":"25/12/19 13:41:26 INFO CodeGenerator: Code generated in 92.023484 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:26.402269Z","level":"info","event":"25/12/19 13:41:26 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:26.405910Z","level":"info","event":"25/12/19 13:41:26 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:26.406190Z","level":"info","event":"25/12/19 13:41:26 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:26.406305Z","level":"info","event":"25/12/19 13:41:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:26.407898Z","level":"info","event":"25/12/19 13:41:26 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:26.409885Z","level":"info","event":"25/12/19 13:41:26 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:26.434510Z","level":"info","event":"25/12/19 13:41:26 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 48.9 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:26.447041Z","level":"info","event":"25/12/19 13:41:26 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 21.7 KiB, free 434.0 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:26.448981Z","level":"info","event":"25/12/19 13:41:26 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 12d706d757b9:34603 (size: 21.7 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:26.453568Z","level":"info","event":"25/12/19 13:41:26 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:26.456514Z","level":"info","event":"25/12/19 13:41:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:26.456723Z","level":"info","event":"25/12/19 13:41:26 INFO YarnScheduler: Adding task set 3.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:26.462231Z","level":"info","event":"25/12/19 13:41:26 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 15) (datanode-nodemanager-2, executor 1, partition 0, NODE_LOCAL, 9010 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:26.462418Z","level":"info","event":"25/12/19 13:41:26 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 12d706d757b9:34603 in memory (size: 20.3 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:26.469407Z","level":"info","event":"25/12/19 13:41:26 INFO BlockManagerInfo: Removed broadcast_2_piece0 on datanode-nodemanager-2:45875 in memory (size: 20.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:26.469691Z","level":"info","event":"25/12/19 13:41:26 INFO BlockManagerInfo: Removed broadcast_2_piece0 on datanode-nodemanager-1:34777 in memory (size: 20.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:26.530949Z","level":"info","event":"25/12/19 13:41:26 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on datanode-nodemanager-2:45875 (size: 21.7 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:26.661715Z","level":"info","event":"25/12/19 13:41:26 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.3:38246","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:27.104761Z","level":"info","event":"25/12/19 13:41:27 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 15) in 646 ms on datanode-nodemanager-2 (executor 1) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:27.110370Z","level":"info","event":"25/12/19 13:41:27 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.678 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:27.110740Z","level":"info","event":"25/12/19 13:41:27 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:27.110904Z","level":"info","event":"25/12/19 13:41:27 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:27.117045Z","level":"info","event":"25/12/19 13:41:27 INFO YarnScheduler: Killing all running tasks in stage 3: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:27.129131Z","level":"info","event":"25/12/19 13:41:27 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.726140 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:27.950076Z","level":"info","event":"25/12/19 13:41:27 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 12d706d757b9:34603 in memory (size: 21.7 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:27.953814Z","level":"info","event":"25/12/19 13:41:27 INFO BlockManagerInfo: Removed broadcast_3_piece0 on datanode-nodemanager-2:45875 in memory (size: 21.7 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:29.516302Z","level":"info","event":"25/12/19 13:41:29 INFO CodeGenerator: Code generated in 13.856422 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:29.541764Z","level":"info","event":"+--------------------+--------------------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:29.541968Z","level":"info","event":"|            topic_id|          topic_name|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:29.542030Z","level":"info","event":"+--------------------+--------------------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:29.542094Z","level":"info","event":"| 7262183755669310342|                 IPO|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:29.542142Z","level":"info","event":"| 2398001789366731595|    Economy - Fiscal|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:29.542184Z","level":"info","event":"|-8706132297326582471|     Economy - Macro|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:29.542225Z","level":"info","event":"|-1496748679391501014|             Finance|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:29.542265Z","level":"info","event":"| -674034787253727345|            Earnings|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:29.542306Z","level":"info","event":"| 6330205094660838234|Real Estate & Con...|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:29.542341Z","level":"info","event":"| 2354747098425553887|  Retail & Wholesale|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:29.542379Z","level":"info","event":"| 2704355848778060929|Energy & Transpor...|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:29.542412Z","level":"info","event":"|  224347739280556027|          Technology|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:29.542449Z","level":"info","event":"| -448291436371370118|   Financial Markets|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:29.542490Z","level":"info","event":"| 5998011884746003305|  Economy - Monetary|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:29.542525Z","level":"info","event":"| -900445728666717830|Mergers & Acquisi...|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:29.542570Z","level":"info","event":"| 8225716003774577002|       Manufacturing|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:29.542608Z","level":"info","event":"| 8636157333299584077|       Life Sciences|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:29.542643Z","level":"info","event":"| 6135255710989356306|          Blockchain|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:29.542677Z","level":"info","event":"+--------------------+--------------------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:29.542716Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.309396Z","level":"info","event":"25/12/19 13:41:34 INFO DataSourceStrategy: Pruning directories with: isnotnull(ingest_date#21),(ingest_date#21 >= 2025-09-01),(ingest_date#21 < 2025-09-08)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.310849Z","level":"info","event":"25/12/19 13:41:34 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.311849Z","level":"info","event":"25/12/19 13:41:34 INFO FileSourceStrategy: Post-Scan Filters: (size(from_json(ArrayType(StructType(StructField(topic,StringType,true)),true), topics_json#19, Some(GMT)).topic, true) > 0),isnotnull(from_json(ArrayType(StructType(StructField(topic,StringType,true)),true), topics_json#19, Some(GMT)).topic)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.423655Z","level":"info","event":"25/12/19 13:41:34 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 202.6 KiB, free 434.0 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.461520Z","level":"info","event":"25/12/19 13:41:34 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.464840Z","level":"info","event":"25/12/19 13:41:34 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 12d706d757b9:34603 (size: 35.9 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.466605Z","level":"info","event":"25/12/19 13:41:34 INFO SparkContext: Created broadcast 4 from jdbc at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.471108Z","level":"info","event":"25/12/19 13:41:34 INFO InMemoryFileIndex: Selected 7 partitions out of 16, pruned 56.25% partitions.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.473850Z","level":"info","event":"25/12/19 13:41:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.536837Z","level":"info","event":"25/12/19 13:41:34 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 12d706d757b9:34603 in memory (size: 35.9 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.541828Z","level":"info","event":"25/12/19 13:41:34 INFO BlockManagerInfo: Removed broadcast_1_piece0 on datanode-nodemanager-2:45875 in memory (size: 35.9 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.542030Z","level":"info","event":"25/12/19 13:41:34 INFO DAGScheduler: Registering RDD 18 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.542316Z","level":"info","event":"25/12/19 13:41:34 INFO DAGScheduler: Got map stage job 3 (jdbc at NativeMethodAccessorImpl.java:0) with 14 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.542449Z","level":"info","event":"25/12/19 13:41:34 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (jdbc at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.542509Z","level":"info","event":"25/12/19 13:41:34 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.542780Z","level":"info","event":"25/12/19 13:41:34 INFO BlockManagerInfo: Removed broadcast_1_piece0 on datanode-nodemanager-1:34777 in memory (size: 35.9 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.558019Z","level":"info","event":"25/12/19 13:41:34 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.564675Z","level":"info","event":"25/12/19 13:41:34 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[18] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.585158Z","level":"info","event":"25/12/19 13:41:34 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 46.5 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.622345Z","level":"info","event":"25/12/19 13:41:34 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 20.3 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.623064Z","level":"info","event":"25/12/19 13:41:34 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 12d706d757b9:34603 (size: 20.3 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.624481Z","level":"info","event":"25/12/19 13:41:34 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.626062Z","level":"info","event":"25/12/19 13:41:34 INFO DAGScheduler: Submitting 14 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[18] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.629217Z","level":"info","event":"25/12/19 13:41:34 INFO YarnScheduler: Adding task set 4.0 with 14 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.636636Z","level":"info","event":"25/12/19 13:41:34 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 16) (datanode-nodemanager-1, executor 2, partition 0, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.642871Z","level":"info","event":"25/12/19 13:41:34 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 17) (datanode-nodemanager-2, executor 1, partition 1, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.692412Z","level":"info","event":"25/12/19 13:41:34 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on datanode-nodemanager-2:45875 (size: 20.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.709348Z","level":"info","event":"25/12/19 13:41:34 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on datanode-nodemanager-1:34777 (size: 20.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.814224Z","level":"info","event":"25/12/19 13:41:34 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on datanode-nodemanager-2:45875 (size: 35.9 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:34.873967Z","level":"info","event":"25/12/19 13:41:34 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on datanode-nodemanager-1:34777 (size: 35.9 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:35.662884Z","level":"info","event":"25/12/19 13:41:35 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 18) (datanode-nodemanager-2, executor 1, partition 2, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:35.665490Z","level":"info","event":"25/12/19 13:41:35 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 17) in 1027 ms on datanode-nodemanager-2 (executor 1) (1/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:35.811247Z","level":"info","event":"25/12/19 13:41:35 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 19) (datanode-nodemanager-1, executor 2, partition 3, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:35.812902Z","level":"info","event":"25/12/19 13:41:35 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 16) in 1178 ms on datanode-nodemanager-1 (executor 2) (2/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:36.356160Z","level":"info","event":"25/12/19 13:41:36 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 20) (datanode-nodemanager-2, executor 1, partition 4, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:36.358918Z","level":"info","event":"25/12/19 13:41:36 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 18) in 696 ms on datanode-nodemanager-2 (executor 1) (3/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:36.782277Z","level":"info","event":"25/12/19 13:41:36 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 21) (datanode-nodemanager-1, executor 2, partition 5, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:36.784266Z","level":"info","event":"25/12/19 13:41:36 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 19) in 974 ms on datanode-nodemanager-1 (executor 2) (4/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:37.204031Z","level":"info","event":"25/12/19 13:41:37 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 22) (datanode-nodemanager-2, executor 1, partition 6, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:37.207394Z","level":"info","event":"25/12/19 13:41:37 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 20) in 855 ms on datanode-nodemanager-2 (executor 1) (5/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:37.747069Z","level":"info","event":"25/12/19 13:41:37 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 23) (datanode-nodemanager-1, executor 2, partition 7, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:37.750144Z","level":"info","event":"25/12/19 13:41:37 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 21) in 969 ms on datanode-nodemanager-1 (executor 2) (6/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:37.908001Z","level":"info","event":"25/12/19 13:41:37 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 24) (datanode-nodemanager-2, executor 1, partition 8, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:37.910298Z","level":"info","event":"25/12/19 13:41:37 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 22) in 706 ms on datanode-nodemanager-2 (executor 1) (7/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:38.483786Z","level":"info","event":"25/12/19 13:41:38 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 25) (datanode-nodemanager-2, executor 1, partition 9, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:38.486618Z","level":"info","event":"25/12/19 13:41:38 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 24) in 578 ms on datanode-nodemanager-2 (executor 1) (8/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:38.523453Z","level":"info","event":"25/12/19 13:41:38 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 26) (datanode-nodemanager-1, executor 2, partition 10, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:38.525844Z","level":"info","event":"25/12/19 13:41:38 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 23) in 780 ms on datanode-nodemanager-1 (executor 2) (9/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:39.248259Z","level":"info","event":"25/12/19 13:41:39 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 27) (datanode-nodemanager-2, executor 1, partition 11, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:39.249964Z","level":"info","event":"25/12/19 13:41:39 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 25) in 767 ms on datanode-nodemanager-2 (executor 1) (10/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:39.416968Z","level":"info","event":"25/12/19 13:41:39 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 28) (datanode-nodemanager-1, executor 2, partition 12, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:39.417255Z","level":"info","event":"25/12/19 13:41:39 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 26) in 893 ms on datanode-nodemanager-1 (executor 2) (11/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:39.836521Z","level":"info","event":"25/12/19 13:41:39 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 29) (datanode-nodemanager-2, executor 1, partition 13, NODE_LOCAL, 13862 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:39.838713Z","level":"info","event":"25/12/19 13:41:39 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 27) in 591 ms on datanode-nodemanager-2 (executor 1) (12/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:40.080420Z","level":"info","event":"25/12/19 13:41:40 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 28) in 667 ms on datanode-nodemanager-1 (executor 2) (13/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:40.280878Z","level":"info","event":"25/12/19 13:41:40 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 29) in 445 ms on datanode-nodemanager-2 (executor 1) (14/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:40.281175Z","level":"info","event":"25/12/19 13:41:40 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:40.283432Z","level":"info","event":"25/12/19 13:41:40 INFO DAGScheduler: ShuffleMapStage 4 (jdbc at NativeMethodAccessorImpl.java:0) finished in 5.704 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:40.283620Z","level":"info","event":"25/12/19 13:41:40 INFO DAGScheduler: looking for newly runnable stages","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:40.283678Z","level":"info","event":"25/12/19 13:41:40 INFO DAGScheduler: running: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:40.283720Z","level":"info","event":"25/12/19 13:41:40 INFO DAGScheduler: waiting: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:40.283766Z","level":"info","event":"25/12/19 13:41:40 INFO DAGScheduler: failed: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:40.289700Z","level":"info","event":"25/12/19 13:41:40 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:40.379811Z","level":"info","event":"25/12/19 13:41:40 INFO CodeGenerator: Code generated in 66.773504 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:40.546737Z","level":"info","event":"25/12/19 13:41:40 INFO SparkContext: Starting job: jdbc at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:40.549125Z","level":"info","event":"25/12/19 13:41:40 INFO DAGScheduler: Got job 4 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:40.549509Z","level":"info","event":"25/12/19 13:41:40 INFO DAGScheduler: Final stage: ResultStage 6 (jdbc at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:40.549680Z","level":"info","event":"25/12/19 13:41:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:40.555821Z","level":"info","event":"25/12/19 13:41:40 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:40.557652Z","level":"info","event":"25/12/19 13:41:40 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[23] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:40.586397Z","level":"info","event":"25/12/19 13:41:40 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 56.5 KiB, free 434.0 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:40.603739Z","level":"info","event":"25/12/19 13:41:40 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 25.0 KiB, free 434.0 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:40.609730Z","level":"info","event":"25/12/19 13:41:40 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 12d706d757b9:34603 (size: 25.0 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:40.615359Z","level":"info","event":"25/12/19 13:41:40 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:40.617411Z","level":"info","event":"25/12/19 13:41:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[23] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:40.618109Z","level":"info","event":"25/12/19 13:41:40 INFO YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:40.626677Z","level":"info","event":"25/12/19 13:41:40 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 12d706d757b9:34603 in memory (size: 20.3 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:40.626867Z","level":"info","event":"25/12/19 13:41:40 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 30) (datanode-nodemanager-1, executor 2, partition 0, NODE_LOCAL, 9010 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:40.627334Z","level":"info","event":"25/12/19 13:41:40 INFO BlockManagerInfo: Removed broadcast_5_piece0 on datanode-nodemanager-1:34777 in memory (size: 20.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:40.642651Z","level":"info","event":"25/12/19 13:41:40 INFO BlockManagerInfo: Removed broadcast_5_piece0 on datanode-nodemanager-2:45875 in memory (size: 20.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:40.684403Z","level":"info","event":"25/12/19 13:41:40 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on datanode-nodemanager-1:34777 (size: 25.0 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:41.429388Z","level":"info","event":"25/12/19 13:41:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.18.0.4:50404","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:45.475291Z","level":"info","event":"25/12/19 13:41:45 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 30) in 4848 ms on datanode-nodemanager-1 (executor 2) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:45.475685Z","level":"info","event":"25/12/19 13:41:45 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:45.476635Z","level":"info","event":"25/12/19 13:41:45 INFO DAGScheduler: ResultStage 6 (jdbc at NativeMethodAccessorImpl.java:0) finished in 4.914 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:45.477207Z","level":"info","event":"25/12/19 13:41:45 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:45.477336Z","level":"info","event":"25/12/19 13:41:45 INFO YarnScheduler: Killing all running tasks in stage 6: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:45.477928Z","level":"info","event":"25/12/19 13:41:45 INFO DAGScheduler: Job 4 finished: jdbc at NativeMethodAccessorImpl.java:0, took 4.930849 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:45.936698Z","level":"info","event":"25/12/19 13:41:45 INFO DataSourceStrategy: Pruning directories with: isnotnull(ingest_date#21),(ingest_date#21 >= 2025-09-01),(ingest_date#21 < 2025-09-08)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:45.940296Z","level":"info","event":"25/12/19 13:41:45 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:45.941197Z","level":"info","event":"25/12/19 13:41:45 INFO FileSourceStrategy: Post-Scan Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:46.116522Z","level":"info","event":"25/12/19 13:41:46 INFO CodeGenerator: Code generated in 125.076748 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:46.130864Z","level":"info","event":"25/12/19 13:41:46 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 202.5 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:46.159796Z","level":"info","event":"25/12/19 13:41:46 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 12d706d757b9:34603 in memory (size: 25.0 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:46.166019Z","level":"info","event":"25/12/19 13:41:46 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 35.8 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:46.169476Z","level":"info","event":"25/12/19 13:41:46 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 12d706d757b9:34603 (size: 35.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:46.172156Z","level":"info","event":"25/12/19 13:41:46 INFO SparkContext: Created broadcast 7 from jdbc at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:46.177267Z","level":"info","event":"25/12/19 13:41:46 INFO InMemoryFileIndex: Selected 7 partitions out of 16, pruned 56.25% partitions.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:46.177371Z","level":"info","event":"25/12/19 13:41:46 INFO BlockManagerInfo: Removed broadcast_6_piece0 on datanode-nodemanager-1:34777 in memory (size: 25.0 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:46.179084Z","level":"info","event":"25/12/19 13:41:46 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:46.213350Z","level":"info","event":"25/12/19 13:41:46 INFO DAGScheduler: Registering RDD 27 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 2","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:46.213595Z","level":"info","event":"25/12/19 13:41:46 INFO DAGScheduler: Got map stage job 5 (jdbc at NativeMethodAccessorImpl.java:0) with 14 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:46.213679Z","level":"info","event":"25/12/19 13:41:46 INFO DAGScheduler: Final stage: ShuffleMapStage 7 (jdbc at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:46.213732Z","level":"info","event":"25/12/19 13:41:46 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:46.220469Z","level":"info","event":"25/12/19 13:41:46 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:46.223989Z","level":"info","event":"25/12/19 13:41:46 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[27] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:46.249332Z","level":"info","event":"25/12/19 13:41:46 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 37.1 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:46.288676Z","level":"info","event":"25/12/19 13:41:46 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:46.291368Z","level":"info","event":"25/12/19 13:41:46 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 12d706d757b9:34603 (size: 16.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:46.297094Z","level":"info","event":"25/12/19 13:41:46 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:46.297358Z","level":"info","event":"25/12/19 13:41:46 INFO DAGScheduler: Submitting 14 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[27] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:46.297431Z","level":"info","event":"25/12/19 13:41:46 INFO YarnScheduler: Adding task set 7.0 with 14 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:46.308574Z","level":"info","event":"25/12/19 13:41:46 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 31) (datanode-nodemanager-2, executor 1, partition 0, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:46.309042Z","level":"info","event":"25/12/19 13:41:46 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 32) (datanode-nodemanager-1, executor 2, partition 1, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:46.358683Z","level":"info","event":"25/12/19 13:41:46 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on datanode-nodemanager-2:45875 (size: 16.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:46.360015Z","level":"info","event":"25/12/19 13:41:46 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on datanode-nodemanager-1:34777 (size: 16.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:46.610307Z","level":"info","event":"25/12/19 13:41:46 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on datanode-nodemanager-1:34777 (size: 35.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:46.623067Z","level":"info","event":"25/12/19 13:41:46 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on datanode-nodemanager-2:45875 (size: 35.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:47.441082Z","level":"info","event":"25/12/19 13:41:47 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 33) (datanode-nodemanager-2, executor 1, partition 2, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:47.445768Z","level":"info","event":"25/12/19 13:41:47 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 31) in 1139 ms on datanode-nodemanager-2 (executor 1) (1/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:47.619050Z","level":"info","event":"25/12/19 13:41:47 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 34) (datanode-nodemanager-1, executor 2, partition 3, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:47.626717Z","level":"info","event":"25/12/19 13:41:47 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 32) in 1318 ms on datanode-nodemanager-1 (executor 2) (2/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:48.249722Z","level":"info","event":"25/12/19 13:41:48 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 35) (datanode-nodemanager-2, executor 1, partition 4, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:48.252513Z","level":"info","event":"25/12/19 13:41:48 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 33) in 812 ms on datanode-nodemanager-2 (executor 1) (3/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:48.391313Z","level":"info","event":"25/12/19 13:41:48 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 36) (datanode-nodemanager-1, executor 2, partition 5, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:48.397091Z","level":"info","event":"25/12/19 13:41:48 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 34) in 774 ms on datanode-nodemanager-1 (executor 2) (4/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:48.751045Z","level":"info","event":"25/12/19 13:41:48 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 37) (datanode-nodemanager-2, executor 1, partition 6, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:48.752421Z","level":"info","event":"25/12/19 13:41:48 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 35) in 503 ms on datanode-nodemanager-2 (executor 1) (5/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:48.927184Z","level":"info","event":"25/12/19 13:41:48 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 38) (datanode-nodemanager-1, executor 2, partition 7, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:48.929374Z","level":"info","event":"25/12/19 13:41:48 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 36) in 538 ms on datanode-nodemanager-1 (executor 2) (6/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:49.258458Z","level":"info","event":"25/12/19 13:41:49 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 39) (datanode-nodemanager-2, executor 1, partition 8, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:49.262176Z","level":"info","event":"25/12/19 13:41:49 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 37) in 511 ms on datanode-nodemanager-2 (executor 1) (7/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:49.488109Z","level":"info","event":"25/12/19 13:41:49 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 40) (datanode-nodemanager-1, executor 2, partition 9, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:49.493327Z","level":"info","event":"25/12/19 13:41:49 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 38) in 563 ms on datanode-nodemanager-1 (executor 2) (8/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:49.753922Z","level":"info","event":"25/12/19 13:41:49 INFO TaskSetManager: Starting task 10.0 in stage 7.0 (TID 41) (datanode-nodemanager-2, executor 1, partition 10, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:49.757278Z","level":"info","event":"25/12/19 13:41:49 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 39) in 500 ms on datanode-nodemanager-2 (executor 1) (9/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:50.020734Z","level":"info","event":"25/12/19 13:41:50 INFO TaskSetManager: Starting task 11.0 in stage 7.0 (TID 42) (datanode-nodemanager-1, executor 2, partition 11, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:50.021938Z","level":"info","event":"25/12/19 13:41:50 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 40) in 534 ms on datanode-nodemanager-1 (executor 2) (10/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:50.136872Z","level":"info","event":"25/12/19 13:41:50 INFO TaskSetManager: Starting task 12.0 in stage 7.0 (TID 43) (datanode-nodemanager-2, executor 1, partition 12, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:50.138229Z","level":"info","event":"25/12/19 13:41:50 INFO TaskSetManager: Finished task 10.0 in stage 7.0 (TID 41) in 385 ms on datanode-nodemanager-2 (executor 1) (11/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:50.440569Z","level":"info","event":"25/12/19 13:41:50 INFO TaskSetManager: Starting task 13.0 in stage 7.0 (TID 44) (datanode-nodemanager-1, executor 2, partition 13, NODE_LOCAL, 13862 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:50.441817Z","level":"info","event":"25/12/19 13:41:50 INFO TaskSetManager: Finished task 11.0 in stage 7.0 (TID 42) in 422 ms on datanode-nodemanager-1 (executor 2) (12/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:50.506373Z","level":"info","event":"25/12/19 13:41:50 INFO TaskSetManager: Finished task 12.0 in stage 7.0 (TID 43) in 369 ms on datanode-nodemanager-2 (executor 1) (13/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:50.770528Z","level":"info","event":"25/12/19 13:41:50 INFO TaskSetManager: Finished task 13.0 in stage 7.0 (TID 44) in 330 ms on datanode-nodemanager-1 (executor 2) (14/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:50.770725Z","level":"info","event":"25/12/19 13:41:50 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:50.771308Z","level":"info","event":"25/12/19 13:41:50 INFO DAGScheduler: ShuffleMapStage 7 (jdbc at NativeMethodAccessorImpl.java:0) finished in 4.533 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:50.771455Z","level":"info","event":"25/12/19 13:41:50 INFO DAGScheduler: looking for newly runnable stages","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:50.771572Z","level":"info","event":"25/12/19 13:41:50 INFO DAGScheduler: running: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:50.771647Z","level":"info","event":"25/12/19 13:41:50 INFO DAGScheduler: waiting: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:50.771703Z","level":"info","event":"25/12/19 13:41:50 INFO DAGScheduler: failed: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:50.777548Z","level":"info","event":"25/12/19 13:41:50 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:50.949182Z","level":"info","event":"25/12/19 13:41:50 INFO CodeGenerator: Code generated in 121.452325 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:51.065881Z","level":"info","event":"25/12/19 13:41:51 INFO SparkContext: Starting job: jdbc at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:51.068121Z","level":"info","event":"25/12/19 13:41:51 INFO DAGScheduler: Got job 6 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:51.068357Z","level":"info","event":"25/12/19 13:41:51 INFO DAGScheduler: Final stage: ResultStage 9 (jdbc at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:51.068417Z","level":"info","event":"25/12/19 13:41:51 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:51.078645Z","level":"info","event":"25/12/19 13:41:51 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:51.081310Z","level":"info","event":"25/12/19 13:41:51 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[32] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:51.111038Z","level":"info","event":"25/12/19 13:41:51 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 54.3 KiB, free 433.8 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:51.158736Z","level":"info","event":"25/12/19 13:41:51 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 23.8 KiB, free 433.8 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:51.160449Z","level":"info","event":"25/12/19 13:41:51 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 12d706d757b9:34603 (size: 23.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:51.167461Z","level":"info","event":"25/12/19 13:41:51 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:51.172251Z","level":"info","event":"25/12/19 13:41:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[32] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:51.174325Z","level":"info","event":"25/12/19 13:41:51 INFO YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:51.189067Z","level":"info","event":"25/12/19 13:41:51 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 45) (datanode-nodemanager-1, executor 2, partition 0, NODE_LOCAL, 9010 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:51.189535Z","level":"info","event":"25/12/19 13:41:51 INFO BlockManagerInfo: Removed broadcast_8_piece0 on datanode-nodemanager-1:34777 in memory (size: 16.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:51.195734Z","level":"info","event":"25/12/19 13:41:51 INFO BlockManagerInfo: Removed broadcast_8_piece0 on datanode-nodemanager-2:45875 in memory (size: 16.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:51.199431Z","level":"info","event":"25/12/19 13:41:51 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 12d706d757b9:34603 in memory (size: 16.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:51.243677Z","level":"info","event":"25/12/19 13:41:51 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on datanode-nodemanager-1:34777 (size: 23.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:51.311816Z","level":"info","event":"25/12/19 13:41:51 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.18.0.4:50404","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:52.822205Z","level":"info","event":"25/12/19 13:41:52 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 45) in 1642 ms on datanode-nodemanager-1 (executor 2) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:52.825986Z","level":"info","event":"25/12/19 13:41:52 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:52.829161Z","level":"info","event":"25/12/19 13:41:52 INFO DAGScheduler: ResultStage 9 (jdbc at NativeMethodAccessorImpl.java:0) finished in 1.743 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:52.829363Z","level":"info","event":"25/12/19 13:41:52 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:52.829445Z","level":"info","event":"25/12/19 13:41:52 INFO YarnScheduler: Killing all running tasks in stage 9: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:52.829932Z","level":"info","event":"25/12/19 13:41:52 INFO DAGScheduler: Job 6 finished: jdbc at NativeMethodAccessorImpl.java:0, took 1.764251 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:53.412289Z","level":"info","event":"25/12/19 13:41:53 INFO DataSourceStrategy: Pruning directories with: isnotnull(ingest_date#21),(ingest_date#21 >= 2025-09-01),(ingest_date#21 < 2025-09-08)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:53.413719Z","level":"info","event":"25/12/19 13:41:53 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:53.413889Z","level":"info","event":"25/12/19 13:41:53 INFO FileSourceStrategy: Post-Scan Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:53.625215Z","level":"info","event":"25/12/19 13:41:53 INFO CodeGenerator: Code generated in 98.159329 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:53.630906Z","level":"info","event":"25/12/19 13:41:53 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 203.3 KiB, free 433.7 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:53.669234Z","level":"info","event":"25/12/19 13:41:53 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.6 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:53.672248Z","level":"info","event":"25/12/19 13:41:53 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 12d706d757b9:34603 (size: 36.1 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:53.675592Z","level":"info","event":"25/12/19 13:41:53 INFO SparkContext: Created broadcast 10 from showString at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:53.681123Z","level":"info","event":"25/12/19 13:41:53 INFO InMemoryFileIndex: Selected 7 partitions out of 16, pruned 56.25% partitions.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:53.681749Z","level":"info","event":"25/12/19 13:41:53 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 12d706d757b9:34603 in memory (size: 35.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:53.691741Z","level":"info","event":"25/12/19 13:41:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:53.694869Z","level":"info","event":"25/12/19 13:41:53 INFO BlockManagerInfo: Removed broadcast_7_piece0 on datanode-nodemanager-2:45875 in memory (size: 35.8 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:53.695073Z","level":"info","event":"25/12/19 13:41:53 INFO BlockManagerInfo: Removed broadcast_7_piece0 on datanode-nodemanager-1:34777 in memory (size: 35.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:53.749815Z","level":"info","event":"25/12/19 13:41:53 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 12d706d757b9:34603 in memory (size: 23.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:53.769224Z","level":"info","event":"25/12/19 13:41:53 INFO BlockManagerInfo: Removed broadcast_9_piece0 on datanode-nodemanager-1:34777 in memory (size: 23.8 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:53.818338Z","level":"info","event":"25/12/19 13:41:53 INFO DAGScheduler: Registering RDD 37 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 3","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:53.819101Z","level":"info","event":"25/12/19 13:41:53 INFO DAGScheduler: Got map stage job 7 (showString at NativeMethodAccessorImpl.java:0) with 14 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:53.819376Z","level":"info","event":"25/12/19 13:41:53 INFO DAGScheduler: Final stage: ShuffleMapStage 10 (showString at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:53.819569Z","level":"info","event":"25/12/19 13:41:53 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:53.848958Z","level":"info","event":"25/12/19 13:41:53 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:53.869249Z","level":"info","event":"25/12/19 13:41:53 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[37] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:53.989411Z","level":"info","event":"25/12/19 13:41:53 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 40.4 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:54.012725Z","level":"info","event":"25/12/19 13:41:54 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:54.015219Z","level":"info","event":"25/12/19 13:41:54 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 12d706d757b9:34603 (size: 17.0 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:54.016968Z","level":"info","event":"25/12/19 13:41:54 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:54.018027Z","level":"info","event":"25/12/19 13:41:54 INFO DAGScheduler: Submitting 14 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[37] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:54.018189Z","level":"info","event":"25/12/19 13:41:54 INFO YarnScheduler: Adding task set 10.0 with 14 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:54.021140Z","level":"info","event":"25/12/19 13:41:54 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 46) (datanode-nodemanager-1, executor 2, partition 0, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:54.023165Z","level":"info","event":"25/12/19 13:41:54 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 47) (datanode-nodemanager-2, executor 1, partition 1, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:54.051801Z","level":"info","event":"25/12/19 13:41:54 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on datanode-nodemanager-2:45875 (size: 17.0 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:54.065097Z","level":"info","event":"25/12/19 13:41:54 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on datanode-nodemanager-1:34777 (size: 17.0 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:54.289916Z","level":"info","event":"25/12/19 13:41:54 INFO BlockManagerInfo: Removed broadcast_4_piece0 on datanode-nodemanager-1:34777 in memory (size: 35.9 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:54.295696Z","level":"info","event":"25/12/19 13:41:54 INFO BlockManagerInfo: Removed broadcast_4_piece0 on datanode-nodemanager-2:45875 in memory (size: 35.9 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:54.325877Z","level":"info","event":"25/12/19 13:41:54 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 12d706d757b9:34603 in memory (size: 35.9 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:54.626354Z","level":"info","event":"25/12/19 13:41:54 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on datanode-nodemanager-2:45875 (size: 36.1 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:54.742251Z","level":"info","event":"25/12/19 13:41:54 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on datanode-nodemanager-1:34777 (size: 36.1 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:56.208310Z","level":"info","event":"25/12/19 13:41:56 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 48) (datanode-nodemanager-2, executor 1, partition 2, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:56.211295Z","level":"info","event":"25/12/19 13:41:56 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 47) in 2188 ms on datanode-nodemanager-2 (executor 1) (1/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:56.419901Z","level":"info","event":"25/12/19 13:41:56 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 49) (datanode-nodemanager-1, executor 2, partition 3, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:56.420490Z","level":"info","event":"25/12/19 13:41:56 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 46) in 2400 ms on datanode-nodemanager-1 (executor 2) (2/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:57.312419Z","level":"info","event":"25/12/19 13:41:57 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 50) (datanode-nodemanager-2, executor 1, partition 4, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:57.313955Z","level":"info","event":"25/12/19 13:41:57 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 48) in 1106 ms on datanode-nodemanager-2 (executor 1) (3/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:57.511917Z","level":"info","event":"25/12/19 13:41:57 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 51) (datanode-nodemanager-1, executor 2, partition 5, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:57.512931Z","level":"info","event":"25/12/19 13:41:57 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 49) in 1093 ms on datanode-nodemanager-1 (executor 2) (4/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:58.161154Z","level":"info","event":"25/12/19 13:41:58 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 52) (datanode-nodemanager-2, executor 1, partition 6, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:58.162506Z","level":"info","event":"25/12/19 13:41:58 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 50) in 853 ms on datanode-nodemanager-2 (executor 1) (5/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:58.401808Z","level":"info","event":"25/12/19 13:41:58 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 53) (datanode-nodemanager-1, executor 2, partition 7, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:58.402190Z","level":"info","event":"25/12/19 13:41:58 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 51) in 891 ms on datanode-nodemanager-1 (executor 2) (6/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:59.070619Z","level":"info","event":"25/12/19 13:41:59 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 54) (datanode-nodemanager-2, executor 1, partition 8, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:59.071342Z","level":"info","event":"25/12/19 13:41:59 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 52) in 911 ms on datanode-nodemanager-2 (executor 1) (7/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:59.383557Z","level":"info","event":"25/12/19 13:41:59 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 55) (datanode-nodemanager-1, executor 2, partition 9, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:59.384384Z","level":"info","event":"25/12/19 13:41:59 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 53) in 985 ms on datanode-nodemanager-1 (executor 2) (8/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:59.762463Z","level":"info","event":"25/12/19 13:41:59 INFO TaskSetManager: Starting task 10.0 in stage 10.0 (TID 56) (datanode-nodemanager-2, executor 1, partition 10, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:59.763345Z","level":"info","event":"25/12/19 13:41:59 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 54) in 693 ms on datanode-nodemanager-2 (executor 1) (9/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:00.043193Z","level":"info","event":"25/12/19 13:42:00 INFO TaskSetManager: Starting task 11.0 in stage 10.0 (TID 57) (datanode-nodemanager-1, executor 2, partition 11, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:00.044410Z","level":"info","event":"25/12/19 13:42:00 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 55) in 662 ms on datanode-nodemanager-1 (executor 2) (10/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:00.511793Z","level":"info","event":"25/12/19 13:42:00 INFO TaskSetManager: Starting task 12.0 in stage 10.0 (TID 58) (datanode-nodemanager-2, executor 1, partition 12, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:00.518178Z","level":"info","event":"25/12/19 13:42:00 INFO TaskSetManager: Finished task 10.0 in stage 10.0 (TID 56) in 752 ms on datanode-nodemanager-2 (executor 1) (11/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:00.751443Z","level":"info","event":"25/12/19 13:42:00 INFO TaskSetManager: Starting task 13.0 in stage 10.0 (TID 59) (datanode-nodemanager-1, executor 2, partition 13, NODE_LOCAL, 13862 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:00.751674Z","level":"info","event":"25/12/19 13:42:00 INFO TaskSetManager: Finished task 11.0 in stage 10.0 (TID 57) in 708 ms on datanode-nodemanager-1 (executor 2) (12/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.233099Z","level":"info","event":"25/12/19 13:42:01 INFO TaskSetManager: Finished task 12.0 in stage 10.0 (TID 58) in 725 ms on datanode-nodemanager-2 (executor 1) (13/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.495096Z","level":"info","event":"25/12/19 13:42:01 INFO TaskSetManager: Finished task 13.0 in stage 10.0 (TID 59) in 752 ms on datanode-nodemanager-1 (executor 2) (14/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.495645Z","level":"info","event":"25/12/19 13:42:01 INFO YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.495815Z","level":"info","event":"25/12/19 13:42:01 INFO DAGScheduler: ShuffleMapStage 10 (showString at NativeMethodAccessorImpl.java:0) finished in 7.627 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.495889Z","level":"info","event":"25/12/19 13:42:01 INFO DAGScheduler: looking for newly runnable stages","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.495937Z","level":"info","event":"25/12/19 13:42:01 INFO DAGScheduler: running: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.495980Z","level":"info","event":"25/12/19 13:42:01 INFO DAGScheduler: waiting: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.496023Z","level":"info","event":"25/12/19 13:42:01 INFO DAGScheduler: failed: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.503985Z","level":"info","event":"25/12/19 13:42:01 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.561483Z","level":"info","event":"25/12/19 13:42:01 INFO CodeGenerator: Code generated in 32.865869 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.596198Z","level":"info","event":"25/12/19 13:42:01 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.598384Z","level":"info","event":"25/12/19 13:42:01 INFO DAGScheduler: Got job 8 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.598711Z","level":"info","event":"25/12/19 13:42:01 INFO DAGScheduler: Final stage: ResultStage 12 (showString at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.599013Z","level":"info","event":"25/12/19 13:42:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.602492Z","level":"info","event":"25/12/19 13:42:01 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.606607Z","level":"info","event":"25/12/19 13:42:01 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[41] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.652436Z","level":"info","event":"25/12/19 13:42:01 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 46.5 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.681027Z","level":"info","event":"25/12/19 13:42:01 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 434.0 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.695143Z","level":"info","event":"25/12/19 13:42:01 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 12d706d757b9:34603 (size: 19.9 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.695525Z","level":"info","event":"25/12/19 13:42:01 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 12d706d757b9:34603 in memory (size: 17.0 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.695620Z","level":"info","event":"25/12/19 13:42:01 INFO BlockManagerInfo: Removed broadcast_11_piece0 on datanode-nodemanager-2:45875 in memory (size: 17.0 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.695677Z","level":"info","event":"25/12/19 13:42:01 INFO BlockManagerInfo: Removed broadcast_11_piece0 on datanode-nodemanager-1:34777 in memory (size: 17.0 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.697518Z","level":"info","event":"25/12/19 13:42:01 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.699477Z","level":"info","event":"25/12/19 13:42:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[41] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.699699Z","level":"info","event":"25/12/19 13:42:01 INFO YarnScheduler: Adding task set 12.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.703798Z","level":"info","event":"25/12/19 13:42:01 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 60) (datanode-nodemanager-2, executor 1, partition 0, NODE_LOCAL, 9010 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.750726Z","level":"info","event":"25/12/19 13:42:01 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on datanode-nodemanager-2:45875 (size: 19.9 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:01.769218Z","level":"info","event":"25/12/19 13:42:01 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.18.0.3:38246","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.326296Z","level":"info","event":"25/12/19 13:42:02 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 60) in 622 ms on datanode-nodemanager-2 (executor 1) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.329252Z","level":"info","event":"25/12/19 13:42:02 INFO DAGScheduler: ResultStage 12 (showString at NativeMethodAccessorImpl.java:0) finished in 0.696 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.332073Z","level":"info","event":"25/12/19 13:42:02 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.332300Z","level":"info","event":"25/12/19 13:42:02 INFO YarnScheduler: Removed TaskSet 12.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.332366Z","level":"info","event":"25/12/19 13:42:02 INFO YarnScheduler: Killing all running tasks in stage 12: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.337212Z","level":"info","event":"25/12/19 13:42:02 INFO DAGScheduler: Job 8 finished: showString at NativeMethodAccessorImpl.java:0, took 0.736220 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.397857Z","level":"info","event":"25/12/19 13:42:02 INFO CodeGenerator: Code generated in 33.543091 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.415190Z","level":"info","event":"+--------------------+--------------------+------------------+--------------------+--------------------+---------------------------+----------------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.418690Z","level":"info","event":"|             news_id|        news_time_id|news_overall_score|          news_title|        news_summary|news_category_within_source|     news_source|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.418841Z","level":"info","event":"+--------------------+--------------------+------------------+--------------------+--------------------+---------------------------+----------------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.419224Z","level":"info","event":"|-9220512935994728299| 6367420500199088588|          0.186512|Cerence AI Files ...|BURLINGTON, Mass....|                       News|        Benzinga|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.419329Z","level":"info","event":"|-9215974233538529989| 2869404304239412763|          0.197429|Macquarie Asset M...|Dow has received ...|                       News|        Benzinga|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.419381Z","level":"info","event":"|-9215253552835217895|-8675524983303160171|           0.30086|Brady Corporation...|MILWAUKEE, Sept. ...|                       News|        Benzinga|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.419429Z","level":"info","event":"|-9214964412323901468|-1861558615695025446|          0.401813|Independent Bank ...|Dividends are one...|                           |Zacks Commentary|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.419477Z","level":"info","event":"|-9213791324328842980|-5757472051673998136|          0.247479|4 Brilliant Stock...|The AI arms race ...|                        n/a|     Motley Fool|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.419528Z","level":"info","event":"|-9211392508655787880|-7039825263568189942|          0.026307|91% of Jensen Hua...|Nvidia uses firm ...|                           |     Motley Fool|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.419573Z","level":"info","event":"|-9206036968928510820| 2869404304239412763|          0.166119|After Plunging 14...|Torrid Holdings (...|                        n/a|Zacks Commentary|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.419622Z","level":"info","event":"|-9205659208143189960|-8675524983303160171|         -0.081615|Alto Neuroscience...|Investors can con...|                       News|        Benzinga|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.419672Z","level":"info","event":"|-9202385578066064236| -222147908674804431|          0.436498|Welnax NeuroRelie...|New York, Sept. 0...|                           |        Benzinga|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.419719Z","level":"info","event":"|-9191398791713679453|-8675524983303160171|          0.396383|NCS Multistage  (...|NCS Multistage (N...|                        n/a|Zacks Commentary|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.419766Z","level":"info","event":"+--------------------+--------------------+------------------+--------------------+--------------------+---------------------------+----------------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.419813Z","level":"info","event":"only showing top 10 rows","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.419863Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.597268Z","level":"info","event":"25/12/19 13:42:02 INFO DataSourceStrategy: Pruning directories with: isnotnull(ingest_date#21),(ingest_date#21 >= 2025-09-01),(ingest_date#21 < 2025-09-08)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.598709Z","level":"info","event":"25/12/19 13:42:02 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.599101Z","level":"info","event":"25/12/19 13:42:02 INFO FileSourceStrategy: Post-Scan Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.664411Z","level":"info","event":"25/12/19 13:42:02 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 203.3 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.691894Z","level":"info","event":"25/12/19 13:42:02 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 12d706d757b9:34603 in memory (size: 36.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.696180Z","level":"info","event":"25/12/19 13:42:02 INFO BlockManagerInfo: Removed broadcast_10_piece0 on datanode-nodemanager-1:34777 in memory (size: 36.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.696517Z","level":"info","event":"25/12/19 13:42:02 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.701196Z","level":"info","event":"25/12/19 13:42:02 INFO BlockManagerInfo: Removed broadcast_10_piece0 on datanode-nodemanager-2:45875 in memory (size: 36.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.701400Z","level":"info","event":"25/12/19 13:42:02 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 12d706d757b9:34603 (size: 36.1 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.705104Z","level":"info","event":"25/12/19 13:42:02 INFO SparkContext: Created broadcast 13 from jdbc at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.706429Z","level":"info","event":"25/12/19 13:42:02 INFO InMemoryFileIndex: Selected 7 partitions out of 16, pruned 56.25% partitions.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.713195Z","level":"info","event":"25/12/19 13:42:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.739288Z","level":"info","event":"25/12/19 13:42:02 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 12d706d757b9:34603 in memory (size: 19.9 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.745118Z","level":"info","event":"25/12/19 13:42:02 INFO BlockManagerInfo: Removed broadcast_12_piece0 on datanode-nodemanager-2:45875 in memory (size: 19.9 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.756725Z","level":"info","event":"25/12/19 13:42:02 INFO DAGScheduler: Registering RDD 46 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 4","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.761269Z","level":"info","event":"25/12/19 13:42:02 INFO DAGScheduler: Got map stage job 9 (jdbc at NativeMethodAccessorImpl.java:0) with 14 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.763315Z","level":"info","event":"25/12/19 13:42:02 INFO DAGScheduler: Final stage: ShuffleMapStage 13 (jdbc at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.765212Z","level":"info","event":"25/12/19 13:42:02 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.778222Z","level":"info","event":"25/12/19 13:42:02 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.784431Z","level":"info","event":"25/12/19 13:42:02 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[46] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.829476Z","level":"info","event":"25/12/19 13:42:02 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 40.4 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.832395Z","level":"info","event":"25/12/19 13:42:02 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.834375Z","level":"info","event":"25/12/19 13:42:02 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 12d706d757b9:34603 (size: 17.0 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.841220Z","level":"info","event":"25/12/19 13:42:02 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.841653Z","level":"info","event":"25/12/19 13:42:02 INFO DAGScheduler: Submitting 14 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[46] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.841714Z","level":"info","event":"25/12/19 13:42:02 INFO YarnScheduler: Adding task set 13.0 with 14 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.841789Z","level":"info","event":"25/12/19 13:42:02 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 61) (datanode-nodemanager-1, executor 2, partition 0, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.841845Z","level":"info","event":"25/12/19 13:42:02 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 62) (datanode-nodemanager-2, executor 1, partition 1, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.863945Z","level":"info","event":"25/12/19 13:42:02 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on datanode-nodemanager-1:34777 (size: 17.0 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.865362Z","level":"info","event":"25/12/19 13:42:02 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on datanode-nodemanager-2:45875 (size: 17.0 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.895252Z","level":"info","event":"25/12/19 13:42:02 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on datanode-nodemanager-1:34777 (size: 36.1 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:02.897893Z","level":"info","event":"25/12/19 13:42:02 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on datanode-nodemanager-2:45875 (size: 36.1 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:03.474840Z","level":"info","event":"25/12/19 13:42:03 INFO TaskSetManager: Starting task 2.0 in stage 13.0 (TID 63) (datanode-nodemanager-1, executor 2, partition 2, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:03.479300Z","level":"info","event":"25/12/19 13:42:03 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 61) in 639 ms on datanode-nodemanager-1 (executor 2) (1/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:59.953690Z","level":"info","event":"25/12/19 13:41:59 INFO TaskSetManager: Starting task 3.0 in stage 13.0 (TID 64) (datanode-nodemanager-2, executor 1, partition 3, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:41:59.960535Z","level":"info","event":"25/12/19 13:41:59 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 62) in -2880 ms on datanode-nodemanager-2 (executor 1) (2/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:07.716394Z","level":"info","event":"25/12/19 13:42:07 INFO TaskSetManager: Starting task 4.0 in stage 13.0 (TID 65) (datanode-nodemanager-2, executor 1, partition 4, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:07.717387Z","level":"info","event":"25/12/19 13:42:07 INFO TaskSetManager: Finished task 3.0 in stage 13.0 (TID 64) in 7775 ms on datanode-nodemanager-2 (executor 1) (3/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:07.748764Z","level":"info","event":"25/12/19 13:42:07 INFO TaskSetManager: Starting task 5.0 in stage 13.0 (TID 66) (datanode-nodemanager-1, executor 2, partition 5, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:07.749008Z","level":"info","event":"25/12/19 13:42:07 INFO TaskSetManager: Finished task 2.0 in stage 13.0 (TID 63) in 4275 ms on datanode-nodemanager-1 (executor 2) (4/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:08.675537Z","level":"info","event":"25/12/19 13:42:08 INFO TaskSetManager: Starting task 6.0 in stage 13.0 (TID 67) (datanode-nodemanager-2, executor 1, partition 6, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:08.681586Z","level":"info","event":"25/12/19 13:42:08 INFO TaskSetManager: Finished task 4.0 in stage 13.0 (TID 65) in 965 ms on datanode-nodemanager-2 (executor 1) (5/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:08.753887Z","level":"info","event":"25/12/19 13:42:08 INFO TaskSetManager: Starting task 7.0 in stage 13.0 (TID 68) (datanode-nodemanager-1, executor 2, partition 7, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:08.754602Z","level":"info","event":"25/12/19 13:42:08 INFO TaskSetManager: Finished task 5.0 in stage 13.0 (TID 66) in 1007 ms on datanode-nodemanager-1 (executor 2) (6/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:09.370860Z","level":"info","event":"25/12/19 13:42:09 INFO TaskSetManager: Starting task 8.0 in stage 13.0 (TID 69) (datanode-nodemanager-2, executor 1, partition 8, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:09.373286Z","level":"info","event":"25/12/19 13:42:09 INFO TaskSetManager: Finished task 6.0 in stage 13.0 (TID 67) in 697 ms on datanode-nodemanager-2 (executor 1) (7/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:09.436517Z","level":"info","event":"25/12/19 13:42:09 INFO TaskSetManager: Starting task 9.0 in stage 13.0 (TID 70) (datanode-nodemanager-1, executor 2, partition 9, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:09.436753Z","level":"info","event":"25/12/19 13:42:09 INFO TaskSetManager: Finished task 7.0 in stage 13.0 (TID 68) in 684 ms on datanode-nodemanager-1 (executor 2) (8/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:09.837629Z","level":"info","event":"25/12/19 13:42:09 INFO TaskSetManager: Starting task 10.0 in stage 13.0 (TID 71) (datanode-nodemanager-2, executor 1, partition 10, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:09.838353Z","level":"info","event":"25/12/19 13:42:09 INFO TaskSetManager: Finished task 8.0 in stage 13.0 (TID 69) in 469 ms on datanode-nodemanager-2 (executor 1) (9/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:09.912219Z","level":"info","event":"25/12/19 13:42:09 INFO TaskSetManager: Starting task 11.0 in stage 13.0 (TID 72) (datanode-nodemanager-1, executor 2, partition 11, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:09.914123Z","level":"info","event":"25/12/19 13:42:09 INFO TaskSetManager: Finished task 9.0 in stage 13.0 (TID 70) in 478 ms on datanode-nodemanager-1 (executor 2) (10/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:10.357800Z","level":"info","event":"25/12/19 13:42:10 INFO TaskSetManager: Starting task 12.0 in stage 13.0 (TID 73) (datanode-nodemanager-2, executor 1, partition 12, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:10.362991Z","level":"info","event":"25/12/19 13:42:10 INFO TaskSetManager: Finished task 10.0 in stage 13.0 (TID 71) in 525 ms on datanode-nodemanager-2 (executor 1) (11/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:10.595110Z","level":"info","event":"25/12/19 13:42:10 INFO TaskSetManager: Starting task 13.0 in stage 13.0 (TID 74) (datanode-nodemanager-1, executor 2, partition 13, NODE_LOCAL, 13862 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:10.596998Z","level":"info","event":"25/12/19 13:42:10 INFO TaskSetManager: Finished task 11.0 in stage 13.0 (TID 72) in 685 ms on datanode-nodemanager-1 (executor 2) (12/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:10.931266Z","level":"info","event":"25/12/19 13:42:10 INFO TaskSetManager: Finished task 12.0 in stage 13.0 (TID 73) in 573 ms on datanode-nodemanager-2 (executor 1) (13/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:11.063924Z","level":"info","event":"25/12/19 13:42:11 INFO TaskSetManager: Finished task 13.0 in stage 13.0 (TID 74) in 469 ms on datanode-nodemanager-1 (executor 2) (14/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:11.064193Z","level":"info","event":"25/12/19 13:42:11 INFO YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:11.064865Z","level":"info","event":"25/12/19 13:42:11 INFO DAGScheduler: ShuffleMapStage 13 (jdbc at NativeMethodAccessorImpl.java:0) finished in 8.278 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:11.065053Z","level":"info","event":"25/12/19 13:42:11 INFO DAGScheduler: looking for newly runnable stages","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:11.065130Z","level":"info","event":"25/12/19 13:42:11 INFO DAGScheduler: running: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:11.065184Z","level":"info","event":"25/12/19 13:42:11 INFO DAGScheduler: waiting: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:11.065225Z","level":"info","event":"25/12/19 13:42:11 INFO DAGScheduler: failed: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:11.081869Z","level":"info","event":"25/12/19 13:42:11 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:11.141957Z","level":"info","event":"25/12/19 13:42:11 INFO SparkContext: Starting job: jdbc at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:11.145070Z","level":"info","event":"25/12/19 13:42:11 INFO DAGScheduler: Got job 10 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:11.145285Z","level":"info","event":"25/12/19 13:42:11 INFO DAGScheduler: Final stage: ResultStage 15 (jdbc at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:11.145346Z","level":"info","event":"25/12/19 13:42:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:11.148781Z","level":"info","event":"25/12/19 13:42:11 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:11.153736Z","level":"info","event":"25/12/19 13:42:11 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[52] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:11.173496Z","level":"info","event":"25/12/19 13:42:11 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 54.7 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:11.183234Z","level":"info","event":"25/12/19 13:42:11 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 23.6 KiB, free 434.0 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:11.184098Z","level":"info","event":"25/12/19 13:42:11 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 12d706d757b9:34603 (size: 23.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:11.187922Z","level":"info","event":"25/12/19 13:42:11 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:11.188175Z","level":"info","event":"25/12/19 13:42:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[52] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:11.188268Z","level":"info","event":"25/12/19 13:42:11 INFO YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:11.192762Z","level":"info","event":"25/12/19 13:42:11 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 12d706d757b9:34603 in memory (size: 17.0 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:11.197683Z","level":"info","event":"25/12/19 13:42:11 INFO BlockManagerInfo: Removed broadcast_14_piece0 on datanode-nodemanager-1:34777 in memory (size: 17.0 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:11.197891Z","level":"info","event":"25/12/19 13:42:11 INFO BlockManagerInfo: Removed broadcast_14_piece0 on datanode-nodemanager-2:45875 in memory (size: 17.0 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:11.200545Z","level":"info","event":"25/12/19 13:42:11 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 75) (datanode-nodemanager-2, executor 1, partition 0, NODE_LOCAL, 9010 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:11.229697Z","level":"info","event":"25/12/19 13:42:11 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on datanode-nodemanager-2:45875 (size: 23.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:11.517066Z","level":"info","event":"25/12/19 13:42:11 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 172.18.0.3:38246","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:13.904995Z","level":"info","event":"25/12/19 13:42:13 WARN TaskSetManager: Lost task 0.0 in stage 15.0 (TID 75) (datanode-nodemanager-2 executor 1): java.sql.BatchUpdateException: Violation of PRIMARY KEY constraint 'PK__dim_news__4C27CCD80BD94A5D'. Cannot insert duplicate key in object 'dbo.dim_news'. The duplicate key value is (-9220512935994728299).","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:13.905224Z","level":"info","event":"at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2187)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:13.905299Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:13.905396Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:13.905444Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:13.905493Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:13.905538Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:13.905584Z","level":"info","event":"at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:13.905623Z","level":"info","event":"at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:13.905672Z","level":"info","event":"at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:13.905722Z","level":"info","event":"at org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:13.905769Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:13.905824Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:13.905866Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:13.905910Z","level":"info","event":"at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:13.905957Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:13.905999Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:13.906047Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:13.906106Z","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:834)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:13.906151Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:13.911283Z","level":"info","event":"25/12/19 13:42:13 INFO TaskSetManager: Starting task 0.1 in stage 15.0 (TID 76) (datanode-nodemanager-1, executor 2, partition 0, NODE_LOCAL, 9010 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:13.929861Z","level":"info","event":"25/12/19 13:42:13 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on datanode-nodemanager-1:34777 (size: 23.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:13.944686Z","level":"info","event":"25/12/19 13:42:13 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 172.18.0.4:50404","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:14.853007Z","level":"info","event":"25/12/19 13:42:14 INFO TaskSetManager: Lost task 0.1 in stage 15.0 (TID 76) on datanode-nodemanager-1, executor 2: java.sql.BatchUpdateException (Violation of PRIMARY KEY constraint 'PK__dim_news__4C27CCD80BD94A5D'. Cannot insert duplicate key in object 'dbo.dim_news'. The duplicate key value is (-9220512935994728299).) [duplicate 1]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:14.854563Z","level":"info","event":"25/12/19 13:42:14 INFO TaskSetManager: Starting task 0.2 in stage 15.0 (TID 77) (datanode-nodemanager-1, executor 2, partition 0, NODE_LOCAL, 9010 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.225075Z","level":"info","event":"25/12/19 13:42:15 INFO TaskSetManager: Lost task 0.2 in stage 15.0 (TID 77) on datanode-nodemanager-1, executor 2: java.sql.BatchUpdateException (Violation of PRIMARY KEY constraint 'PK__dim_news__4C27CCD80BD94A5D'. Cannot insert duplicate key in object 'dbo.dim_news'. The duplicate key value is (-9220512935994728299).) [duplicate 2]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.226646Z","level":"info","event":"25/12/19 13:42:15 INFO TaskSetManager: Starting task 0.3 in stage 15.0 (TID 78) (datanode-nodemanager-2, executor 1, partition 0, NODE_LOCAL, 9010 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.549770Z","level":"info","event":"25/12/19 13:42:15 INFO TaskSetManager: Lost task 0.3 in stage 15.0 (TID 78) on datanode-nodemanager-2, executor 1: java.sql.BatchUpdateException (Violation of PRIMARY KEY constraint 'PK__dim_news__4C27CCD80BD94A5D'. Cannot insert duplicate key in object 'dbo.dim_news'. The duplicate key value is (-9220512935994728299).) [duplicate 3]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.551452Z","level":"info","event":"25/12/19 13:42:15 ERROR TaskSetManager: Task 0 in stage 15.0 failed 4 times; aborting job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.553961Z","level":"info","event":"25/12/19 13:42:15 INFO YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.556850Z","level":"info","event":"25/12/19 13:42:15 INFO YarnScheduler: Cancelling stage 15","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.557052Z","level":"info","event":"25/12/19 13:42:15 INFO YarnScheduler: Killing all running tasks in stage 15: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 15.0 failed 4 times, most recent failure: Lost task 0.3 in stage 15.0 (TID 78) (datanode-nodemanager-2 executor 1): java.sql.BatchUpdateException: Violation of PRIMARY KEY constraint 'PK__dim_news__4C27CCD80BD94A5D'. Cannot insert duplicate key in object 'dbo.dim_news'. The duplicate key value is (-9220512935994728299).","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.557100Z","level":"info","event":"at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2187)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.557127Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.557151Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.557173Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.557194Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.557215Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.557237Z","level":"info","event":"at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.557256Z","level":"info","event":"at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.557277Z","level":"info","event":"at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.557298Z","level":"info","event":"at org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.557319Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.557340Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.557362Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.557403Z","level":"info","event":"at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.557448Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.557473Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.557495Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.557516Z","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:834)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.557539Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.557564Z","level":"info","event":"Driver stacktrace:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.558264Z","level":"info","event":"25/12/19 13:42:15 INFO DAGScheduler: ResultStage 15 (jdbc at NativeMethodAccessorImpl.java:0) failed in 4.397 s due to Job aborted due to stage failure: Task 0 in stage 15.0 failed 4 times, most recent failure: Lost task 0.3 in stage 15.0 (TID 78) (datanode-nodemanager-2 executor 1): java.sql.BatchUpdateException: Violation of PRIMARY KEY constraint 'PK__dim_news__4C27CCD80BD94A5D'. Cannot insert duplicate key in object 'dbo.dim_news'. The duplicate key value is (-9220512935994728299).","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.558394Z","level":"info","event":"at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2187)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.558448Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.558485Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.558531Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.558575Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.558622Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.558669Z","level":"info","event":"at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.558713Z","level":"info","event":"at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.558755Z","level":"info","event":"at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.558801Z","level":"info","event":"at org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.558849Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.558897Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.558942Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.558986Z","level":"info","event":"at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.559012Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.559035Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.559058Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.559079Z","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:834)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.559113Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.559159Z","level":"info","event":"Driver stacktrace:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.560965Z","level":"info","event":"25/12/19 13:42:15 INFO DAGScheduler: Job 10 failed: jdbc at NativeMethodAccessorImpl.java:0, took 4.418717 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.997705Z","level":"info","event":"Traceback (most recent call last):","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:15.997877Z","level":"info","event":"File \"/workspace/airflow/spark-jobs/update_news_data.py\", line 73, in <module>","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.002355Z","level":"info","event":"dim_news.write.jdbc(","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.002580Z","level":"info","event":"File \"/workspace/airflow/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 1984, in jdbc","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.010987Z","level":"info","event":"File \"/workspace/airflow/.venv/lib/python3.12/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.016549Z","level":"info","event":"File \"/workspace/airflow/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.021958Z","level":"info","event":"File \"/workspace/airflow/.venv/lib/python3.12/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.069559Z","level":"info","event":"py4j.protocol.Py4JJavaError: An error occurred while calling o124.jdbc.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.069766Z","level":"info","event":": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 4 times, most recent failure: Lost task 0.3 in stage 15.0 (TID 78) (datanode-nodemanager-2 executor 1): java.sql.BatchUpdateException: Violation of PRIMARY KEY constraint 'PK__dim_news__4C27CCD80BD94A5D'. Cannot insert duplicate key in object 'dbo.dim_news'. The duplicate key value is (-9220512935994728299).","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.069831Z","level":"info","event":"at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2187)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.069877Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.069916Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.069953Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.069992Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.070031Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.070120Z","level":"info","event":"at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.070163Z","level":"info","event":"at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.070199Z","level":"info","event":"at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.070237Z","level":"info","event":"at org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.070286Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.070327Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.070362Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.070397Z","level":"info","event":"at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.070432Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.070467Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.070510Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.070569Z","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:834)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.070610Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.070655Z","level":"info","event":"Driver stacktrace:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.070695Z","level":"info","event":"at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2898)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.070732Z","level":"info","event":"at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2834)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.070777Z","level":"info","event":"at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2833)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.070805Z","level":"info","event":"at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.070828Z","level":"info","event":"at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.070866Z","level":"info","event":"at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.070904Z","level":"info","event":"at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2833)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.070943Z","level":"info","event":"at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1253)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.070981Z","level":"info","event":"at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1253)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.071025Z","level":"info","event":"at scala.Option.foreach(Option.scala:407)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.071064Z","level":"info","event":"at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1253)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.071113Z","level":"info","event":"at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3102)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.071173Z","level":"info","event":"at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3036)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.071225Z","level":"info","event":"at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3025)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.071265Z","level":"info","event":"at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.071289Z","level":"info","event":"at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:995)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.071324Z","level":"info","event":"at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.071367Z","level":"info","event":"at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.071406Z","level":"info","event":"at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.071443Z","level":"info","event":"at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.071515Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.071580Z","level":"info","event":"at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.071622Z","level":"info","event":"at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.071657Z","level":"info","event":"at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.071692Z","level":"info","event":"at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.071733Z","level":"info","event":"at org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3516)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.071769Z","level":"info","event":"at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.071801Z","level":"info","event":"at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4310)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.071912Z","level":"info","event":"at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.071981Z","level":"info","event":"at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.072031Z","level":"info","event":"at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.072110Z","level":"info","event":"at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.072190Z","level":"info","event":"at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.072235Z","level":"info","event":"at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4308)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.072279Z","level":"info","event":"at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3516)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.072324Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:903)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.072359Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.072401Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.072477Z","level":"info","event":"at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.072517Z","level":"info","event":"at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.072562Z","level":"info","event":"at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.072601Z","level":"info","event":"at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.072638Z","level":"info","event":"at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.072672Z","level":"info","event":"at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.072709Z","level":"info","event":"at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.072745Z","level":"info","event":"at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.072781Z","level":"info","event":"at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.072815Z","level":"info","event":"at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.072930Z","level":"info","event":"at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.072979Z","level":"info","event":"at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.073020Z","level":"info","event":"at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.073057Z","level":"info","event":"at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.073096Z","level":"info","event":"at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.073133Z","level":"info","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.073167Z","level":"info","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.073201Z","level":"info","event":"at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.073273Z","level":"info","event":"at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.073323Z","level":"info","event":"at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.073363Z","level":"info","event":"at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.073401Z","level":"info","event":"at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.073437Z","level":"info","event":"at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.073492Z","level":"info","event":"at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.073547Z","level":"info","event":"at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.073585Z","level":"info","event":"at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.073621Z","level":"info","event":"at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.073681Z","level":"info","event":"at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.073736Z","level":"info","event":"at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:766)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.073774Z","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.073832Z","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.073871Z","level":"info","event":"at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.073914Z","level":"info","event":"at java.base/java.lang.reflect.Method.invoke(Method.java:566)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.073949Z","level":"info","event":"at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.073981Z","level":"info","event":"at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.074013Z","level":"info","event":"at py4j.Gateway.invoke(Gateway.java:282)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.074047Z","level":"info","event":"at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.074080Z","level":"info","event":"at py4j.commands.CallCommand.execute(CallCommand.java:79)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.074153Z","level":"info","event":"at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.074197Z","level":"info","event":"at py4j.ClientServerConnection.run(ClientServerConnection.java:106)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.074238Z","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:829)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.074276Z","level":"info","event":"Caused by: java.sql.BatchUpdateException: Violation of PRIMARY KEY constraint 'PK__dim_news__4C27CCD80BD94A5D'. Cannot insert duplicate key in object 'dbo.dim_news'. The duplicate key value is (-9220512935994728299).","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.074318Z","level":"info","event":"at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2187)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.074357Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.074391Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.074424Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.074456Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.074523Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.074567Z","level":"info","event":"at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.074603Z","level":"info","event":"at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.074637Z","level":"info","event":"at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.074672Z","level":"info","event":"at org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.074706Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.074742Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.074782Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.074822Z","level":"info","event":"at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.074860Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.074897Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.074932Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.074965Z","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:834)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.075043Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.183020Z","level":"info","event":"25/12/19 13:42:16 INFO SparkContext: Invoking stop() from shutdown hook","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.183563Z","level":"info","event":"25/12/19 13:42:16 INFO SparkContext: SparkContext is stopping with exitCode 0.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.214923Z","level":"info","event":"25/12/19 13:42:16 INFO SparkUI: Stopped Spark web UI at http://12d706d757b9:4040","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.225606Z","level":"info","event":"25/12/19 13:42:16 INFO YarnClientSchedulerBackend: Interrupting monitor thread","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.262089Z","level":"info","event":"25/12/19 13:42:16 INFO YarnClientSchedulerBackend: Shutting down all executors","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.263057Z","level":"info","event":"25/12/19 13:42:16 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.279277Z","level":"info","event":"25/12/19 13:42:16 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.313581Z","level":"info","event":"25/12/19 13:42:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.375615Z","level":"info","event":"25/12/19 13:42:16 INFO MemoryStore: MemoryStore cleared","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.377314Z","level":"info","event":"25/12/19 13:42:16 INFO BlockManager: BlockManager stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.395477Z","level":"info","event":"25/12/19 13:42:16 INFO BlockManagerMaster: BlockManagerMaster stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.409968Z","level":"info","event":"25/12/19 13:42:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.437109Z","level":"info","event":"25/12/19 13:42:16 INFO SparkContext: Successfully stopped SparkContext","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.437301Z","level":"info","event":"25/12/19 13:42:16 INFO ShutdownHookManager: Shutdown hook called","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.437913Z","level":"info","event":"25/12/19 13:42:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-6193e1ce-59a7-4107-9d2a-f94304713f6a","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.448396Z","level":"info","event":"25/12/19 13:42:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-157c2819-f7fd-4cf2-932a-2932b8fd775a/pyspark-22865f91-446c-469c-b2f7-3fd9e51ac3af","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.461051Z","level":"info","event":"25/12/19 13:42:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-157c2819-f7fd-4cf2-932a-2932b8fd775a","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:42:16.715125Z","level":"error","event":"Task failed with exception","logger":"task","filename":"task_runner.py","lineno":980,"error_detail":[{"exc_type":"AirflowException","exc_value":"Cannot execute: spark-submit --master yarn --jars /workspace/airflow/connector/mssql-jdbc-12.2.0.jre11.jar --name arrow-spark --verbose --deploy-mode client /workspace/airflow/spark-jobs/update_news_data.py 2025-09-07 2025-09-14. Error code is: 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/workspace/airflow/.venv/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":928,"name":"run"},{"filename":"/workspace/airflow/.venv/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1315,"name":"_execute_task"},{"filename":"/workspace/airflow/.venv/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":416,"name":"wrapper"},{"filename":"/workspace/airflow/.venv/lib/python3.12/site-packages/airflow/providers/apache/spark/operators/spark_submit.py","lineno":197,"name":"execute"},{"filename":"/workspace/airflow/.venv/lib/python3.12/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py","lineno":566,"name":"submit"}],"is_group":false,"exceptions":[]}]}
