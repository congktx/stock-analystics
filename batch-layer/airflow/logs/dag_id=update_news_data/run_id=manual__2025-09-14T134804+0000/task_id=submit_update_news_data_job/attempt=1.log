{"timestamp":"2025-12-19T13:48:23.556875Z","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager","filename":"manager.py","lineno":179}
{"timestamp":"2025-12-19T13:48:23.561693Z","level":"info","event":"Filling up the DagBag from /workspace/airflow/dags/update_news_data.py","logger":"airflow.models.dagbag.DagBag","filename":"dagbag.py","lineno":593}
{"timestamp":"2025-12-19T13:48:24.579561Z","level":"info","event":"Spark-Submit cmd: spark-submit --master yarn --jars /workspace/airflow/connector/mssql-jdbc-12.2.0.jre11.jar --name arrow-spark --verbose --deploy-mode client /workspace/airflow/spark-jobs/update_news_data.py 2025-09-07 2025-09-14","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":474}
{"timestamp":"2025-12-19T13:48:31.909337Z","level":"info","event":"Using properties file: null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.106251Z","level":"info","event":"Parsed arguments:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.106400Z","level":"info","event":"master                  yarn","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.106446Z","level":"info","event":"remote                  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.106472Z","level":"info","event":"deployMode              client","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.106494Z","level":"info","event":"executorMemory          null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.106515Z","level":"info","event":"executorCores           null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.106536Z","level":"info","event":"totalExecutorCores      null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.106562Z","level":"info","event":"propertiesFile          null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.106601Z","level":"info","event":"driverMemory            null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.106623Z","level":"info","event":"driverCores             null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.106642Z","level":"info","event":"driverExtraClassPath    null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.106663Z","level":"info","event":"driverExtraLibraryPath  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.106682Z","level":"info","event":"driverExtraJavaOptions  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.106703Z","level":"info","event":"supervise               false","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.106724Z","level":"info","event":"queue                   null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.106744Z","level":"info","event":"numExecutors            null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.106763Z","level":"info","event":"files                   null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.106783Z","level":"info","event":"pyFiles                 null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.106803Z","level":"info","event":"archives                null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.106825Z","level":"info","event":"mainClass               null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.106845Z","level":"info","event":"primaryResource         file:/workspace/airflow/spark-jobs/update_news_data.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.106867Z","level":"info","event":"name                    arrow-spark","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.106891Z","level":"info","event":"childArgs               [2025-09-07 2025-09-14]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.106930Z","level":"info","event":"jars                    file:/workspace/airflow/connector/mssql-jdbc-12.2.0.jre11.jar","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.106967Z","level":"info","event":"packages                null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.106993Z","level":"info","event":"packagesExclusions      null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.107013Z","level":"info","event":"repositories            null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.107037Z","level":"info","event":"verbose                 true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.107057Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.107088Z","level":"info","event":"Spark properties used, including those specified through","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.107112Z","level":"info","event":"--conf and those from the properties file null:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.107134Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.107155Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.107175Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:32.930981Z","level":"info","event":"25/12/19 13:48:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:33.507895Z","level":"info","event":"Main class:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:33.508516Z","level":"info","event":"org.apache.spark.deploy.PythonRunner","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:33.509401Z","level":"info","event":"Arguments:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:33.509646Z","level":"info","event":"file:/workspace/airflow/spark-jobs/update_news_data.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:33.509708Z","level":"info","event":"null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:33.509898Z","level":"info","event":"2025-09-07","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:33.509965Z","level":"info","event":"2025-09-14","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:33.520802Z","level":"info","event":"Spark config:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:33.521001Z","level":"info","event":"(spark.app.name,arrow-spark)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:33.521076Z","level":"info","event":"(spark.app.submitTime,1766152113472)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:33.521130Z","level":"info","event":"(spark.master,yarn)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:33.521180Z","level":"info","event":"(spark.repl.local.jars,file:///workspace/airflow/connector/mssql-jdbc-12.2.0.jre11.jar)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:33.521220Z","level":"info","event":"(spark.submit.deployMode,client)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:33.521263Z","level":"info","event":"(spark.submit.pyFiles,)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:33.521306Z","level":"info","event":"(spark.yarn.dist.jars,file:///workspace/airflow/connector/mssql-jdbc-12.2.0.jre11.jar)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:33.521395Z","level":"info","event":"(spark.yarn.isPython,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:33.523222Z","level":"info","event":"Classpath elements:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:33.523579Z","level":"info","event":"file:///workspace/airflow/connector/mssql-jdbc-12.2.0.jre11.jar","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:33.524424Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:33.524686Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:35.903872Z","level":"info","event":"25/12/19 13:48:35 INFO SparkContext: Running Spark version 3.5.7","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:35.904524Z","level":"info","event":"25/12/19 13:48:35 INFO SparkContext: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:35.905283Z","level":"info","event":"25/12/19 13:48:35 INFO SparkContext: Java version 11.0.23","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:35.993753Z","level":"info","event":"25/12/19 13:48:35 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:35.994885Z","level":"info","event":"25/12/19 13:48:35 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:35.995802Z","level":"info","event":"25/12/19 13:48:35 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:35.996660Z","level":"info","event":"25/12/19 13:48:35 INFO SparkContext: Submitted application: Checking news data","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:36.069926Z","level":"info","event":"25/12/19 13:48:36 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:36.100237Z","level":"info","event":"25/12/19 13:48:36 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:36.103732Z","level":"info","event":"25/12/19 13:48:36 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:36.313306Z","level":"info","event":"25/12/19 13:48:36 INFO SecurityManager: Changing view acls to: root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:36.314189Z","level":"info","event":"25/12/19 13:48:36 INFO SecurityManager: Changing modify acls to: root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:36.315167Z","level":"info","event":"25/12/19 13:48:36 INFO SecurityManager: Changing view acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:36.315718Z","level":"info","event":"25/12/19 13:48:36 INFO SecurityManager: Changing modify acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:36.316295Z","level":"info","event":"25/12/19 13:48:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:36.886659Z","level":"info","event":"25/12/19 13:48:36 INFO Utils: Successfully started service 'sparkDriver' on port 36969.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:36.947809Z","level":"info","event":"25/12/19 13:48:36 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:37.121909Z","level":"info","event":"25/12/19 13:48:37 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:37.250465Z","level":"info","event":"25/12/19 13:48:37 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:37.251386Z","level":"info","event":"25/12/19 13:48:37 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:37.363745Z","level":"info","event":"25/12/19 13:48:37 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:37.455267Z","level":"info","event":"25/12/19 13:48:37 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cf0d628e-c58d-4f1d-aad5-4a1363c15735","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:37.501425Z","level":"info","event":"25/12/19 13:48:37 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:37.603952Z","level":"info","event":"25/12/19 13:48:37 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:37.964315Z","level":"info","event":"25/12/19 13:48:37 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:38.151853Z","level":"info","event":"25/12/19 13:48:38 INFO Utils: Successfully started service 'SparkUI' on port 4040.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:38.662942Z","level":"info","event":"25/12/19 13:48:38 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at namenode/172.18.0.5:8032","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:39.875239Z","level":"info","event":"25/12/19 13:48:39 INFO Configuration: resource-types.xml not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:39.875769Z","level":"info","event":"25/12/19 13:48:39 INFO ResourceUtils: Unable to find 'resource-types.xml'.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:39.903569Z","level":"info","event":"25/12/19 13:48:39 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:39.904232Z","level":"info","event":"25/12/19 13:48:39 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:39.904718Z","level":"info","event":"25/12/19 13:48:39 INFO Client: Setting up container launch context for our AM","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:39.910747Z","level":"info","event":"25/12/19 13:48:39 INFO Client: Setting up the launch environment for our AM container","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:39.926625Z","level":"info","event":"25/12/19 13:48:39 INFO Client: Preparing resources for our AM container","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:39.993055Z","level":"info","event":"25/12/19 13:48:39 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:56.311168Z","level":"info","event":"25/12/19 13:48:56 INFO Client: Uploading resource file:/tmp/spark-0b6ed2e3-69d8-4d67-8054-31017d5d1942/__spark_libs__6268563973920914833.zip -> hdfs://namenode:8020/user/root/.sparkStaging/application_1766137997195_0018/__spark_libs__6268563973920914833.zip","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:58.262397Z","level":"info","event":"25/12/19 13:48:58 INFO Client: Uploading resource file:/workspace/airflow/connector/mssql-jdbc-12.2.0.jre11.jar -> hdfs://namenode:8020/user/root/.sparkStaging/application_1766137997195_0018/mssql-jdbc-12.2.0.jre11.jar","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:58.320843Z","level":"info","event":"25/12/19 13:48:58 INFO Client: Uploading resource file:/workspace/airflow/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip -> hdfs://namenode:8020/user/root/.sparkStaging/application_1766137997195_0018/pyspark.zip","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:58.414690Z","level":"info","event":"25/12/19 13:48:58 INFO Client: Uploading resource file:/workspace/airflow/.venv/lib/python3.12/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://namenode:8020/user/root/.sparkStaging/application_1766137997195_0018/py4j-0.10.9.7-src.zip","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:58.877897Z","level":"info","event":"25/12/19 13:48:58 INFO Client: Uploading resource file:/tmp/spark-0b6ed2e3-69d8-4d67-8054-31017d5d1942/__spark_conf__3758544205855613668.zip -> hdfs://namenode:8020/user/root/.sparkStaging/application_1766137997195_0018/__spark_conf__.zip","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:58.950231Z","level":"info","event":"25/12/19 13:48:58 INFO SecurityManager: Changing view acls to: root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:58.951232Z","level":"info","event":"25/12/19 13:48:58 INFO SecurityManager: Changing modify acls to: root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:58.951613Z","level":"info","event":"25/12/19 13:48:58 INFO SecurityManager: Changing view acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:58.951860Z","level":"info","event":"25/12/19 13:48:58 INFO SecurityManager: Changing modify acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:58.952158Z","level":"info","event":"25/12/19 13:48:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:59.004797Z","level":"info","event":"25/12/19 13:48:59 INFO Client: Submitting application application_1766137997195_0018 to ResourceManager","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:48:59.077832Z","level":"info","event":"25/12/19 13:48:59 INFO YarnClientImpl: Submitted application application_1766137997195_0018","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:00.082942Z","level":"info","event":"25/12/19 13:49:00 INFO Client: Application report for application_1766137997195_0018 (state: ACCEPTED)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:00.087060Z","level":"info","event":"25/12/19 13:49:00 INFO Client:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:00.087322Z","level":"info","event":"client token: N/A","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:00.087429Z","level":"info","event":"diagnostics: AM container is launched, waiting for AM container to Register with RM","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:00.087486Z","level":"info","event":"ApplicationMaster host: N/A","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:00.087538Z","level":"info","event":"ApplicationMaster RPC port: -1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:00.087618Z","level":"info","event":"queue: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:00.087672Z","level":"info","event":"start time: 1766152139033","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:00.087726Z","level":"info","event":"final status: UNDEFINED","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:00.087777Z","level":"info","event":"tracking URL: http://namenode:8088/proxy/application_1766137997195_0018/","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:00.087828Z","level":"info","event":"user: root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:05.825861Z","level":"info","event":"25/12/19 13:49:05 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> namenode, PROXY_URI_BASES -> http://namenode:8088/proxy/application_1766137997195_0018), /proxy/application_1766137997195_0018","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.101242Z","level":"info","event":"25/12/19 13:49:06 INFO Client: Application report for application_1766137997195_0018 (state: RUNNING)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.101533Z","level":"info","event":"25/12/19 13:49:06 INFO Client:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.101618Z","level":"info","event":"client token: N/A","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.101665Z","level":"info","event":"diagnostics: N/A","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.101704Z","level":"info","event":"ApplicationMaster host: 172.18.0.4","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.101743Z","level":"info","event":"ApplicationMaster RPC port: -1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.101777Z","level":"info","event":"queue: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.101814Z","level":"info","event":"start time: 1766152139033","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.101851Z","level":"info","event":"final status: UNDEFINED","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.101887Z","level":"info","event":"tracking URL: http://namenode:8088/proxy/application_1766137997195_0018/","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.101925Z","level":"info","event":"user: root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.103782Z","level":"info","event":"25/12/19 13:49:06 INFO YarnClientSchedulerBackend: Application application_1766137997195_0018 has started running.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.130868Z","level":"info","event":"25/12/19 13:49:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39431.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.131179Z","level":"info","event":"25/12/19 13:49:06 INFO NettyBlockTransferService: Server created on 12d706d757b9:39431","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.134698Z","level":"info","event":"25/12/19 13:49:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.172389Z","level":"info","event":"25/12/19 13:49:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 12d706d757b9, 39431, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.178496Z","level":"info","event":"25/12/19 13:49:06 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.182277Z","level":"info","event":"25/12/19 13:49:06 INFO BlockManagerMasterEndpoint: Registering block manager 12d706d757b9:39431 with 434.4 MiB RAM, BlockManagerId(driver, 12d706d757b9, 39431, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.194895Z","level":"info","event":"25/12/19 13:49:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 12d706d757b9, 39431, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.198082Z","level":"info","event":"25/12/19 13:49:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 12d706d757b9, 39431, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.609310Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.619756Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.621213Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.623069Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.624441Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.625930Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.627595Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.629770Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.631709Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.633501Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.635056Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.636651Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.637990Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.640435Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.642077Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.643648Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.645229Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.646702Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.648311Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.651131Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.653063Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /executors/heapHistogram: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.655413Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /executors/heapHistogram/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.657825Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.693250Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.695544Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.709139Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.711795Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:06.722799Z","level":"info","event":"25/12/19 13:49:06 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:11.740980Z","level":"info","event":"25/12/19 13:49:11 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:12.152604Z","level":"info","event":"25/12/19 13:49:12 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:50698) with ID 1,  ResourceProfileId 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:12.351038Z","level":"info","event":"25/12/19 13:49:12 INFO BlockManagerMasterEndpoint: Registering block manager datanode-nodemanager-1:39491 with 434.4 MiB RAM, BlockManagerId(1, datanode-nodemanager-1, 39491, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:12.489416Z","level":"info","event":"25/12/19 13:49:12 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:12.493026Z","level":"info","event":"25/12/19 13:49:12 INFO SharedState: Warehouse path is 'file:/workspace/airflow/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:12.522647Z","level":"info","event":"25/12/19 13:49:12 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:12.526188Z","level":"info","event":"25/12/19 13:49:12 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:12.528840Z","level":"info","event":"25/12/19 13:49:12 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:12.531181Z","level":"info","event":"25/12/19 13:49:12 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:12.536117Z","level":"info","event":"25/12/19 13:49:12 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:16.292845Z","level":"info","event":"25/12/19 13:49:16 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:41844) with ID 2,  ResourceProfileId 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:16.407349Z","level":"info","event":"25/12/19 13:49:16 INFO InMemoryFileIndex: It took 714 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:16.452046Z","level":"info","event":"25/12/19 13:49:16 INFO BlockManagerMasterEndpoint: Registering block manager datanode-nodemanager-2:34089 with 434.4 MiB RAM, BlockManagerId(2, datanode-nodemanager-2, 34089, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:20.845479Z","level":"info","event":"25/12/19 13:49:20 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:20.880189Z","level":"info","event":"25/12/19 13:49:20 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:20.880906Z","level":"info","event":"25/12/19 13:49:20 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:20.883883Z","level":"info","event":"25/12/19 13:49:20 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:20.884043Z","level":"info","event":"25/12/19 13:49:20 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:20.892591Z","level":"info","event":"25/12/19 13:49:20 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:21.042088Z","level":"info","event":"25/12/19 13:49:21 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.9 KiB, free 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:21.162406Z","level":"info","event":"25/12/19 13:49:21 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 38.1 KiB, free 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:21.170447Z","level":"info","event":"25/12/19 13:49:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 12d706d757b9:39431 (size: 38.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:21.182747Z","level":"info","event":"25/12/19 13:49:21 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:21.231376Z","level":"info","event":"25/12/19 13:49:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:21.233881Z","level":"info","event":"25/12/19 13:49:21 INFO YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:21.300525Z","level":"info","event":"25/12/19 13:49:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (datanode-nodemanager-2, executor 2, partition 0, PROCESS_LOCAL, 9195 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:21.786623Z","level":"info","event":"25/12/19 13:49:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on datanode-nodemanager-2:34089 (size: 38.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:23.606407Z","level":"info","event":"25/12/19 13:49:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2332 ms on datanode-nodemanager-2 (executor 2) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:23.611224Z","level":"info","event":"25/12/19 13:49:23 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:23.628286Z","level":"info","event":"25/12/19 13:49:23 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.702 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:23.635158Z","level":"info","event":"25/12/19 13:49:23 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:23.636257Z","level":"info","event":"25/12/19 13:49:23 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:23.641686Z","level":"info","event":"25/12/19 13:49:23 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.795475 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:23.801272Z","level":"info","event":"25/12/19 13:49:23 INFO BlockManagerInfo: Removed broadcast_0_piece0 on datanode-nodemanager-2:34089 in memory (size: 38.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:23.804425Z","level":"info","event":"25/12/19 13:49:23 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 12d706d757b9:39431 in memory (size: 38.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:27.756684Z","level":"info","event":"25/12/19 13:49:27 INFO DataSourceStrategy: Pruning directories with: isnotnull(ingest_date#21),(ingest_date#21 >= 2025-09-07),(ingest_date#21 < 2025-09-14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:27.772934Z","level":"info","event":"25/12/19 13:49:27 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:27.776780Z","level":"info","event":"25/12/19 13:49:27 INFO FileSourceStrategy: Post-Scan Filters: (size(from_json(ArrayType(StructType(StructField(topic,StringType,true)),true), topics_json#19, Some(GMT)).topic, true) > 0),isnotnull(from_json(ArrayType(StructType(StructField(topic,StringType,true)),true), topics_json#19, Some(GMT)).topic)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:30.300320Z","level":"info","event":"25/12/19 13:49:30 INFO CodeGenerator: Code generated in 1370.204937 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:30.381363Z","level":"info","event":"25/12/19 13:49:30 INFO CodeGenerator: Code generated in 48.795494 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:30.426342Z","level":"info","event":"25/12/19 13:49:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 202.6 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:30.450734Z","level":"info","event":"25/12/19 13:49:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:30.451753Z","level":"info","event":"25/12/19 13:49:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 12d706d757b9:39431 (size: 35.9 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:30.455288Z","level":"info","event":"25/12/19 13:49:30 INFO SparkContext: Created broadcast 1 from showString at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:30.480484Z","level":"info","event":"25/12/19 13:49:30 INFO InMemoryFileIndex: Selected 7 partitions out of 16, pruned 56.25% partitions.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:30.522589Z","level":"info","event":"25/12/19 13:49:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:30.885566Z","level":"info","event":"25/12/19 13:49:30 INFO DAGScheduler: Registering RDD 8 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:30.899358Z","level":"info","event":"25/12/19 13:49:30 INFO DAGScheduler: Got map stage job 1 (showString at NativeMethodAccessorImpl.java:0) with 15 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:30.900553Z","level":"info","event":"25/12/19 13:49:30 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (showString at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:30.901847Z","level":"info","event":"25/12/19 13:49:30 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:30.906272Z","level":"info","event":"25/12/19 13:49:30 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:30.909579Z","level":"info","event":"25/12/19 13:49:30 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:31.003584Z","level":"info","event":"25/12/19 13:49:31 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 46.5 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:31.047004Z","level":"info","event":"25/12/19 13:49:31 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.3 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:31.050927Z","level":"info","event":"25/12/19 13:49:31 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 12d706d757b9:39431 (size: 20.3 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:31.052116Z","level":"info","event":"25/12/19 13:49:31 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:31.056462Z","level":"info","event":"25/12/19 13:49:31 INFO DAGScheduler: Submitting 15 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:31.056805Z","level":"info","event":"25/12/19 13:49:31 INFO YarnScheduler: Adding task set 1.0 with 15 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:31.095745Z","level":"info","event":"25/12/19 13:49:31 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (datanode-nodemanager-2, executor 2, partition 0, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:31.097017Z","level":"info","event":"25/12/19 13:49:31 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (datanode-nodemanager-1, executor 1, partition 1, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:31.271363Z","level":"info","event":"25/12/19 13:49:31 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on datanode-nodemanager-2:34089 (size: 20.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:31.659155Z","level":"info","event":"25/12/19 13:49:31 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on datanode-nodemanager-1:39491 (size: 20.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:33.949877Z","level":"info","event":"25/12/19 13:49:33 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on datanode-nodemanager-2:34089 (size: 35.9 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:35.113007Z","level":"info","event":"25/12/19 13:49:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on datanode-nodemanager-1:39491 (size: 35.9 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:37.335879Z","level":"info","event":"25/12/19 13:49:37 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (datanode-nodemanager-2, executor 2, partition 2, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:37.344098Z","level":"info","event":"25/12/19 13:49:37 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 6259 ms on datanode-nodemanager-2 (executor 2) (1/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:38.345797Z","level":"info","event":"25/12/19 13:49:38 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (datanode-nodemanager-2, executor 2, partition 3, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:38.346018Z","level":"info","event":"25/12/19 13:49:38 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 1017 ms on datanode-nodemanager-2 (executor 2) (2/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:39.554703Z","level":"info","event":"25/12/19 13:49:39 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (datanode-nodemanager-2, executor 2, partition 4, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:39.560820Z","level":"info","event":"25/12/19 13:49:39 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 1217 ms on datanode-nodemanager-2 (executor 2) (3/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:40.326871Z","level":"info","event":"25/12/19 13:49:40 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (datanode-nodemanager-1, executor 1, partition 5, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:40.333892Z","level":"info","event":"25/12/19 13:49:40 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 9235 ms on datanode-nodemanager-1 (executor 1) (4/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:40.408301Z","level":"info","event":"25/12/19 13:49:40 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (datanode-nodemanager-2, executor 2, partition 6, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:40.414838Z","level":"info","event":"25/12/19 13:49:40 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 863 ms on datanode-nodemanager-2 (executor 2) (5/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:41.379109Z","level":"info","event":"25/12/19 13:49:41 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (datanode-nodemanager-2, executor 2, partition 7, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:41.384880Z","level":"info","event":"25/12/19 13:49:41 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 976 ms on datanode-nodemanager-2 (executor 2) (6/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:41.775187Z","level":"info","event":"25/12/19 13:49:41 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9) (datanode-nodemanager-1, executor 1, partition 8, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:41.777108Z","level":"info","event":"25/12/19 13:49:41 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 1451 ms on datanode-nodemanager-1 (executor 1) (7/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:42.637145Z","level":"info","event":"25/12/19 13:49:42 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 10) (datanode-nodemanager-2, executor 2, partition 9, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:42.639248Z","level":"info","event":"25/12/19 13:49:42 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 1265 ms on datanode-nodemanager-2 (executor 2) (8/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:43.126488Z","level":"info","event":"25/12/19 13:49:43 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 11) (datanode-nodemanager-1, executor 1, partition 10, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:43.127846Z","level":"info","event":"25/12/19 13:49:43 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 1358 ms on datanode-nodemanager-1 (executor 1) (9/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:43.423743Z","level":"info","event":"25/12/19 13:49:43 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 12) (datanode-nodemanager-2, executor 2, partition 11, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:43.435770Z","level":"info","event":"25/12/19 13:49:43 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 10) in 795 ms on datanode-nodemanager-2 (executor 2) (10/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:43.919331Z","level":"info","event":"25/12/19 13:49:43 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 13) (datanode-nodemanager-1, executor 1, partition 12, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:43.919570Z","level":"info","event":"25/12/19 13:49:43 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 11) in 790 ms on datanode-nodemanager-1 (executor 1) (11/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:44.152670Z","level":"info","event":"25/12/19 13:49:44 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 14) (datanode-nodemanager-2, executor 2, partition 13, NODE_LOCAL, 14261 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:44.159895Z","level":"info","event":"25/12/19 13:49:44 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 12) in 734 ms on datanode-nodemanager-2 (executor 2) (12/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:44.776082Z","level":"info","event":"25/12/19 13:49:44 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 15) (datanode-nodemanager-1, executor 1, partition 14, NODE_LOCAL, 10418 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:44.777737Z","level":"info","event":"25/12/19 13:49:44 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 13) in 869 ms on datanode-nodemanager-1 (executor 1) (13/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:44.831857Z","level":"info","event":"25/12/19 13:49:44 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 14) in 681 ms on datanode-nodemanager-2 (executor 2) (14/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:44.991967Z","level":"info","event":"25/12/19 13:49:44 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 15) in 216 ms on datanode-nodemanager-1 (executor 1) (15/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:44.992187Z","level":"info","event":"25/12/19 13:49:44 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:44.995609Z","level":"info","event":"25/12/19 13:49:44 INFO DAGScheduler: ShuffleMapStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 14.062 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:44.996502Z","level":"info","event":"25/12/19 13:49:44 INFO DAGScheduler: looking for newly runnable stages","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:45.000380Z","level":"info","event":"25/12/19 13:49:44 INFO DAGScheduler: running: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:45.000603Z","level":"info","event":"25/12/19 13:49:44 INFO DAGScheduler: waiting: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:45.000669Z","level":"info","event":"25/12/19 13:49:44 INFO DAGScheduler: failed: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:45.098512Z","level":"info","event":"25/12/19 13:49:45 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:45.243862Z","level":"info","event":"25/12/19 13:49:45 INFO CodeGenerator: Code generated in 71.531582 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:45.318514Z","level":"info","event":"25/12/19 13:49:45 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:45.322382Z","level":"info","event":"25/12/19 13:49:45 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:45.322657Z","level":"info","event":"25/12/19 13:49:45 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:45.322769Z","level":"info","event":"25/12/19 13:49:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:45.324697Z","level":"info","event":"25/12/19 13:49:45 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:45.326367Z","level":"info","event":"25/12/19 13:49:45 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:45.347636Z","level":"info","event":"25/12/19 13:49:45 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 48.9 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:45.370446Z","level":"info","event":"25/12/19 13:49:45 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 21.7 KiB, free 434.0 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:45.374288Z","level":"info","event":"25/12/19 13:49:45 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 12d706d757b9:39431 (size: 21.7 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:45.375729Z","level":"info","event":"25/12/19 13:49:45 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:45.379441Z","level":"info","event":"25/12/19 13:49:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:45.379638Z","level":"info","event":"25/12/19 13:49:45 INFO YarnScheduler: Adding task set 3.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:45.379703Z","level":"info","event":"25/12/19 13:49:45 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 12d706d757b9:39431 in memory (size: 20.3 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:45.382820Z","level":"info","event":"25/12/19 13:49:45 INFO BlockManagerInfo: Removed broadcast_2_piece0 on datanode-nodemanager-2:34089 in memory (size: 20.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:45.388083Z","level":"info","event":"25/12/19 13:49:45 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 16) (datanode-nodemanager-1, executor 1, partition 0, NODE_LOCAL, 9010 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:45.389576Z","level":"info","event":"25/12/19 13:49:45 INFO BlockManagerInfo: Removed broadcast_2_piece0 on datanode-nodemanager-1:39491 in memory (size: 20.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:45.436191Z","level":"info","event":"25/12/19 13:49:45 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on datanode-nodemanager-1:39491 (size: 21.7 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:45.711324Z","level":"info","event":"25/12/19 13:49:45 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.4:50698","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:46.122567Z","level":"info","event":"25/12/19 13:49:46 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 16) in 737 ms on datanode-nodemanager-1 (executor 1) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:46.122860Z","level":"info","event":"25/12/19 13:49:46 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:46.127481Z","level":"info","event":"25/12/19 13:49:46 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.786 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:46.127746Z","level":"info","event":"25/12/19 13:49:46 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:46.127937Z","level":"info","event":"25/12/19 13:49:46 INFO YarnScheduler: Killing all running tasks in stage 3: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:46.128852Z","level":"info","event":"25/12/19 13:49:46 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.809802 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:47.066646Z","level":"info","event":"25/12/19 13:49:47 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 12d706d757b9:39431 in memory (size: 21.7 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:47.075239Z","level":"info","event":"25/12/19 13:49:47 INFO BlockManagerInfo: Removed broadcast_3_piece0 on datanode-nodemanager-1:39491 in memory (size: 21.7 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:49.197887Z","level":"info","event":"25/12/19 13:49:49 INFO CodeGenerator: Code generated in 25.158365 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:49.234762Z","level":"info","event":"+--------------------+--------------------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:49.235138Z","level":"info","event":"|            topic_id|          topic_name|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:49.235316Z","level":"info","event":"+--------------------+--------------------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:49.235412Z","level":"info","event":"| 7262183755669310342|                 IPO|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:49.235561Z","level":"info","event":"| 2398001789366731595|    Economy - Fiscal|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:49.235759Z","level":"info","event":"|-8706132297326582471|     Economy - Macro|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:49.235826Z","level":"info","event":"|-1496748679391501014|             Finance|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:49.235871Z","level":"info","event":"| -674034787253727345|            Earnings|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:49.235915Z","level":"info","event":"| 6330205094660838234|Real Estate & Con...|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:49.236040Z","level":"info","event":"| 2354747098425553887|  Retail & Wholesale|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:49.236124Z","level":"info","event":"| 2704355848778060929|Energy & Transpor...|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:49.236163Z","level":"info","event":"|  224347739280556027|          Technology|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:49.236190Z","level":"info","event":"| -448291436371370118|   Financial Markets|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:49.236214Z","level":"info","event":"| 5998011884746003305|  Economy - Monetary|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:49.236286Z","level":"info","event":"| -900445728666717830|Mergers & Acquisi...|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:49.236518Z","level":"info","event":"| 8225716003774577002|       Manufacturing|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:49.236687Z","level":"info","event":"| 8636157333299584077|       Life Sciences|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:49.236809Z","level":"info","event":"| 6135255710989356306|          Blockchain|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:49.236984Z","level":"info","event":"+--------------------+--------------------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:49.237046Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:46.684464Z","level":"info","event":"25/12/19 13:49:46 INFO BlockManagerInfo: Removed broadcast_1_piece0 on datanode-nodemanager-2:34089 in memory (size: 35.9 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:46.693025Z","level":"info","event":"25/12/19 13:49:46 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 12d706d757b9:39431 in memory (size: 35.9 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:46.693262Z","level":"info","event":"25/12/19 13:49:46 INFO BlockManagerInfo: Removed broadcast_1_piece0 on datanode-nodemanager-1:39491 in memory (size: 35.9 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:53.943523Z","level":"info","event":"25/12/19 13:49:53 INFO DataSourceStrategy: Pruning directories with: isnotnull(ingest_date#21),(ingest_date#21 >= 2025-09-07),(ingest_date#21 < 2025-09-14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:53.944971Z","level":"info","event":"25/12/19 13:49:53 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:53.946701Z","level":"info","event":"25/12/19 13:49:53 INFO FileSourceStrategy: Post-Scan Filters: (size(from_json(ArrayType(StructType(StructField(topic,StringType,true)),true), topics_json#19, Some(GMT)).topic, true) > 0),isnotnull(from_json(ArrayType(StructType(StructField(topic,StringType,true)),true), topics_json#19, Some(GMT)).topic)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:54.043916Z","level":"info","event":"25/12/19 13:49:54 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 202.6 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:54.074479Z","level":"info","event":"25/12/19 13:49:54 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:54.077702Z","level":"info","event":"25/12/19 13:49:54 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 12d706d757b9:39431 (size: 35.9 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:54.079377Z","level":"info","event":"25/12/19 13:49:54 INFO SparkContext: Created broadcast 4 from jdbc at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:54.084628Z","level":"info","event":"25/12/19 13:49:54 INFO InMemoryFileIndex: Selected 7 partitions out of 16, pruned 56.25% partitions.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:54.084839Z","level":"info","event":"25/12/19 13:49:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:54.148023Z","level":"info","event":"25/12/19 13:49:54 INFO DAGScheduler: Registering RDD 18 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:54.148404Z","level":"info","event":"25/12/19 13:49:54 INFO DAGScheduler: Got map stage job 3 (jdbc at NativeMethodAccessorImpl.java:0) with 15 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:54.148493Z","level":"info","event":"25/12/19 13:49:54 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (jdbc at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:54.148544Z","level":"info","event":"25/12/19 13:49:54 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:54.155832Z","level":"info","event":"25/12/19 13:49:54 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:54.157703Z","level":"info","event":"25/12/19 13:49:54 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[18] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:54.183969Z","level":"info","event":"25/12/19 13:49:54 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 46.5 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:54.188721Z","level":"info","event":"25/12/19 13:49:54 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 20.3 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:54.191434Z","level":"info","event":"25/12/19 13:49:54 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 12d706d757b9:39431 (size: 20.3 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:54.192766Z","level":"info","event":"25/12/19 13:49:54 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:54.196416Z","level":"info","event":"25/12/19 13:49:54 INFO DAGScheduler: Submitting 15 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[18] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:54.197095Z","level":"info","event":"25/12/19 13:49:54 INFO YarnScheduler: Adding task set 4.0 with 15 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:54.201148Z","level":"info","event":"25/12/19 13:49:54 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 17) (datanode-nodemanager-1, executor 1, partition 0, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:54.201412Z","level":"info","event":"25/12/19 13:49:54 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 18) (datanode-nodemanager-2, executor 2, partition 1, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:54.250659Z","level":"info","event":"25/12/19 13:49:54 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on datanode-nodemanager-1:39491 (size: 20.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:54.253742Z","level":"info","event":"25/12/19 13:49:54 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on datanode-nodemanager-2:34089 (size: 20.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:54.332259Z","level":"info","event":"25/12/19 13:49:54 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on datanode-nodemanager-2:34089 (size: 35.9 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:54.338849Z","level":"info","event":"25/12/19 13:49:54 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on datanode-nodemanager-1:39491 (size: 35.9 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:55.313782Z","level":"info","event":"25/12/19 13:49:55 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 19) (datanode-nodemanager-2, executor 2, partition 2, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:55.321089Z","level":"info","event":"25/12/19 13:49:55 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 18) in 1118 ms on datanode-nodemanager-2 (executor 2) (1/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:55.411532Z","level":"info","event":"25/12/19 13:49:55 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 20) (datanode-nodemanager-1, executor 1, partition 3, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:55.417828Z","level":"info","event":"25/12/19 13:49:55 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 17) in 1213 ms on datanode-nodemanager-1 (executor 1) (2/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:55.997035Z","level":"info","event":"25/12/19 13:49:55 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 21) (datanode-nodemanager-2, executor 2, partition 4, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:55.998036Z","level":"info","event":"25/12/19 13:49:55 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 19) in 686 ms on datanode-nodemanager-2 (executor 2) (3/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:56.177141Z","level":"info","event":"25/12/19 13:49:56 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 22) (datanode-nodemanager-1, executor 1, partition 5, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:56.177408Z","level":"info","event":"25/12/19 13:49:56 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 20) in 767 ms on datanode-nodemanager-1 (executor 1) (4/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:56.789457Z","level":"info","event":"25/12/19 13:49:56 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 23) (datanode-nodemanager-2, executor 2, partition 6, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:56.794440Z","level":"info","event":"25/12/19 13:49:56 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 21) in 799 ms on datanode-nodemanager-2 (executor 2) (5/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:57.052601Z","level":"info","event":"25/12/19 13:49:57 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 24) (datanode-nodemanager-1, executor 1, partition 7, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:57.054203Z","level":"info","event":"25/12/19 13:49:57 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 22) in 880 ms on datanode-nodemanager-1 (executor 1) (6/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:57.510643Z","level":"info","event":"25/12/19 13:49:57 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 25) (datanode-nodemanager-2, executor 2, partition 8, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:57.511563Z","level":"info","event":"25/12/19 13:49:57 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 23) in 723 ms on datanode-nodemanager-2 (executor 2) (7/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:57.833157Z","level":"info","event":"25/12/19 13:49:57 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 26) (datanode-nodemanager-1, executor 1, partition 9, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:57.833387Z","level":"info","event":"25/12/19 13:49:57 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 24) in 781 ms on datanode-nodemanager-1 (executor 1) (8/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:58.150644Z","level":"info","event":"25/12/19 13:49:58 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 27) (datanode-nodemanager-2, executor 2, partition 10, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:58.151751Z","level":"info","event":"25/12/19 13:49:58 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 25) in 642 ms on datanode-nodemanager-2 (executor 2) (9/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:58.612540Z","level":"info","event":"25/12/19 13:49:58 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 28) (datanode-nodemanager-1, executor 1, partition 11, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:58.612783Z","level":"info","event":"25/12/19 13:49:58 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 26) in 785 ms on datanode-nodemanager-1 (executor 1) (10/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:58.859179Z","level":"info","event":"25/12/19 13:49:58 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 29) (datanode-nodemanager-2, executor 2, partition 12, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:58.860162Z","level":"info","event":"25/12/19 13:49:58 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 27) in 710 ms on datanode-nodemanager-2 (executor 2) (11/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:59.294287Z","level":"info","event":"25/12/19 13:49:59 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 30) (datanode-nodemanager-1, executor 1, partition 13, NODE_LOCAL, 14261 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:59.302897Z","level":"info","event":"25/12/19 13:49:59 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 28) in 689 ms on datanode-nodemanager-1 (executor 1) (12/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:59.502430Z","level":"info","event":"25/12/19 13:49:59 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 31) (datanode-nodemanager-2, executor 2, partition 14, NODE_LOCAL, 10418 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:59.503110Z","level":"info","event":"25/12/19 13:49:59 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 29) in 644 ms on datanode-nodemanager-2 (executor 2) (13/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:59.721721Z","level":"info","event":"25/12/19 13:49:59 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 31) in 219 ms on datanode-nodemanager-2 (executor 2) (14/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:59.909410Z","level":"info","event":"25/12/19 13:49:59 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 30) in 618 ms on datanode-nodemanager-1 (executor 1) (15/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:59.909668Z","level":"info","event":"25/12/19 13:49:59 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:59.911397Z","level":"info","event":"25/12/19 13:49:59 INFO DAGScheduler: ShuffleMapStage 4 (jdbc at NativeMethodAccessorImpl.java:0) finished in 5.741 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:59.911610Z","level":"info","event":"25/12/19 13:49:59 INFO DAGScheduler: looking for newly runnable stages","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:59.911748Z","level":"info","event":"25/12/19 13:49:59 INFO DAGScheduler: running: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:59.911835Z","level":"info","event":"25/12/19 13:49:59 INFO DAGScheduler: waiting: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:59.911890Z","level":"info","event":"25/12/19 13:49:59 INFO DAGScheduler: failed: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:49:59.918934Z","level":"info","event":"25/12/19 13:49:59 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:00.024543Z","level":"info","event":"25/12/19 13:50:00 INFO CodeGenerator: Code generated in 81.129526 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:00.151274Z","level":"info","event":"25/12/19 13:50:00 INFO SparkContext: Starting job: jdbc at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:00.153148Z","level":"info","event":"25/12/19 13:50:00 INFO DAGScheduler: Got job 4 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:00.153377Z","level":"info","event":"25/12/19 13:50:00 INFO DAGScheduler: Final stage: ResultStage 6 (jdbc at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:00.153450Z","level":"info","event":"25/12/19 13:50:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:00.157005Z","level":"info","event":"25/12/19 13:50:00 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:00.168349Z","level":"info","event":"25/12/19 13:50:00 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[23] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:00.174976Z","level":"info","event":"25/12/19 13:50:00 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 12d706d757b9:39431 in memory (size: 20.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:00.175625Z","level":"info","event":"25/12/19 13:50:00 INFO BlockManagerInfo: Removed broadcast_5_piece0 on datanode-nodemanager-1:39491 in memory (size: 20.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:00.180909Z","level":"info","event":"25/12/19 13:50:00 INFO BlockManagerInfo: Removed broadcast_5_piece0 on datanode-nodemanager-2:34089 in memory (size: 20.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:00.200491Z","level":"info","event":"25/12/19 13:50:00 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 56.5 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:00.216791Z","level":"info","event":"25/12/19 13:50:00 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 25.0 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:00.219350Z","level":"info","event":"25/12/19 13:50:00 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 12d706d757b9:39431 (size: 25.0 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:00.219800Z","level":"info","event":"25/12/19 13:50:00 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:00.220721Z","level":"info","event":"25/12/19 13:50:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[23] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:00.220881Z","level":"info","event":"25/12/19 13:50:00 INFO YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:00.223775Z","level":"info","event":"25/12/19 13:50:00 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 32) (datanode-nodemanager-1, executor 1, partition 0, NODE_LOCAL, 9010 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:00.254397Z","level":"info","event":"25/12/19 13:50:00 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on datanode-nodemanager-1:39491 (size: 25.0 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:00.633074Z","level":"info","event":"25/12/19 13:50:00 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.18.0.4:50698","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.161333Z","level":"info","event":"25/12/19 13:50:03 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 32) in 2938 ms on datanode-nodemanager-1 (executor 1) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.161562Z","level":"info","event":"25/12/19 13:50:03 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.162825Z","level":"info","event":"25/12/19 13:50:03 INFO DAGScheduler: ResultStage 6 (jdbc at NativeMethodAccessorImpl.java:0) finished in 2.987 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.163264Z","level":"info","event":"25/12/19 13:50:03 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.163421Z","level":"info","event":"25/12/19 13:50:03 INFO YarnScheduler: Killing all running tasks in stage 6: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.163746Z","level":"info","event":"25/12/19 13:50:03 INFO DAGScheduler: Job 4 finished: jdbc at NativeMethodAccessorImpl.java:0, took 3.012210 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.606335Z","level":"info","event":"25/12/19 13:50:03 INFO DataSourceStrategy: Pruning directories with: isnotnull(ingest_date#21),(ingest_date#21 >= 2025-09-07),(ingest_date#21 < 2025-09-14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.607332Z","level":"info","event":"25/12/19 13:50:03 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.607487Z","level":"info","event":"25/12/19 13:50:03 INFO FileSourceStrategy: Post-Scan Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.743417Z","level":"info","event":"25/12/19 13:50:03 INFO CodeGenerator: Code generated in 94.586014 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.749245Z","level":"info","event":"25/12/19 13:50:03 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 202.5 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.774980Z","level":"info","event":"25/12/19 13:50:03 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 12d706d757b9:39431 in memory (size: 25.0 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.784611Z","level":"info","event":"25/12/19 13:50:03 INFO BlockManagerInfo: Removed broadcast_6_piece0 on datanode-nodemanager-1:39491 in memory (size: 25.0 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.789117Z","level":"info","event":"25/12/19 13:50:03 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 35.8 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.791323Z","level":"info","event":"25/12/19 13:50:03 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 12d706d757b9:39431 (size: 35.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.794568Z","level":"info","event":"25/12/19 13:50:03 INFO SparkContext: Created broadcast 7 from showString at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.801354Z","level":"info","event":"25/12/19 13:50:03 INFO InMemoryFileIndex: Selected 7 partitions out of 16, pruned 56.25% partitions.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.801546Z","level":"info","event":"25/12/19 13:50:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.820975Z","level":"info","event":"25/12/19 13:50:03 INFO DAGScheduler: Registering RDD 27 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 2","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.821524Z","level":"info","event":"25/12/19 13:50:03 INFO DAGScheduler: Got map stage job 5 (showString at NativeMethodAccessorImpl.java:0) with 15 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.821669Z","level":"info","event":"25/12/19 13:50:03 INFO DAGScheduler: Final stage: ShuffleMapStage 7 (showString at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.821778Z","level":"info","event":"25/12/19 13:50:03 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.824433Z","level":"info","event":"25/12/19 13:50:03 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.825970Z","level":"info","event":"25/12/19 13:50:03 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[27] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.854494Z","level":"info","event":"25/12/19 13:50:03 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 37.1 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.857149Z","level":"info","event":"25/12/19 13:50:03 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.858892Z","level":"info","event":"25/12/19 13:50:03 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 12d706d757b9:39431 (size: 16.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.859639Z","level":"info","event":"25/12/19 13:50:03 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.860863Z","level":"info","event":"25/12/19 13:50:03 INFO DAGScheduler: Submitting 15 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[27] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.861259Z","level":"info","event":"25/12/19 13:50:03 INFO YarnScheduler: Adding task set 7.0 with 15 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.863672Z","level":"info","event":"25/12/19 13:50:03 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 33) (datanode-nodemanager-1, executor 1, partition 0, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.864338Z","level":"info","event":"25/12/19 13:50:03 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 34) (datanode-nodemanager-2, executor 2, partition 1, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.888553Z","level":"info","event":"25/12/19 13:50:03 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on datanode-nodemanager-2:34089 (size: 16.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:03.894482Z","level":"info","event":"25/12/19 13:50:03 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on datanode-nodemanager-1:39491 (size: 16.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:04.150149Z","level":"info","event":"25/12/19 13:50:04 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on datanode-nodemanager-1:39491 (size: 35.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:04.164675Z","level":"info","event":"25/12/19 13:50:04 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on datanode-nodemanager-2:34089 (size: 35.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:04.943570Z","level":"info","event":"25/12/19 13:50:04 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 35) (datanode-nodemanager-2, executor 2, partition 2, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:04.944963Z","level":"info","event":"25/12/19 13:50:04 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 34) in 1081 ms on datanode-nodemanager-2 (executor 2) (1/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:04.996244Z","level":"info","event":"25/12/19 13:50:04 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 36) (datanode-nodemanager-1, executor 1, partition 3, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:05.003806Z","level":"info","event":"25/12/19 13:50:05 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 33) in 1140 ms on datanode-nodemanager-1 (executor 1) (2/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:05.578344Z","level":"info","event":"25/12/19 13:50:05 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 37) (datanode-nodemanager-2, executor 2, partition 4, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:05.579047Z","level":"info","event":"25/12/19 13:50:05 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 35) in 633 ms on datanode-nodemanager-2 (executor 2) (3/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:05.710366Z","level":"info","event":"25/12/19 13:50:05 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 38) (datanode-nodemanager-1, executor 1, partition 5, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:05.711492Z","level":"info","event":"25/12/19 13:50:05 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 36) in 718 ms on datanode-nodemanager-1 (executor 1) (4/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:05.994691Z","level":"info","event":"25/12/19 13:50:05 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 39) (datanode-nodemanager-2, executor 2, partition 6, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:05.997921Z","level":"info","event":"25/12/19 13:50:05 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 37) in 427 ms on datanode-nodemanager-2 (executor 2) (5/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:06.126106Z","level":"info","event":"25/12/19 13:50:06 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 40) (datanode-nodemanager-1, executor 1, partition 7, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:06.127104Z","level":"info","event":"25/12/19 13:50:06 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 38) in 418 ms on datanode-nodemanager-1 (executor 1) (6/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:06.468736Z","level":"info","event":"25/12/19 13:50:06 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 41) (datanode-nodemanager-2, executor 2, partition 8, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:06.470830Z","level":"info","event":"25/12/19 13:50:06 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 39) in 476 ms on datanode-nodemanager-2 (executor 2) (7/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:06.586360Z","level":"info","event":"25/12/19 13:50:06 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 42) (datanode-nodemanager-1, executor 1, partition 9, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:06.586560Z","level":"info","event":"25/12/19 13:50:06 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 40) in 460 ms on datanode-nodemanager-1 (executor 1) (8/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:06.901181Z","level":"info","event":"25/12/19 13:50:06 INFO TaskSetManager: Starting task 10.0 in stage 7.0 (TID 43) (datanode-nodemanager-2, executor 2, partition 10, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:06.901456Z","level":"info","event":"25/12/19 13:50:06 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 41) in 433 ms on datanode-nodemanager-2 (executor 2) (9/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:07.231903Z","level":"info","event":"25/12/19 13:50:07 INFO TaskSetManager: Starting task 11.0 in stage 7.0 (TID 44) (datanode-nodemanager-1, executor 1, partition 11, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:07.232574Z","level":"info","event":"25/12/19 13:50:07 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 42) in 649 ms on datanode-nodemanager-1 (executor 1) (10/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:07.502006Z","level":"info","event":"25/12/19 13:50:07 INFO TaskSetManager: Finished task 10.0 in stage 7.0 (TID 43) in 611 ms on datanode-nodemanager-2 (executor 2) (11/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:07.503789Z","level":"info","event":"25/12/19 13:50:07 INFO TaskSetManager: Starting task 12.0 in stage 7.0 (TID 45) (datanode-nodemanager-2, executor 2, partition 12, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:07.768386Z","level":"info","event":"25/12/19 13:50:07 INFO TaskSetManager: Starting task 13.0 in stage 7.0 (TID 46) (datanode-nodemanager-1, executor 1, partition 13, NODE_LOCAL, 14261 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:07.769829Z","level":"info","event":"25/12/19 13:50:07 INFO TaskSetManager: Finished task 11.0 in stage 7.0 (TID 44) in 539 ms on datanode-nodemanager-1 (executor 1) (12/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.057666Z","level":"info","event":"25/12/19 13:50:08 INFO TaskSetManager: Starting task 14.0 in stage 7.0 (TID 47) (datanode-nodemanager-2, executor 2, partition 14, NODE_LOCAL, 10418 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.061777Z","level":"info","event":"25/12/19 13:50:08 INFO TaskSetManager: Finished task 12.0 in stage 7.0 (TID 45) in 559 ms on datanode-nodemanager-2 (executor 2) (13/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.272791Z","level":"info","event":"25/12/19 13:50:08 INFO TaskSetManager: Finished task 14.0 in stage 7.0 (TID 47) in 216 ms on datanode-nodemanager-2 (executor 2) (14/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.462922Z","level":"info","event":"25/12/19 13:50:08 INFO TaskSetManager: Finished task 13.0 in stage 7.0 (TID 46) in 695 ms on datanode-nodemanager-1 (executor 1) (15/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.463150Z","level":"info","event":"25/12/19 13:50:08 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.465833Z","level":"info","event":"25/12/19 13:50:08 INFO DAGScheduler: ShuffleMapStage 7 (showString at NativeMethodAccessorImpl.java:0) finished in 4.635 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.466065Z","level":"info","event":"25/12/19 13:50:08 INFO DAGScheduler: looking for newly runnable stages","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.466155Z","level":"info","event":"25/12/19 13:50:08 INFO DAGScheduler: running: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.466208Z","level":"info","event":"25/12/19 13:50:08 INFO DAGScheduler: waiting: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.466256Z","level":"info","event":"25/12/19 13:50:08 INFO DAGScheduler: failed: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.491886Z","level":"info","event":"25/12/19 13:50:08 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.640524Z","level":"info","event":"25/12/19 13:50:08 INFO CodeGenerator: Code generated in 108.677283 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.712902Z","level":"info","event":"25/12/19 13:50:08 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.720397Z","level":"info","event":"25/12/19 13:50:08 INFO DAGScheduler: Got job 6 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.720682Z","level":"info","event":"25/12/19 13:50:08 INFO DAGScheduler: Final stage: ResultStage 9 (showString at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.720774Z","level":"info","event":"25/12/19 13:50:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.722432Z","level":"info","event":"25/12/19 13:50:08 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.725498Z","level":"info","event":"25/12/19 13:50:08 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[30] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.740346Z","level":"info","event":"25/12/19 13:50:08 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 47.5 KiB, free 433.8 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.769063Z","level":"info","event":"25/12/19 13:50:08 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 20.8 KiB, free 433.8 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.775544Z","level":"info","event":"25/12/19 13:50:08 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 12d706d757b9:39431 (size: 20.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.776659Z","level":"info","event":"25/12/19 13:50:08 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.777779Z","level":"info","event":"25/12/19 13:50:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[30] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.780228Z","level":"info","event":"25/12/19 13:50:08 INFO YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.781968Z","level":"info","event":"25/12/19 13:50:08 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 48) (datanode-nodemanager-1, executor 1, partition 0, NODE_LOCAL, 9010 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.786089Z","level":"info","event":"25/12/19 13:50:08 INFO BlockManagerInfo: Removed broadcast_8_piece0 on datanode-nodemanager-1:39491 in memory (size: 16.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.799564Z","level":"info","event":"25/12/19 13:50:08 INFO BlockManagerInfo: Removed broadcast_8_piece0 on datanode-nodemanager-2:34089 in memory (size: 16.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.816241Z","level":"info","event":"25/12/19 13:50:08 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 12d706d757b9:39431 in memory (size: 16.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.861365Z","level":"info","event":"25/12/19 13:50:08 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on datanode-nodemanager-1:39491 (size: 20.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.911545Z","level":"info","event":"25/12/19 13:50:08 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 12d706d757b9:39431 in memory (size: 35.9 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:08.957949Z","level":"info","event":"25/12/19 13:50:08 INFO BlockManagerInfo: Removed broadcast_4_piece0 on datanode-nodemanager-1:39491 in memory (size: 35.9 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.007653Z","level":"info","event":"25/12/19 13:50:09 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.18.0.4:50698","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.011339Z","level":"info","event":"25/12/19 13:50:09 INFO BlockManagerInfo: Removed broadcast_4_piece0 on datanode-nodemanager-2:34089 in memory (size: 35.9 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.397632Z","level":"info","event":"25/12/19 13:50:09 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 48) in 613 ms on datanode-nodemanager-1 (executor 1) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.398106Z","level":"info","event":"25/12/19 13:50:09 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.403964Z","level":"info","event":"25/12/19 13:50:09 INFO DAGScheduler: ResultStage 9 (showString at NativeMethodAccessorImpl.java:0) finished in 0.669 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.404383Z","level":"info","event":"25/12/19 13:50:09 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.404559Z","level":"info","event":"25/12/19 13:50:09 INFO YarnScheduler: Killing all running tasks in stage 9: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.404609Z","level":"info","event":"25/12/19 13:50:09 INFO DAGScheduler: Job 6 finished: showString at NativeMethodAccessorImpl.java:0, took 0.688265 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.456302Z","level":"info","event":"25/12/19 13:50:09 INFO CodeGenerator: Code generated in 35.177566 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.466716Z","level":"info","event":"+-------------------+----------+----------------+----------+------------+---------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.467077Z","level":"info","event":"|            time_id| time_date|time_day_of_week|time_month|time_quarter|time_year|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.467199Z","level":"info","event":"+-------------------+----------+----------------+----------+------------+---------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.467256Z","level":"info","event":"| 798927963440825024|2025-09-10|               4|         9|           3|     2025|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.467305Z","level":"info","event":"|8893285539898621379|2025-09-11|               5|         9|           3|     2025|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.467348Z","level":"info","event":"|-222147908674804431|2025-09-07|               1|         9|           3|     2025|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.467389Z","level":"info","event":"|7150283814656891700|2025-09-09|               3|         9|           3|     2025|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.467431Z","level":"info","event":"|6948219474172485782|2025-09-08|               2|         9|           3|     2025|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.467476Z","level":"info","event":"|-381703233359023787|2025-09-12|               6|         9|           3|     2025|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.467529Z","level":"info","event":"|7181520199071383846|2025-09-13|               7|         9|           3|     2025|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.467580Z","level":"info","event":"+-------------------+----------+----------------+----------+------------+---------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.467637Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.752404Z","level":"info","event":"25/12/19 13:50:09 INFO DataSourceStrategy: Pruning directories with: isnotnull(ingest_date#21),(ingest_date#21 >= 2025-09-07),(ingest_date#21 < 2025-09-14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.754930Z","level":"info","event":"25/12/19 13:50:09 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.755258Z","level":"info","event":"25/12/19 13:50:09 INFO FileSourceStrategy: Post-Scan Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.818761Z","level":"info","event":"25/12/19 13:50:09 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 202.5 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.854851Z","level":"info","event":"25/12/19 13:50:09 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 35.8 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.859978Z","level":"info","event":"25/12/19 13:50:09 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 12d706d757b9:39431 (size: 35.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.866994Z","level":"info","event":"25/12/19 13:50:09 INFO SparkContext: Created broadcast 10 from jdbc at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.867537Z","level":"info","event":"25/12/19 13:50:09 INFO BlockManagerInfo: Removed broadcast_9_piece0 on datanode-nodemanager-1:39491 in memory (size: 20.8 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.871026Z","level":"info","event":"25/12/19 13:50:09 INFO InMemoryFileIndex: Selected 7 partitions out of 16, pruned 56.25% partitions.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.883698Z","level":"info","event":"25/12/19 13:50:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.883904Z","level":"info","event":"25/12/19 13:50:09 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 12d706d757b9:39431 in memory (size: 20.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.913438Z","level":"info","event":"25/12/19 13:50:09 INFO DAGScheduler: Registering RDD 34 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 3","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.914305Z","level":"info","event":"25/12/19 13:50:09 INFO DAGScheduler: Got map stage job 7 (jdbc at NativeMethodAccessorImpl.java:0) with 15 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.914622Z","level":"info","event":"25/12/19 13:50:09 INFO DAGScheduler: Final stage: ShuffleMapStage 10 (jdbc at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.914880Z","level":"info","event":"25/12/19 13:50:09 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.924642Z","level":"info","event":"25/12/19 13:50:09 INFO BlockManagerInfo: Removed broadcast_7_piece0 on datanode-nodemanager-2:34089 in memory (size: 35.8 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.925734Z","level":"info","event":"25/12/19 13:50:09 INFO BlockManagerInfo: Removed broadcast_7_piece0 on datanode-nodemanager-1:39491 in memory (size: 35.8 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.929579Z","level":"info","event":"25/12/19 13:50:09 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 12d706d757b9:39431 in memory (size: 35.8 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.935272Z","level":"info","event":"25/12/19 13:50:09 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.938376Z","level":"info","event":"25/12/19 13:50:09 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[34] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.949268Z","level":"info","event":"25/12/19 13:50:09 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 37.1 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.953648Z","level":"info","event":"25/12/19 13:50:09 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.956131Z","level":"info","event":"25/12/19 13:50:09 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 12d706d757b9:39431 (size: 16.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.958133Z","level":"info","event":"25/12/19 13:50:09 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.959318Z","level":"info","event":"25/12/19 13:50:09 INFO DAGScheduler: Submitting 15 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[34] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.959534Z","level":"info","event":"25/12/19 13:50:09 INFO YarnScheduler: Adding task set 10.0 with 15 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.962599Z","level":"info","event":"25/12/19 13:50:09 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 49) (datanode-nodemanager-1, executor 1, partition 0, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:09.963224Z","level":"info","event":"25/12/19 13:50:09 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 50) (datanode-nodemanager-2, executor 2, partition 1, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:10.007838Z","level":"info","event":"25/12/19 13:50:10 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on datanode-nodemanager-1:39491 (size: 16.6 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:10.032115Z","level":"info","event":"25/12/19 13:50:10 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on datanode-nodemanager-2:34089 (size: 16.6 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:10.087509Z","level":"info","event":"25/12/19 13:50:10 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on datanode-nodemanager-2:34089 (size: 35.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:10.099757Z","level":"info","event":"25/12/19 13:50:10 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on datanode-nodemanager-1:39491 (size: 35.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:10.546011Z","level":"info","event":"25/12/19 13:50:10 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 51) (datanode-nodemanager-2, executor 2, partition 2, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:10.547409Z","level":"info","event":"25/12/19 13:50:10 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 50) in 584 ms on datanode-nodemanager-2 (executor 2) (1/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:10.599864Z","level":"info","event":"25/12/19 13:50:10 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 52) (datanode-nodemanager-1, executor 1, partition 3, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:10.602995Z","level":"info","event":"25/12/19 13:50:10 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 49) in 641 ms on datanode-nodemanager-1 (executor 1) (2/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:10.972960Z","level":"info","event":"25/12/19 13:50:10 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 53) (datanode-nodemanager-2, executor 2, partition 4, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:10.974563Z","level":"info","event":"25/12/19 13:50:10 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 51) in 429 ms on datanode-nodemanager-2 (executor 2) (3/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:11.058925Z","level":"info","event":"25/12/19 13:50:11 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 54) (datanode-nodemanager-1, executor 1, partition 5, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:11.062745Z","level":"info","event":"25/12/19 13:50:11 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 52) in 464 ms on datanode-nodemanager-1 (executor 1) (4/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:11.455921Z","level":"info","event":"25/12/19 13:50:11 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 55) (datanode-nodemanager-2, executor 2, partition 6, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:11.457262Z","level":"info","event":"25/12/19 13:50:11 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 53) in 484 ms on datanode-nodemanager-2 (executor 2) (5/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:11.506792Z","level":"info","event":"25/12/19 13:50:11 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 56) (datanode-nodemanager-1, executor 1, partition 7, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:11.507017Z","level":"info","event":"25/12/19 13:50:11 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 54) in 451 ms on datanode-nodemanager-1 (executor 1) (6/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:11.856478Z","level":"info","event":"25/12/19 13:50:11 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 57) (datanode-nodemanager-2, executor 2, partition 8, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:11.856828Z","level":"info","event":"25/12/19 13:50:11 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 55) in 399 ms on datanode-nodemanager-2 (executor 2) (7/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:12.006370Z","level":"info","event":"25/12/19 13:50:12 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 58) (datanode-nodemanager-1, executor 1, partition 9, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:12.006637Z","level":"info","event":"25/12/19 13:50:12 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 56) in 498 ms on datanode-nodemanager-1 (executor 1) (8/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:12.493453Z","level":"info","event":"25/12/19 13:50:12 INFO TaskSetManager: Starting task 10.0 in stage 10.0 (TID 59) (datanode-nodemanager-2, executor 2, partition 10, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:12.494972Z","level":"info","event":"25/12/19 13:50:12 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 57) in 643 ms on datanode-nodemanager-2 (executor 2) (9/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:12.714472Z","level":"info","event":"25/12/19 13:50:12 INFO TaskSetManager: Starting task 11.0 in stage 10.0 (TID 60) (datanode-nodemanager-1, executor 1, partition 11, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:12.717082Z","level":"info","event":"25/12/19 13:50:12 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 58) in 714 ms on datanode-nodemanager-1 (executor 1) (10/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:13.054663Z","level":"info","event":"25/12/19 13:50:13 INFO TaskSetManager: Starting task 12.0 in stage 10.0 (TID 61) (datanode-nodemanager-2, executor 2, partition 12, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:13.055988Z","level":"info","event":"25/12/19 13:50:13 INFO TaskSetManager: Finished task 10.0 in stage 10.0 (TID 59) in 563 ms on datanode-nodemanager-2 (executor 2) (11/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:13.211899Z","level":"info","event":"25/12/19 13:50:13 INFO TaskSetManager: Starting task 13.0 in stage 10.0 (TID 62) (datanode-nodemanager-1, executor 1, partition 13, NODE_LOCAL, 14261 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:13.214209Z","level":"info","event":"25/12/19 13:50:13 INFO TaskSetManager: Finished task 11.0 in stage 10.0 (TID 60) in 503 ms on datanode-nodemanager-1 (executor 1) (12/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:13.544395Z","level":"info","event":"25/12/19 13:50:13 INFO TaskSetManager: Starting task 14.0 in stage 10.0 (TID 63) (datanode-nodemanager-2, executor 2, partition 14, NODE_LOCAL, 10418 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:13.545479Z","level":"info","event":"25/12/19 13:50:13 INFO TaskSetManager: Finished task 12.0 in stage 10.0 (TID 61) in 491 ms on datanode-nodemanager-2 (executor 2) (13/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:13.687673Z","level":"info","event":"25/12/19 13:50:13 INFO TaskSetManager: Finished task 14.0 in stage 10.0 (TID 63) in 145 ms on datanode-nodemanager-2 (executor 2) (14/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:13.858786Z","level":"info","event":"25/12/19 13:50:13 INFO TaskSetManager: Finished task 13.0 in stage 10.0 (TID 62) in 648 ms on datanode-nodemanager-1 (executor 1) (15/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:13.859061Z","level":"info","event":"25/12/19 13:50:13 INFO YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:13.863107Z","level":"info","event":"25/12/19 13:50:13 INFO DAGScheduler: ShuffleMapStage 10 (jdbc at NativeMethodAccessorImpl.java:0) finished in 3.919 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:13.863466Z","level":"info","event":"25/12/19 13:50:13 INFO DAGScheduler: looking for newly runnable stages","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:13.863606Z","level":"info","event":"25/12/19 13:50:13 INFO DAGScheduler: running: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:13.863675Z","level":"info","event":"25/12/19 13:50:13 INFO DAGScheduler: waiting: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:13.863745Z","level":"info","event":"25/12/19 13:50:13 INFO DAGScheduler: failed: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:13.886177Z","level":"info","event":"25/12/19 13:50:13 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:14.002753Z","level":"info","event":"25/12/19 13:50:14 INFO CodeGenerator: Code generated in 87.391942 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:14.073237Z","level":"info","event":"25/12/19 13:50:14 INFO SparkContext: Starting job: jdbc at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:14.075350Z","level":"info","event":"25/12/19 13:50:14 INFO DAGScheduler: Got job 8 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:14.077588Z","level":"info","event":"25/12/19 13:50:14 INFO DAGScheduler: Final stage: ResultStage 12 (jdbc at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:14.078369Z","level":"info","event":"25/12/19 13:50:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:14.078490Z","level":"info","event":"25/12/19 13:50:14 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:14.080561Z","level":"info","event":"25/12/19 13:50:14 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[39] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:14.114016Z","level":"info","event":"25/12/19 13:50:14 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 54.3 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:14.136515Z","level":"info","event":"25/12/19 13:50:14 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 23.8 KiB, free 434.0 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:14.140115Z","level":"info","event":"25/12/19 13:50:14 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 12d706d757b9:39431 (size: 23.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:14.141761Z","level":"info","event":"25/12/19 13:50:14 INFO BlockManagerInfo: Removed broadcast_11_piece0 on datanode-nodemanager-2:34089 in memory (size: 16.6 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:14.142174Z","level":"info","event":"25/12/19 13:50:14 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:14.144688Z","level":"info","event":"25/12/19 13:50:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[39] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:14.144846Z","level":"info","event":"25/12/19 13:50:14 INFO YarnScheduler: Adding task set 12.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:14.148405Z","level":"info","event":"25/12/19 13:50:14 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 12d706d757b9:39431 in memory (size: 16.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:14.148678Z","level":"info","event":"25/12/19 13:50:14 INFO BlockManagerInfo: Removed broadcast_11_piece0 on datanode-nodemanager-1:39491 in memory (size: 16.6 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:14.152272Z","level":"info","event":"25/12/19 13:50:14 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 64) (datanode-nodemanager-2, executor 2, partition 0, NODE_LOCAL, 9010 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:14.194040Z","level":"info","event":"25/12/19 13:50:14 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on datanode-nodemanager-2:34089 (size: 23.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:14.533386Z","level":"info","event":"25/12/19 13:50:14 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.18.0.3:41844","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:16.251013Z","level":"info","event":"25/12/19 13:50:16 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 64) in 2100 ms on datanode-nodemanager-2 (executor 2) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:16.251400Z","level":"info","event":"25/12/19 13:50:16 INFO YarnScheduler: Removed TaskSet 12.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:16.251564Z","level":"info","event":"25/12/19 13:50:16 INFO DAGScheduler: ResultStage 12 (jdbc at NativeMethodAccessorImpl.java:0) finished in 2.166 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:16.251624Z","level":"info","event":"25/12/19 13:50:16 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:16.251670Z","level":"info","event":"25/12/19 13:50:16 INFO YarnScheduler: Killing all running tasks in stage 12: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:16.251717Z","level":"info","event":"25/12/19 13:50:16 INFO DAGScheduler: Job 8 finished: jdbc at NativeMethodAccessorImpl.java:0, took 2.178153 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:16.627009Z","level":"info","event":"25/12/19 13:50:16 INFO DataSourceStrategy: Pruning directories with: isnotnull(ingest_date#21),(ingest_date#21 >= 2025-09-07),(ingest_date#21 < 2025-09-14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:16.628212Z","level":"info","event":"25/12/19 13:50:16 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:16.628442Z","level":"info","event":"25/12/19 13:50:16 INFO FileSourceStrategy: Post-Scan Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:16.874354Z","level":"info","event":"25/12/19 13:50:16 INFO CodeGenerator: Code generated in 52.490711 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:16.885350Z","level":"info","event":"25/12/19 13:50:16 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 203.3 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:16.927762Z","level":"info","event":"25/12/19 13:50:16 INFO BlockManagerInfo: Removed broadcast_10_piece0 on datanode-nodemanager-2:34089 in memory (size: 35.8 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:16.931163Z","level":"info","event":"25/12/19 13:50:16 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 12d706d757b9:39431 in memory (size: 35.8 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:16.931410Z","level":"info","event":"25/12/19 13:50:16 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:16.931489Z","level":"info","event":"25/12/19 13:50:16 INFO BlockManagerInfo: Removed broadcast_10_piece0 on datanode-nodemanager-1:39491 in memory (size: 35.8 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:16.933852Z","level":"info","event":"25/12/19 13:50:16 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 12d706d757b9:39431 (size: 36.1 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:16.936873Z","level":"info","event":"25/12/19 13:50:16 INFO SparkContext: Created broadcast 13 from showString at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:16.945511Z","level":"info","event":"25/12/19 13:50:16 INFO InMemoryFileIndex: Selected 7 partitions out of 16, pruned 56.25% partitions.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:16.947384Z","level":"info","event":"25/12/19 13:50:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:16.971353Z","level":"info","event":"25/12/19 13:50:16 INFO BlockManagerInfo: Removed broadcast_12_piece0 on datanode-nodemanager-2:34089 in memory (size: 23.8 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:16.980655Z","level":"info","event":"25/12/19 13:50:16 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 12d706d757b9:39431 in memory (size: 23.8 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:17.026123Z","level":"info","event":"25/12/19 13:50:17 INFO DAGScheduler: Registering RDD 44 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 4","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:17.026928Z","level":"info","event":"25/12/19 13:50:17 INFO DAGScheduler: Got map stage job 9 (showString at NativeMethodAccessorImpl.java:0) with 15 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:17.027127Z","level":"info","event":"25/12/19 13:50:17 INFO DAGScheduler: Final stage: ShuffleMapStage 13 (showString at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:17.032215Z","level":"info","event":"25/12/19 13:50:17 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:17.041432Z","level":"info","event":"25/12/19 13:50:17 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:17.047580Z","level":"info","event":"25/12/19 13:50:17 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[44] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:17.175235Z","level":"info","event":"25/12/19 13:50:17 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 40.4 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:17.181064Z","level":"info","event":"25/12/19 13:50:17 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 17.1 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:17.182369Z","level":"info","event":"25/12/19 13:50:17 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 12d706d757b9:39431 (size: 17.1 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:17.183579Z","level":"info","event":"25/12/19 13:50:17 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:17.184828Z","level":"info","event":"25/12/19 13:50:17 INFO DAGScheduler: Submitting 15 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[44] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:17.187810Z","level":"info","event":"25/12/19 13:50:17 INFO YarnScheduler: Adding task set 13.0 with 15 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:17.189760Z","level":"info","event":"25/12/19 13:50:17 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 65) (datanode-nodemanager-2, executor 2, partition 0, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:17.190358Z","level":"info","event":"25/12/19 13:50:17 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 66) (datanode-nodemanager-1, executor 1, partition 1, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:17.250785Z","level":"info","event":"25/12/19 13:50:17 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on datanode-nodemanager-1:39491 (size: 17.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:17.277929Z","level":"info","event":"25/12/19 13:50:17 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on datanode-nodemanager-2:34089 (size: 17.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:17.579791Z","level":"info","event":"25/12/19 13:50:17 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on datanode-nodemanager-1:39491 (size: 36.1 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:17.725341Z","level":"info","event":"25/12/19 13:50:17 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on datanode-nodemanager-2:34089 (size: 36.1 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:19.817122Z","level":"info","event":"25/12/19 13:50:19 INFO TaskSetManager: Starting task 2.0 in stage 13.0 (TID 67) (datanode-nodemanager-1, executor 1, partition 2, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:19.818305Z","level":"info","event":"25/12/19 13:50:19 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 66) in 2628 ms on datanode-nodemanager-1 (executor 1) (1/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:19.835203Z","level":"info","event":"25/12/19 13:50:19 INFO TaskSetManager: Starting task 3.0 in stage 13.0 (TID 68) (datanode-nodemanager-2, executor 2, partition 3, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:19.836057Z","level":"info","event":"25/12/19 13:50:19 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 65) in 2647 ms on datanode-nodemanager-2 (executor 2) (2/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:20.966238Z","level":"info","event":"25/12/19 13:50:20 INFO TaskSetManager: Starting task 4.0 in stage 13.0 (TID 69) (datanode-nodemanager-1, executor 1, partition 4, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:20.967815Z","level":"info","event":"25/12/19 13:50:20 INFO TaskSetManager: Finished task 2.0 in stage 13.0 (TID 67) in 1151 ms on datanode-nodemanager-1 (executor 1) (3/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:21.031426Z","level":"info","event":"25/12/19 13:50:21 INFO TaskSetManager: Starting task 5.0 in stage 13.0 (TID 70) (datanode-nodemanager-2, executor 2, partition 5, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:21.042145Z","level":"info","event":"25/12/19 13:50:21 INFO TaskSetManager: Finished task 3.0 in stage 13.0 (TID 68) in 1207 ms on datanode-nodemanager-2 (executor 2) (4/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:22.025297Z","level":"info","event":"25/12/19 13:50:22 INFO TaskSetManager: Starting task 6.0 in stage 13.0 (TID 71) (datanode-nodemanager-1, executor 1, partition 6, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:22.025548Z","level":"info","event":"25/12/19 13:50:22 INFO TaskSetManager: Finished task 4.0 in stage 13.0 (TID 69) in 1059 ms on datanode-nodemanager-1 (executor 1) (5/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:22.165842Z","level":"info","event":"25/12/19 13:50:22 INFO TaskSetManager: Starting task 7.0 in stage 13.0 (TID 72) (datanode-nodemanager-2, executor 2, partition 7, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:22.166990Z","level":"info","event":"25/12/19 13:50:22 INFO TaskSetManager: Finished task 5.0 in stage 13.0 (TID 70) in 1137 ms on datanode-nodemanager-2 (executor 2) (6/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:22.993263Z","level":"info","event":"25/12/19 13:50:22 INFO TaskSetManager: Starting task 8.0 in stage 13.0 (TID 73) (datanode-nodemanager-1, executor 1, partition 8, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:22.993492Z","level":"info","event":"25/12/19 13:50:22 INFO TaskSetManager: Finished task 6.0 in stage 13.0 (TID 71) in 966 ms on datanode-nodemanager-1 (executor 1) (7/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:23.020614Z","level":"info","event":"25/12/19 13:50:23 INFO TaskSetManager: Starting task 9.0 in stage 13.0 (TID 74) (datanode-nodemanager-2, executor 2, partition 9, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:23.022203Z","level":"info","event":"25/12/19 13:50:23 INFO TaskSetManager: Finished task 7.0 in stage 13.0 (TID 72) in 857 ms on datanode-nodemanager-2 (executor 2) (8/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:27.458956Z","level":"info","event":"25/12/19 13:50:27 INFO TaskSetManager: Starting task 10.0 in stage 13.0 (TID 75) (datanode-nodemanager-2, executor 2, partition 10, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:27.460342Z","level":"info","event":"25/12/19 13:50:27 INFO TaskSetManager: Finished task 9.0 in stage 13.0 (TID 74) in 4440 ms on datanode-nodemanager-2 (executor 2) (9/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:27.498019Z","level":"info","event":"25/12/19 13:50:27 INFO TaskSetManager: Starting task 11.0 in stage 13.0 (TID 76) (datanode-nodemanager-1, executor 1, partition 11, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:27.500657Z","level":"info","event":"25/12/19 13:50:27 INFO TaskSetManager: Finished task 8.0 in stage 13.0 (TID 73) in 4512 ms on datanode-nodemanager-1 (executor 1) (10/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:28.251178Z","level":"info","event":"25/12/19 13:50:28 INFO TaskSetManager: Starting task 12.0 in stage 13.0 (TID 77) (datanode-nodemanager-2, executor 2, partition 12, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:28.251986Z","level":"info","event":"25/12/19 13:50:28 INFO TaskSetManager: Finished task 10.0 in stage 13.0 (TID 75) in 794 ms on datanode-nodemanager-2 (executor 2) (11/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:28.287806Z","level":"info","event":"25/12/19 13:50:28 INFO TaskSetManager: Starting task 13.0 in stage 13.0 (TID 78) (datanode-nodemanager-1, executor 1, partition 13, NODE_LOCAL, 14261 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:28.289097Z","level":"info","event":"25/12/19 13:50:28 INFO TaskSetManager: Finished task 11.0 in stage 13.0 (TID 76) in 792 ms on datanode-nodemanager-1 (executor 1) (12/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.104495Z","level":"info","event":"25/12/19 13:50:29 INFO TaskSetManager: Starting task 14.0 in stage 13.0 (TID 79) (datanode-nodemanager-2, executor 2, partition 14, NODE_LOCAL, 10418 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.104725Z","level":"info","event":"25/12/19 13:50:29 INFO TaskSetManager: Finished task 12.0 in stage 13.0 (TID 77) in 854 ms on datanode-nodemanager-2 (executor 2) (13/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.172162Z","level":"info","event":"25/12/19 13:50:29 INFO TaskSetManager: Finished task 13.0 in stage 13.0 (TID 78) in 883 ms on datanode-nodemanager-1 (executor 1) (14/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.433363Z","level":"info","event":"25/12/19 13:50:29 INFO TaskSetManager: Finished task 14.0 in stage 13.0 (TID 79) in 330 ms on datanode-nodemanager-2 (executor 2) (15/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.434663Z","level":"info","event":"25/12/19 13:50:29 INFO DAGScheduler: ShuffleMapStage 13 (showString at NativeMethodAccessorImpl.java:0) finished in 12.338 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.434856Z","level":"info","event":"25/12/19 13:50:29 INFO DAGScheduler: looking for newly runnable stages","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.434917Z","level":"info","event":"25/12/19 13:50:29 INFO DAGScheduler: running: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.434962Z","level":"info","event":"25/12/19 13:50:29 INFO DAGScheduler: waiting: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.435007Z","level":"info","event":"25/12/19 13:50:29 INFO DAGScheduler: failed: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.439729Z","level":"info","event":"25/12/19 13:50:29 INFO YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.452688Z","level":"info","event":"25/12/19 13:50:29 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1143424, minimum partition size: 1048576","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.588934Z","level":"info","event":"25/12/19 13:50:29 INFO CodeGenerator: Code generated in 92.811268 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.645107Z","level":"info","event":"25/12/19 13:50:29 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.650706Z","level":"info","event":"25/12/19 13:50:29 INFO DAGScheduler: Got job 10 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.655663Z","level":"info","event":"25/12/19 13:50:29 INFO DAGScheduler: Final stage: ResultStage 15 (showString at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.655950Z","level":"info","event":"25/12/19 13:50:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.658955Z","level":"info","event":"25/12/19 13:50:29 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.661058Z","level":"info","event":"25/12/19 13:50:29 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[48] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.709680Z","level":"info","event":"25/12/19 13:50:29 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 46.6 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.763800Z","level":"info","event":"25/12/19 13:50:29 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 20.0 KiB, free 434.0 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.764876Z","level":"info","event":"25/12/19 13:50:29 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 12d706d757b9:39431 (size: 20.0 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.765520Z","level":"info","event":"25/12/19 13:50:29 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.766770Z","level":"info","event":"25/12/19 13:50:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[48] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.766898Z","level":"info","event":"25/12/19 13:50:29 INFO YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.781069Z","level":"info","event":"25/12/19 13:50:29 INFO BlockManagerInfo: Removed broadcast_14_piece0 on datanode-nodemanager-1:39491 in memory (size: 17.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.791627Z","level":"info","event":"25/12/19 13:50:29 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 80) (datanode-nodemanager-2, executor 2, partition 0, NODE_LOCAL, 9010 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.791878Z","level":"info","event":"25/12/19 13:50:29 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 12d706d757b9:39431 in memory (size: 17.1 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.794898Z","level":"info","event":"25/12/19 13:50:29 INFO BlockManagerInfo: Removed broadcast_14_piece0 on datanode-nodemanager-2:34089 in memory (size: 17.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.849472Z","level":"info","event":"25/12/19 13:50:29 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on datanode-nodemanager-2:34089 (size: 20.0 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:29.896054Z","level":"info","event":"25/12/19 13:50:29 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 172.18.0.3:41844","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:30.511197Z","level":"info","event":"25/12/19 13:50:30 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 80) in 727 ms on datanode-nodemanager-2 (executor 2) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:30.511585Z","level":"info","event":"25/12/19 13:50:30 INFO YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:30.513974Z","level":"info","event":"25/12/19 13:50:30 INFO DAGScheduler: ResultStage 15 (showString at NativeMethodAccessorImpl.java:0) finished in 0.837 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:30.514166Z","level":"info","event":"25/12/19 13:50:30 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:30.514240Z","level":"info","event":"25/12/19 13:50:30 INFO YarnScheduler: Killing all running tasks in stage 15: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:30.519426Z","level":"info","event":"25/12/19 13:50:30 INFO DAGScheduler: Job 10 finished: showString at NativeMethodAccessorImpl.java:0, took 0.874078 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:30.621941Z","level":"info","event":"25/12/19 13:50:30 INFO CodeGenerator: Code generated in 83.346619 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:30.635941Z","level":"info","event":"+--------------------+-------------------+------------------+--------------------+--------------------+---------------------------+----------------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:30.636145Z","level":"info","event":"|             news_id|       news_time_id|news_overall_score|          news_title|        news_summary|news_category_within_source|     news_source|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:30.636223Z","level":"info","event":"+--------------------+-------------------+------------------+--------------------+--------------------+---------------------------+----------------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:30.636281Z","level":"info","event":"|-9223213831179807009|8893285539898621379|          0.244209|NANO Nuclear to b...|New York, N.Y., S...|                        n/a|   GlobeNewswire|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:30.636327Z","level":"info","event":"|-9212086765851615322|6948219474172485782|          0.213985|International Fla...|IFF's DEB platfor...|                           |Zacks Commentary|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:30.636370Z","level":"info","event":"|-9211094784278072117|8893285539898621379|          0.252493|Interactive Stren...|Shares of Interac...|                       News|        Benzinga|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:30.636411Z","level":"info","event":"|-9198831313870398994|7150283814656891700|          0.067845|Deadline Alert: L...|LOS ANGELES, Sept...|                           |        Benzinga|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:30.636457Z","level":"info","event":"|-9190030143288821540|-381703233359023787|          0.097045|3 Top EV Stocks t...|BYD, QuantumScape...|                        n/a|     Motley Fool|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:30.636502Z","level":"info","event":"|-9184431519410189589|8893285539898621379|         -0.004907|Levi & Korsinsky ...|NEW YORK, Sept. 1...|                           |        Benzinga|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:30.636546Z","level":"info","event":"|-9182378392390297335|8893285539898621379|          0.295513|Why Lovesac Stock...|Lovesac keeps gai...|                           |     Motley Fool|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:30.636590Z","level":"info","event":"|-9180855611240361043|7181520199071383846|         -0.056694|Trump Admin Pushe...|The U.S. has urge...|                       News|        Benzinga|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:30.636634Z","level":"info","event":"|-9159505875354618112|6948219474172485782|          0.297092|What Makes York W...|York Water (YORW)...|                           |Zacks Commentary|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:30.636676Z","level":"info","event":"|-9153418346311000527|6948219474172485782|          0.585808|Will Sally Beauty...|SBH revamps its b...|                           |Zacks Commentary|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:30.636719Z","level":"info","event":"+--------------------+-------------------+------------------+--------------------+--------------------+---------------------------+----------------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:30.636764Z","level":"info","event":"only showing top 10 rows","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:30.636817Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:30.917048Z","level":"info","event":"25/12/19 13:50:30 INFO DataSourceStrategy: Pruning directories with: isnotnull(ingest_date#21),(ingest_date#21 >= 2025-09-07),(ingest_date#21 < 2025-09-14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:30.918349Z","level":"info","event":"25/12/19 13:50:30 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:30.918566Z","level":"info","event":"25/12/19 13:50:30 INFO FileSourceStrategy: Post-Scan Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.028952Z","level":"info","event":"25/12/19 13:50:31 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 203.3 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.121940Z","level":"info","event":"25/12/19 13:50:31 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.122148Z","level":"info","event":"25/12/19 13:50:31 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 12d706d757b9:39431 (size: 36.1 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.122233Z","level":"info","event":"25/12/19 13:50:31 INFO SparkContext: Created broadcast 16 from jdbc at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.123984Z","level":"info","event":"25/12/19 13:50:31 INFO InMemoryFileIndex: Selected 7 partitions out of 16, pruned 56.25% partitions.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.125617Z","level":"info","event":"25/12/19 13:50:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.165508Z","level":"info","event":"25/12/19 13:50:31 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 12d706d757b9:39431 in memory (size: 36.1 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.166004Z","level":"info","event":"25/12/19 13:50:31 INFO BlockManagerInfo: Removed broadcast_13_piece0 on datanode-nodemanager-1:39491 in memory (size: 36.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.188632Z","level":"info","event":"25/12/19 13:50:31 INFO BlockManagerInfo: Removed broadcast_13_piece0 on datanode-nodemanager-2:34089 in memory (size: 36.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.236798Z","level":"info","event":"25/12/19 13:50:31 INFO BlockManagerInfo: Removed broadcast_15_piece0 on datanode-nodemanager-2:34089 in memory (size: 20.0 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.255813Z","level":"info","event":"25/12/19 13:50:31 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 12d706d757b9:39431 in memory (size: 20.0 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.275243Z","level":"info","event":"25/12/19 13:50:31 INFO DAGScheduler: Registering RDD 53 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 5","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.275510Z","level":"info","event":"25/12/19 13:50:31 INFO DAGScheduler: Got map stage job 11 (jdbc at NativeMethodAccessorImpl.java:0) with 15 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.275794Z","level":"info","event":"25/12/19 13:50:31 INFO DAGScheduler: Final stage: ShuffleMapStage 16 (jdbc at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.275925Z","level":"info","event":"25/12/19 13:50:31 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.289001Z","level":"info","event":"25/12/19 13:50:31 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.291394Z","level":"info","event":"25/12/19 13:50:31 INFO DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[53] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.372605Z","level":"info","event":"25/12/19 13:50:31 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 40.4 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.374316Z","level":"info","event":"25/12/19 13:50:31 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.375316Z","level":"info","event":"25/12/19 13:50:31 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 12d706d757b9:39431 (size: 17.0 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.375809Z","level":"info","event":"25/12/19 13:50:31 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.377037Z","level":"info","event":"25/12/19 13:50:31 INFO DAGScheduler: Submitting 15 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[53] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.377194Z","level":"info","event":"25/12/19 13:50:31 INFO YarnScheduler: Adding task set 16.0 with 15 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.382494Z","level":"info","event":"25/12/19 13:50:31 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 81) (datanode-nodemanager-1, executor 1, partition 0, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.382978Z","level":"info","event":"25/12/19 13:50:31 INFO TaskSetManager: Starting task 1.0 in stage 16.0 (TID 82) (datanode-nodemanager-2, executor 2, partition 1, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.425184Z","level":"info","event":"25/12/19 13:50:31 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on datanode-nodemanager-1:39491 (size: 17.0 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.442555Z","level":"info","event":"25/12/19 13:50:31 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on datanode-nodemanager-2:34089 (size: 17.0 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.481201Z","level":"info","event":"25/12/19 13:50:31 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on datanode-nodemanager-1:39491 (size: 36.1 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:31.486774Z","level":"info","event":"25/12/19 13:50:31 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on datanode-nodemanager-2:34089 (size: 36.1 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:32.341755Z","level":"info","event":"25/12/19 13:50:32 INFO TaskSetManager: Starting task 2.0 in stage 16.0 (TID 83) (datanode-nodemanager-2, executor 2, partition 2, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:32.342260Z","level":"info","event":"25/12/19 13:50:32 INFO TaskSetManager: Finished task 1.0 in stage 16.0 (TID 82) in 961 ms on datanode-nodemanager-2 (executor 2) (1/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:32.387669Z","level":"info","event":"25/12/19 13:50:32 INFO TaskSetManager: Starting task 3.0 in stage 16.0 (TID 84) (datanode-nodemanager-1, executor 1, partition 3, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:32.398529Z","level":"info","event":"25/12/19 13:50:32 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 81) in 1019 ms on datanode-nodemanager-1 (executor 1) (2/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:33.064022Z","level":"info","event":"25/12/19 13:50:33 INFO TaskSetManager: Starting task 4.0 in stage 16.0 (TID 85) (datanode-nodemanager-2, executor 2, partition 4, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:33.068531Z","level":"info","event":"25/12/19 13:50:33 INFO TaskSetManager: Finished task 2.0 in stage 16.0 (TID 83) in 727 ms on datanode-nodemanager-2 (executor 2) (3/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:33.139605Z","level":"info","event":"25/12/19 13:50:33 INFO TaskSetManager: Starting task 5.0 in stage 16.0 (TID 86) (datanode-nodemanager-1, executor 1, partition 5, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:33.140842Z","level":"info","event":"25/12/19 13:50:33 INFO TaskSetManager: Finished task 3.0 in stage 16.0 (TID 84) in 753 ms on datanode-nodemanager-1 (executor 1) (4/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:33.781024Z","level":"info","event":"25/12/19 13:50:33 INFO TaskSetManager: Starting task 6.0 in stage 16.0 (TID 87) (datanode-nodemanager-2, executor 2, partition 6, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:33.785160Z","level":"info","event":"25/12/19 13:50:33 INFO TaskSetManager: Finished task 4.0 in stage 16.0 (TID 85) in 719 ms on datanode-nodemanager-2 (executor 2) (5/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:33.981799Z","level":"info","event":"25/12/19 13:50:33 INFO TaskSetManager: Finished task 5.0 in stage 16.0 (TID 86) in 843 ms on datanode-nodemanager-1 (executor 1) (6/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:33.994203Z","level":"info","event":"25/12/19 13:50:33 INFO TaskSetManager: Starting task 7.0 in stage 16.0 (TID 88) (datanode-nodemanager-1, executor 1, partition 7, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:34.687858Z","level":"info","event":"25/12/19 13:50:34 INFO TaskSetManager: Starting task 8.0 in stage 16.0 (TID 89) (datanode-nodemanager-2, executor 2, partition 8, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:34.692905Z","level":"info","event":"25/12/19 13:50:34 INFO TaskSetManager: Finished task 6.0 in stage 16.0 (TID 87) in 913 ms on datanode-nodemanager-2 (executor 2) (7/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:34.853906Z","level":"info","event":"25/12/19 13:50:34 INFO TaskSetManager: Starting task 9.0 in stage 16.0 (TID 90) (datanode-nodemanager-1, executor 1, partition 9, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:34.862230Z","level":"info","event":"25/12/19 13:50:34 INFO TaskSetManager: Finished task 7.0 in stage 16.0 (TID 88) in 871 ms on datanode-nodemanager-1 (executor 1) (8/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:35.713511Z","level":"info","event":"25/12/19 13:50:35 INFO TaskSetManager: Starting task 10.0 in stage 16.0 (TID 91) (datanode-nodemanager-2, executor 2, partition 10, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:35.721095Z","level":"info","event":"25/12/19 13:50:35 INFO TaskSetManager: Finished task 8.0 in stage 16.0 (TID 89) in 1031 ms on datanode-nodemanager-2 (executor 2) (9/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:35.791283Z","level":"info","event":"25/12/19 13:50:35 INFO TaskSetManager: Finished task 9.0 in stage 16.0 (TID 90) in 939 ms on datanode-nodemanager-1 (executor 1) (10/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:35.793918Z","level":"info","event":"25/12/19 13:50:35 INFO TaskSetManager: Starting task 11.0 in stage 16.0 (TID 92) (datanode-nodemanager-1, executor 1, partition 11, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:36.519040Z","level":"info","event":"25/12/19 13:50:36 INFO TaskSetManager: Starting task 12.0 in stage 16.0 (TID 93) (datanode-nodemanager-2, executor 2, partition 12, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:36.524270Z","level":"info","event":"25/12/19 13:50:36 INFO TaskSetManager: Finished task 10.0 in stage 16.0 (TID 91) in 811 ms on datanode-nodemanager-2 (executor 2) (11/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:36.625308Z","level":"info","event":"25/12/19 13:50:36 INFO TaskSetManager: Starting task 13.0 in stage 16.0 (TID 94) (datanode-nodemanager-1, executor 1, partition 13, NODE_LOCAL, 14261 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:36.626868Z","level":"info","event":"25/12/19 13:50:36 INFO TaskSetManager: Finished task 11.0 in stage 16.0 (TID 92) in 835 ms on datanode-nodemanager-1 (executor 1) (12/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.060869Z","level":"info","event":"25/12/19 13:50:37 INFO TaskSetManager: Starting task 14.0 in stage 16.0 (TID 95) (datanode-nodemanager-2, executor 2, partition 14, NODE_LOCAL, 10418 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.061508Z","level":"info","event":"25/12/19 13:50:37 INFO TaskSetManager: Finished task 12.0 in stage 16.0 (TID 93) in 544 ms on datanode-nodemanager-2 (executor 2) (13/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.182857Z","level":"info","event":"25/12/19 13:50:37 INFO TaskSetManager: Finished task 14.0 in stage 16.0 (TID 95) in 123 ms on datanode-nodemanager-2 (executor 2) (14/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.223316Z","level":"info","event":"25/12/19 13:50:37 INFO TaskSetManager: Finished task 13.0 in stage 16.0 (TID 94) in 598 ms on datanode-nodemanager-1 (executor 1) (15/15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.224267Z","level":"info","event":"25/12/19 13:50:37 INFO YarnScheduler: Removed TaskSet 16.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.224405Z","level":"info","event":"25/12/19 13:50:37 INFO DAGScheduler: ShuffleMapStage 16 (jdbc at NativeMethodAccessorImpl.java:0) finished in 5.918 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.224471Z","level":"info","event":"25/12/19 13:50:37 INFO DAGScheduler: looking for newly runnable stages","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.225498Z","level":"info","event":"25/12/19 13:50:37 INFO DAGScheduler: running: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.225687Z","level":"info","event":"25/12/19 13:50:37 INFO DAGScheduler: waiting: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.225746Z","level":"info","event":"25/12/19 13:50:37 INFO DAGScheduler: failed: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.237554Z","level":"info","event":"25/12/19 13:50:37 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1143424, minimum partition size: 1048576","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.309065Z","level":"info","event":"25/12/19 13:50:37 INFO SparkContext: Starting job: jdbc at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.310670Z","level":"info","event":"25/12/19 13:50:37 INFO DAGScheduler: Got job 12 (jdbc at NativeMethodAccessorImpl.java:0) with 2 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.310890Z","level":"info","event":"25/12/19 13:50:37 INFO DAGScheduler: Final stage: ResultStage 18 (jdbc at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.310966Z","level":"info","event":"25/12/19 13:50:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.313961Z","level":"info","event":"25/12/19 13:50:37 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.315386Z","level":"info","event":"25/12/19 13:50:37 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[59] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.338616Z","level":"info","event":"25/12/19 13:50:37 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 54.7 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.354309Z","level":"info","event":"25/12/19 13:50:37 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 23.6 KiB, free 434.0 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.360032Z","level":"info","event":"25/12/19 13:50:37 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 12d706d757b9:39431 (size: 23.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.360231Z","level":"info","event":"25/12/19 13:50:37 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 12d706d757b9:39431 in memory (size: 17.0 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.366057Z","level":"info","event":"25/12/19 13:50:37 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.366432Z","level":"info","event":"25/12/19 13:50:37 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 18 (MapPartitionsRDD[59] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.366507Z","level":"info","event":"25/12/19 13:50:37 INFO YarnScheduler: Adding task set 18.0 with 2 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.366557Z","level":"info","event":"25/12/19 13:50:37 INFO BlockManagerInfo: Removed broadcast_17_piece0 on datanode-nodemanager-2:34089 in memory (size: 17.0 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.366653Z","level":"info","event":"25/12/19 13:50:37 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 96) (datanode-nodemanager-1, executor 1, partition 0, NODE_LOCAL, 9010 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.366796Z","level":"info","event":"25/12/19 13:50:37 INFO BlockManagerInfo: Removed broadcast_17_piece0 on datanode-nodemanager-1:39491 in memory (size: 17.0 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.366860Z","level":"info","event":"25/12/19 13:50:37 INFO TaskSetManager: Starting task 1.0 in stage 18.0 (TID 97) (datanode-nodemanager-2, executor 2, partition 1, NODE_LOCAL, 9010 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.394414Z","level":"info","event":"25/12/19 13:50:37 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on datanode-nodemanager-2:34089 (size: 23.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.398623Z","level":"info","event":"25/12/19 13:50:37 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on datanode-nodemanager-1:39491 (size: 23.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.424493Z","level":"info","event":"25/12/19 13:50:37 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 172.18.0.4:50698","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:37.432390Z","level":"info","event":"25/12/19 13:50:37 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 172.18.0.3:41844","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:38.385098Z","level":"info","event":"25/12/19 13:50:38 WARN TaskSetManager: Lost task 0.0 in stage 18.0 (TID 96) (datanode-nodemanager-1 executor 1): java.sql.BatchUpdateException: Violation of PRIMARY KEY constraint 'PK__dim_news__4C27CCD80BD94A5D'. Cannot insert duplicate key in object 'dbo.dim_news'. The duplicate key value is (-9082011978506495428).","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:38.385293Z","level":"info","event":"at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2187)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:38.385354Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:38.385499Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:38.385646Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:38.385796Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:38.385919Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:38.386105Z","level":"info","event":"at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:38.386222Z","level":"info","event":"at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:38.386325Z","level":"info","event":"at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:38.386425Z","level":"info","event":"at org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:38.386618Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:38.386825Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:38.386948Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:38.388577Z","level":"info","event":"at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:38.390561Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:38.390651Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:38.390699Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:38.390741Z","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:834)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:38.390820Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:38.399002Z","level":"info","event":"25/12/19 13:50:38 INFO TaskSetManager: Starting task 0.1 in stage 18.0 (TID 98) (datanode-nodemanager-1, executor 1, partition 0, NODE_LOCAL, 9010 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:39.046836Z","level":"info","event":"25/12/19 13:50:39 INFO TaskSetManager: Lost task 0.1 in stage 18.0 (TID 98) on datanode-nodemanager-1, executor 1: java.sql.BatchUpdateException (Violation of PRIMARY KEY constraint 'PK__dim_news__4C27CCD80BD94A5D'. Cannot insert duplicate key in object 'dbo.dim_news'. The duplicate key value is (-9082011978506495428).) [duplicate 1]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:39.052312Z","level":"info","event":"25/12/19 13:50:39 INFO TaskSetManager: Starting task 0.2 in stage 18.0 (TID 99) (datanode-nodemanager-1, executor 1, partition 0, NODE_LOCAL, 9010 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:39.563967Z","level":"info","event":"25/12/19 13:50:39 INFO TaskSetManager: Lost task 0.2 in stage 18.0 (TID 99) on datanode-nodemanager-1, executor 1: java.sql.BatchUpdateException (Violation of PRIMARY KEY constraint 'PK__dim_news__4C27CCD80BD94A5D'. Cannot insert duplicate key in object 'dbo.dim_news'. The duplicate key value is (-9082011978506495428).) [duplicate 2]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:39.566638Z","level":"info","event":"25/12/19 13:50:39 INFO TaskSetManager: Starting task 0.3 in stage 18.0 (TID 100) (datanode-nodemanager-1, executor 1, partition 0, NODE_LOCAL, 9010 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.025157Z","level":"info","event":"25/12/19 13:50:40 INFO TaskSetManager: Lost task 0.3 in stage 18.0 (TID 100) on datanode-nodemanager-1, executor 1: java.sql.BatchUpdateException (Violation of PRIMARY KEY constraint 'PK__dim_news__4C27CCD80BD94A5D'. Cannot insert duplicate key in object 'dbo.dim_news'. The duplicate key value is (-9082011978506495428).) [duplicate 3]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.026475Z","level":"info","event":"25/12/19 13:50:40 ERROR TaskSetManager: Task 0 in stage 18.0 failed 4 times; aborting job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.047676Z","level":"info","event":"25/12/19 13:50:40 INFO YarnScheduler: Cancelling stage 18","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.048395Z","level":"info","event":"25/12/19 13:50:40 INFO YarnScheduler: Killing all running tasks in stage 18: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 18.0 failed 4 times, most recent failure: Lost task 0.3 in stage 18.0 (TID 100) (datanode-nodemanager-1 executor 1): java.sql.BatchUpdateException: Violation of PRIMARY KEY constraint 'PK__dim_news__4C27CCD80BD94A5D'. Cannot insert duplicate key in object 'dbo.dim_news'. The duplicate key value is (-9082011978506495428).","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.048530Z","level":"info","event":"at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2187)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.049197Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.049695Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.050016Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.050168Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.050406Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.050518Z","level":"info","event":"at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.051001Z","level":"info","event":"at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.051250Z","level":"info","event":"at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.052969Z","level":"info","event":"at org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.053128Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.055187Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.055934Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.056482Z","level":"info","event":"at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.057257Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.057351Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.057407Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.057456Z","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:834)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.057505Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.057553Z","level":"info","event":"Driver stacktrace:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.057626Z","level":"info","event":"25/12/19 13:50:40 INFO YarnScheduler: Stage 18 was cancelled","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.059615Z","level":"info","event":"25/12/19 13:50:40 INFO DAGScheduler: ResultStage 18 (jdbc at NativeMethodAccessorImpl.java:0) failed in 2.733 s due to Job aborted due to stage failure: Task 0 in stage 18.0 failed 4 times, most recent failure: Lost task 0.3 in stage 18.0 (TID 100) (datanode-nodemanager-1 executor 1): java.sql.BatchUpdateException: Violation of PRIMARY KEY constraint 'PK__dim_news__4C27CCD80BD94A5D'. Cannot insert duplicate key in object 'dbo.dim_news'. The duplicate key value is (-9082011978506495428).","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.059821Z","level":"info","event":"at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2187)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.059896Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.059946Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.059991Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.060034Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.060078Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.060121Z","level":"info","event":"at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.060164Z","level":"info","event":"at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.060208Z","level":"info","event":"at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.060250Z","level":"info","event":"at org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.060296Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.060341Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.060384Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.060428Z","level":"info","event":"at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.060472Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.060515Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.060562Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.060607Z","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:834)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.060653Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.060699Z","level":"info","event":"Driver stacktrace:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.069507Z","level":"info","event":"25/12/19 13:50:40 INFO DAGScheduler: Job 12 failed: jdbc at NativeMethodAccessorImpl.java:0, took 2.758289 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.803158Z","level":"info","event":"Traceback (most recent call last):","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.803431Z","level":"info","event":"File \"/workspace/airflow/spark-jobs/update_news_data.py\", line 73, in <module>","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.807422Z","level":"info","event":"dim_news.write.jdbc(","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.807716Z","level":"info","event":"File \"/workspace/airflow/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 1984, in jdbc","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.816432Z","level":"info","event":"File \"/workspace/airflow/.venv/lib/python3.12/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.823402Z","level":"info","event":"File \"/workspace/airflow/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.830244Z","level":"info","event":"File \"/workspace/airflow/.venv/lib/python3.12/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.871337Z","level":"info","event":"py4j.protocol.Py4JJavaError25/12/19 13:50:40 WARN TaskSetManager: Lost task 1.0 in stage 18.0 (TID 97) (datanode-nodemanager-2 executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 18.0 failed 4 times, most recent failure: Lost task 0.3 in stage 18.0 (TID 100) (datanode-nodemanager-1 executor 1): java.sql.BatchUpdateException: Violation of PRIMARY KEY constraint 'PK__dim_news__4C27CCD80BD94A5D'. Cannot insert duplicate key in object 'dbo.dim_news'. The duplicate key value is (-9082011978506495428).","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.871575Z","level":"info","event":"at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2187)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.871632Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.871663Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.871688Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.871713Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.871738Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.871761Z","level":"info","event":"at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.871783Z","level":"info","event":"at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.871806Z","level":"info","event":"at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.871828Z","level":"info","event":"at org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.871851Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.871875Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.871905Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.871943Z","level":"info","event":"at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.871967Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.871990Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.872013Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.872048Z","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:834)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.872095Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.872141Z","level":"info","event":"Driver stacktrace:)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.872208Z","level":"info","event":"25/12/19 13:50:40 INFO YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.879955Z","level":"info","event":": An error occurred while calling o124.jdbc.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.880130Z","level":"info","event":": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 4 times, most recent failure: Lost task 0.3 in stage 18.0 (TID 100) (datanode-nodemanager-1 executor 1): java.sql.BatchUpdateException: Violation of PRIMARY KEY constraint 'PK__dim_news__4C27CCD80BD94A5D'. Cannot insert duplicate key in object 'dbo.dim_news'. The duplicate key value is (-9082011978506495428).","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.880212Z","level":"info","event":"at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2187)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.880270Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.880320Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.880367Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.880411Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.880463Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.880510Z","level":"info","event":"at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.880558Z","level":"info","event":"at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.880607Z","level":"info","event":"at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.880660Z","level":"info","event":"at org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.880712Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.880761Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.880811Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.880865Z","level":"info","event":"at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.880907Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.880955Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.881005Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.881045Z","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:834)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.881092Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.881123Z","level":"info","event":"Driver stacktrace:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.881176Z","level":"info","event":"at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2898)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.881230Z","level":"info","event":"at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2834)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.881279Z","level":"info","event":"at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2833)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.881309Z","level":"info","event":"at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.881356Z","level":"info","event":"at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.881406Z","level":"info","event":"at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.881496Z","level":"info","event":"at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2833)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.881682Z","level":"info","event":"at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1253)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.881737Z","level":"info","event":"at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1253)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.881765Z","level":"info","event":"at scala.Option.foreach(Option.scala:407)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.881826Z","level":"info","event":"at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1253)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.882021Z","level":"info","event":"at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3102)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.882173Z","level":"info","event":"at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3036)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.882242Z","level":"info","event":"at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3025)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.882288Z","level":"info","event":"at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.882427Z","level":"info","event":"at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:995)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.882566Z","level":"info","event":"at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.882631Z","level":"info","event":"at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.882733Z","level":"info","event":"at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.882795Z","level":"info","event":"at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.882842Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.882928Z","level":"info","event":"at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.883041Z","level":"info","event":"at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.883134Z","level":"info","event":"at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.883246Z","level":"info","event":"at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.883659Z","level":"info","event":"at org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3516)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.883833Z","level":"info","event":"at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.883891Z","level":"info","event":"at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4310)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.883924Z","level":"info","event":"at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.883971Z","level":"info","event":"at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.884017Z","level":"info","event":"at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.884059Z","level":"info","event":"at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.884106Z","level":"info","event":"at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.884151Z","level":"info","event":"at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4308)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.884202Z","level":"info","event":"at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3516)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.884254Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:903)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.884313Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.884393Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.884452Z","level":"info","event":"at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.884514Z","level":"info","event":"at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.884561Z","level":"info","event":"at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.884608Z","level":"info","event":"at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.884662Z","level":"info","event":"at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.884710Z","level":"info","event":"at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.884762Z","level":"info","event":"at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.884801Z","level":"info","event":"at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.884850Z","level":"info","event":"at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.884897Z","level":"info","event":"at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.884948Z","level":"info","event":"at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.885005Z","level":"info","event":"at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.885055Z","level":"info","event":"at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.885098Z","level":"info","event":"at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.885157Z","level":"info","event":"at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.885217Z","level":"info","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.885262Z","level":"info","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.885308Z","level":"info","event":"at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.885359Z","level":"info","event":"at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.885408Z","level":"info","event":"at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.885459Z","level":"info","event":"at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.885501Z","level":"info","event":"at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.885542Z","level":"info","event":"at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.885584Z","level":"info","event":"at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.885622Z","level":"info","event":"at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.885659Z","level":"info","event":"at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.885697Z","level":"info","event":"at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.885736Z","level":"info","event":"at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.885775Z","level":"info","event":"at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:766)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.885815Z","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.885857Z","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.885895Z","level":"info","event":"at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.885933Z","level":"info","event":"at java.base/java.lang.reflect.Method.invoke(Method.java:566)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.885969Z","level":"info","event":"at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.886015Z","level":"info","event":"at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.886062Z","level":"info","event":"at py4j.Gateway.invoke(Gateway.java:282)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.886103Z","level":"info","event":"at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.886133Z","level":"info","event":"at py4j.commands.CallCommand.execute(CallCommand.java:79)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.886158Z","level":"info","event":"at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.886181Z","level":"info","event":"at py4j.ClientServerConnection.run(ClientServerConnection.java:106)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.886206Z","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:829)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.886252Z","level":"info","event":"Caused by: java.sql.BatchUpdateException: Violation of PRIMARY KEY constraint 'PK__dim_news__4C27CCD80BD94A5D'. Cannot insert duplicate key in object 'dbo.dim_news'. The duplicate key value is (-9082011978506495428).","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.886297Z","level":"info","event":"at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2187)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.886346Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.886393Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.886427Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.886471Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.886503Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.886527Z","level":"info","event":"at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.886553Z","level":"info","event":"at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.886577Z","level":"info","event":"at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.886600Z","level":"info","event":"at org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.886641Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.886681Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.886714Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.886740Z","level":"info","event":"at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.886771Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.886951Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.887023Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.887061Z","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:834)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.887122Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.963880Z","level":"info","event":"25/12/19 13:50:40 INFO SparkContext: Invoking stop() from shutdown hook","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.964494Z","level":"info","event":"25/12/19 13:50:40 INFO SparkContext: SparkContext is stopping with exitCode 0.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:40.996368Z","level":"info","event":"25/12/19 13:50:40 INFO SparkUI: Stopped Spark web UI at http://12d706d757b9:4040","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:41.010726Z","level":"info","event":"25/12/19 13:50:41 INFO YarnClientSchedulerBackend: Interrupting monitor thread","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:41.048743Z","level":"info","event":"25/12/19 13:50:41 INFO YarnClientSchedulerBackend: Shutting down all executors","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:41.049839Z","level":"info","event":"25/12/19 13:50:41 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:41.092706Z","level":"info","event":"25/12/19 13:50:41 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:41.120809Z","level":"info","event":"25/12/19 13:50:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:41.190003Z","level":"info","event":"25/12/19 13:50:41 INFO MemoryStore: MemoryStore cleared","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:41.199621Z","level":"info","event":"25/12/19 13:50:41 INFO BlockManager: BlockManager stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:41.231005Z","level":"info","event":"25/12/19 13:50:41 INFO BlockManagerMaster: BlockManagerMaster stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:41.245417Z","level":"info","event":"25/12/19 13:50:41 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:41.291659Z","level":"info","event":"25/12/19 13:50:41 INFO SparkContext: Successfully stopped SparkContext","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:41.291844Z","level":"info","event":"25/12/19 13:50:41 INFO ShutdownHookManager: Shutdown hook called","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:41.291907Z","level":"info","event":"25/12/19 13:50:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-0b6ed2e3-69d8-4d67-8054-31017d5d1942/pyspark-e3ef0782-1b97-4371-91e0-e29a782c9ac5","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:41.311645Z","level":"info","event":"25/12/19 13:50:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-0b6ed2e3-69d8-4d67-8054-31017d5d1942","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:41.322456Z","level":"info","event":"25/12/19 13:50:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-528a5c6a-f6ec-4be6-b8fc-28713a4ef36e","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:50:41.581731Z","level":"error","event":"Task failed with exception","logger":"task","filename":"task_runner.py","lineno":980,"error_detail":[{"exc_type":"AirflowException","exc_value":"Cannot execute: spark-submit --master yarn --jars /workspace/airflow/connector/mssql-jdbc-12.2.0.jre11.jar --name arrow-spark --verbose --deploy-mode client /workspace/airflow/spark-jobs/update_news_data.py 2025-09-07 2025-09-14. Error code is: 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/workspace/airflow/.venv/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":928,"name":"run"},{"filename":"/workspace/airflow/.venv/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1315,"name":"_execute_task"},{"filename":"/workspace/airflow/.venv/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":416,"name":"wrapper"},{"filename":"/workspace/airflow/.venv/lib/python3.12/site-packages/airflow/providers/apache/spark/operators/spark_submit.py","lineno":197,"name":"execute"},{"filename":"/workspace/airflow/.venv/lib/python3.12/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py","lineno":566,"name":"submit"}],"is_group":false,"exceptions":[]}]}
