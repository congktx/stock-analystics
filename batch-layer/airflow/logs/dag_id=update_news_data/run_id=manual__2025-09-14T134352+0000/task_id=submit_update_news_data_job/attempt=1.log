{"timestamp":"2025-12-19T13:44:13.331926Z","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager","filename":"manager.py","lineno":179}
{"timestamp":"2025-12-19T13:44:13.338909Z","level":"info","event":"Filling up the DagBag from /workspace/airflow/dags/update_news_data.py","logger":"airflow.models.dagbag.DagBag","filename":"dagbag.py","lineno":593}
{"timestamp":"2025-12-19T13:44:14.724169Z","level":"info","event":"Spark-Submit cmd: spark-submit --master yarn --jars /workspace/airflow/connector/mssql-jdbc-12.2.0.jre11.jar --name arrow-spark --verbose --deploy-mode client /workspace/airflow/spark-jobs/update_news_data.py 2025-09-07 2025-09-14","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":474}
{"timestamp":"2025-12-19T13:44:27.489388Z","level":"info","event":"Using properties file: null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.788669Z","level":"info","event":"Parsed arguments:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.788867Z","level":"info","event":"master                  yarn","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.788944Z","level":"info","event":"remote                  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.788992Z","level":"info","event":"deployMode              client","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.789064Z","level":"info","event":"executorMemory          null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.789121Z","level":"info","event":"executorCores           null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.789167Z","level":"info","event":"totalExecutorCores      null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.789213Z","level":"info","event":"propertiesFile          null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.789252Z","level":"info","event":"driverMemory            null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.789330Z","level":"info","event":"driverCores             null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.789371Z","level":"info","event":"driverExtraClassPath    null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.789412Z","level":"info","event":"driverExtraLibraryPath  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.789438Z","level":"info","event":"driverExtraJavaOptions  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.789488Z","level":"info","event":"supervise               false","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.789546Z","level":"info","event":"queue                   null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.789595Z","level":"info","event":"numExecutors            null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.789637Z","level":"info","event":"files                   null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.789682Z","level":"info","event":"pyFiles                 null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.789727Z","level":"info","event":"archives                null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.789775Z","level":"info","event":"mainClass               null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.789818Z","level":"info","event":"primaryResource         file:/workspace/airflow/spark-jobs/update_news_data.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.789864Z","level":"info","event":"name                    arrow-spark","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.789906Z","level":"info","event":"childArgs               [2025-09-07 2025-09-14]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.790010Z","level":"info","event":"jars                    file:/workspace/airflow/connector/mssql-jdbc-12.2.0.jre11.jar","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.790046Z","level":"info","event":"packages                null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.790097Z","level":"info","event":"packagesExclusions      null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.790147Z","level":"info","event":"repositories            null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.790197Z","level":"info","event":"verbose                 true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.790244Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.790292Z","level":"info","event":"Spark properties used, including those specified through","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.790344Z","level":"info","event":"--conf and those from the properties file null:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.790374Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.790416Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:27.790485Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:29.030928Z","level":"info","event":"25/12/19 13:44:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:29.736588Z","level":"info","event":"Main class:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:29.736846Z","level":"info","event":"org.apache.spark.deploy.PythonRunner","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:29.737193Z","level":"info","event":"Arguments:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:29.737272Z","level":"info","event":"file:/workspace/airflow/spark-jobs/update_news_data.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:29.737327Z","level":"info","event":"null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:29.737377Z","level":"info","event":"2025-09-07","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:29.737435Z","level":"info","event":"2025-09-14","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:29.756574Z","level":"info","event":"Spark config:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:29.756800Z","level":"info","event":"(spark.app.name,arrow-spark)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:29.756875Z","level":"info","event":"(spark.app.submitTime,1766151869640)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:29.756926Z","level":"info","event":"(spark.master,yarn)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:29.756972Z","level":"info","event":"(spark.repl.local.jars,file:///workspace/airflow/connector/mssql-jdbc-12.2.0.jre11.jar)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:29.757016Z","level":"info","event":"(spark.submit.deployMode,client)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:29.757062Z","level":"info","event":"(spark.submit.pyFiles,)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:29.757108Z","level":"info","event":"(spark.yarn.dist.jars,file:///workspace/airflow/connector/mssql-jdbc-12.2.0.jre11.jar)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:29.757153Z","level":"info","event":"(spark.yarn.isPython,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:29.757207Z","level":"info","event":"Classpath elements:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:29.757261Z","level":"info","event":"file:///workspace/airflow/connector/mssql-jdbc-12.2.0.jre11.jar","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:29.758401Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:29.758550Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:32.470021Z","level":"info","event":"25/12/19 13:44:32 INFO SparkContext: Running Spark version 3.5.7","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:32.470640Z","level":"info","event":"25/12/19 13:44:32 INFO SparkContext: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:32.471069Z","level":"info","event":"25/12/19 13:44:32 INFO SparkContext: Java version 11.0.23","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:32.534906Z","level":"info","event":"25/12/19 13:44:32 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:32.535594Z","level":"info","event":"25/12/19 13:44:32 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:32.536659Z","level":"info","event":"25/12/19 13:44:32 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:32.537863Z","level":"info","event":"25/12/19 13:44:32 INFO SparkContext: Submitted application: Checking news data","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:32.592280Z","level":"info","event":"25/12/19 13:44:32 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:32.611342Z","level":"info","event":"25/12/19 13:44:32 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:32.614141Z","level":"info","event":"25/12/19 13:44:32 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:32.758924Z","level":"info","event":"25/12/19 13:44:32 INFO SecurityManager: Changing view acls to: root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:32.759767Z","level":"info","event":"25/12/19 13:44:32 INFO SecurityManager: Changing modify acls to: root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:32.760279Z","level":"info","event":"25/12/19 13:44:32 INFO SecurityManager: Changing view acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:32.760657Z","level":"info","event":"25/12/19 13:44:32 INFO SecurityManager: Changing modify acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:32.761094Z","level":"info","event":"25/12/19 13:44:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:33.318430Z","level":"info","event":"25/12/19 13:44:33 INFO Utils: Successfully started service 'sparkDriver' on port 46211.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:33.378150Z","level":"info","event":"25/12/19 13:44:33 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:33.482928Z","level":"info","event":"25/12/19 13:44:33 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:33.564872Z","level":"info","event":"25/12/19 13:44:33 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:33.565865Z","level":"info","event":"25/12/19 13:44:33 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:33.647403Z","level":"info","event":"25/12/19 13:44:33 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:33.704883Z","level":"info","event":"25/12/19 13:44:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b7f6260c-2ba4-4995-8f2a-25e6c07457ed","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:33.751574Z","level":"info","event":"25/12/19 13:44:33 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:33.840865Z","level":"info","event":"25/12/19 13:44:33 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:34.158208Z","level":"info","event":"25/12/19 13:44:34 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:34.321044Z","level":"info","event":"25/12/19 13:44:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:34.849316Z","level":"info","event":"25/12/19 13:44:34 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at namenode/172.18.0.5:8032","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:36.379361Z","level":"info","event":"25/12/19 13:44:36 INFO Configuration: resource-types.xml not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:36.380268Z","level":"info","event":"25/12/19 13:44:36 INFO ResourceUtils: Unable to find 'resource-types.xml'.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:36.421091Z","level":"info","event":"25/12/19 13:44:36 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:36.422224Z","level":"info","event":"25/12/19 13:44:36 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:36.422915Z","level":"info","event":"25/12/19 13:44:36 INFO Client: Setting up container launch context for our AM","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:36.430080Z","level":"info","event":"25/12/19 13:44:36 INFO Client: Setting up the launch environment for our AM container","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:36.452160Z","level":"info","event":"25/12/19 13:44:36 INFO Client: Preparing resources for our AM container","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:36.538934Z","level":"info","event":"25/12/19 13:44:36 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:55.177481Z","level":"info","event":"25/12/19 13:44:55 INFO Client: Uploading resource file:/tmp/spark-1a89fce6-45d5-4b11-b100-68fc71097f04/__spark_libs__10952823007253701298.zip -> hdfs://namenode:8020/user/root/.sparkStaging/application_1766137997195_0017/__spark_libs__10952823007253701298.zip","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:57.707192Z","level":"info","event":"25/12/19 13:44:57 INFO Client: Uploading resource file:/workspace/airflow/connector/mssql-jdbc-12.2.0.jre11.jar -> hdfs://namenode:8020/user/root/.sparkStaging/application_1766137997195_0017/mssql-jdbc-12.2.0.jre11.jar","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:57.759577Z","level":"info","event":"25/12/19 13:44:57 INFO Client: Uploading resource file:/workspace/airflow/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip -> hdfs://namenode:8020/user/root/.sparkStaging/application_1766137997195_0017/pyspark.zip","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:57.853250Z","level":"info","event":"25/12/19 13:44:57 INFO Client: Uploading resource file:/workspace/airflow/.venv/lib/python3.12/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://namenode:8020/user/root/.sparkStaging/application_1766137997195_0017/py4j-0.10.9.7-src.zip","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:58.292368Z","level":"info","event":"25/12/19 13:44:58 INFO Client: Uploading resource file:/tmp/spark-1a89fce6-45d5-4b11-b100-68fc71097f04/__spark_conf__4711264834033665570.zip -> hdfs://namenode:8020/user/root/.sparkStaging/application_1766137997195_0017/__spark_conf__.zip","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:58.400895Z","level":"info","event":"25/12/19 13:44:58 INFO SecurityManager: Changing view acls to: root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:58.401451Z","level":"info","event":"25/12/19 13:44:58 INFO SecurityManager: Changing modify acls to: root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:58.401842Z","level":"info","event":"25/12/19 13:44:58 INFO SecurityManager: Changing view acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:58.402654Z","level":"info","event":"25/12/19 13:44:58 INFO SecurityManager: Changing modify acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:58.402763Z","level":"info","event":"25/12/19 13:44:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:58.466038Z","level":"info","event":"25/12/19 13:44:58 INFO Client: Submitting application application_1766137997195_0017 to ResourceManager","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:58.617263Z","level":"info","event":"25/12/19 13:44:58 INFO YarnClientImpl: Submitted application application_1766137997195_0017","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:59.624843Z","level":"info","event":"25/12/19 13:44:59 INFO Client: Application report for application_1766137997195_0017 (state: ACCEPTED)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:59.629119Z","level":"info","event":"25/12/19 13:44:59 INFO Client:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:59.629312Z","level":"info","event":"client token: N/A","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:59.629432Z","level":"info","event":"diagnostics: AM container is launched, waiting for AM container to Register with RM","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:59.629499Z","level":"info","event":"ApplicationMaster host: N/A","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:59.629549Z","level":"info","event":"ApplicationMaster RPC port: -1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:59.629598Z","level":"info","event":"queue: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:59.629684Z","level":"info","event":"start time: 1766151898524","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:59.629738Z","level":"info","event":"final status: UNDEFINED","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:59.629776Z","level":"info","event":"tracking URL: http://namenode:8088/proxy/application_1766137997195_0017/","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:44:59.629829Z","level":"info","event":"user: root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:07.657729Z","level":"info","event":"25/12/19 13:45:07 INFO Client: Application report for application_1766137997195_0017 (state: RUNNING)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:07.658649Z","level":"info","event":"25/12/19 13:45:07 INFO Client:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:07.658787Z","level":"info","event":"client token: N/A","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:07.658842Z","level":"info","event":"diagnostics: N/A","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:07.658890Z","level":"info","event":"ApplicationMaster host: 172.18.0.3","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:07.658933Z","level":"info","event":"ApplicationMaster RPC port: -1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:07.658975Z","level":"info","event":"queue: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:07.659020Z","level":"info","event":"start time: 1766151898524","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:07.659065Z","level":"info","event":"final status: UNDEFINED","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:07.659107Z","level":"info","event":"tracking URL: http://namenode:8088/proxy/application_1766137997195_0017/","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:07.659150Z","level":"info","event":"user: root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:07.664839Z","level":"info","event":"25/12/19 13:45:07 INFO YarnClientSchedulerBackend: Application application_1766137997195_0017 has started running.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:07.722388Z","level":"info","event":"25/12/19 13:45:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38241.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:07.722672Z","level":"info","event":"25/12/19 13:45:07 INFO NettyBlockTransferService: Server created on 12d706d757b9:38241","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:07.727820Z","level":"info","event":"25/12/19 13:45:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:07.791568Z","level":"info","event":"25/12/19 13:45:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 12d706d757b9, 38241, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:07.822296Z","level":"info","event":"25/12/19 13:45:07 INFO BlockManagerMasterEndpoint: Registering block manager 12d706d757b9:38241 with 434.4 MiB RAM, BlockManagerId(driver, 12d706d757b9, 38241, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:07.844188Z","level":"info","event":"25/12/19 13:45:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 12d706d757b9, 38241, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:07.845635Z","level":"info","event":"25/12/19 13:45:07 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> namenode, PROXY_URI_BASES -> http://namenode:8088/proxy/application_1766137997195_0017), /proxy/application_1766137997195_0017","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:07.847888Z","level":"info","event":"25/12/19 13:45:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 12d706d757b9, 38241, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.481300Z","level":"info","event":"25/12/19 13:45:08 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.527644Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.544035Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.546473Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.550462Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.552761Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.558744Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.558951Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.561236Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.564218Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.566647Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.568796Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.572225Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.575311Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.579194Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.582111Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.585203Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.587232Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.589883Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.592895Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.595977Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.598750Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /executors/heapHistogram: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.603644Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /executors/heapHistogram/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.606726Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.640245Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.642466Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.652956Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.655593Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.665855Z","level":"info","event":"25/12/19 13:45:08 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:08.669180Z","level":"info","event":"25/12/19 13:45:08 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:09.119311Z","level":"info","event":"25/12/19 13:45:09 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:09.122657Z","level":"info","event":"25/12/19 13:45:09 INFO SharedState: Warehouse path is 'file:/workspace/airflow/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:09.156591Z","level":"info","event":"25/12/19 13:45:09 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:09.160487Z","level":"info","event":"25/12/19 13:45:09 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:09.163604Z","level":"info","event":"25/12/19 13:45:09 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:09.165683Z","level":"info","event":"25/12/19 13:45:09 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:09.168984Z","level":"info","event":"25/12/19 13:45:09 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:13.734774Z","level":"info","event":"25/12/19 13:45:13 INFO InMemoryFileIndex: It took 849 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:15.938461Z","level":"info","event":"25/12/19 13:45:15 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:50658) with ID 1,  ResourceProfileId 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:16.157742Z","level":"info","event":"25/12/19 13:45:16 INFO BlockManagerMasterEndpoint: Registering block manager datanode-nodemanager-2:36751 with 434.4 MiB RAM, BlockManagerId(1, datanode-nodemanager-2, 36751, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:16.513921Z","level":"info","event":"25/12/19 13:45:16 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:16.577473Z","level":"info","event":"25/12/19 13:45:16 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:16.578183Z","level":"info","event":"25/12/19 13:45:16 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:16.582737Z","level":"info","event":"25/12/19 13:45:16 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:16.585393Z","level":"info","event":"25/12/19 13:45:16 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:16.596974Z","level":"info","event":"25/12/19 13:45:16 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:16.834762Z","level":"info","event":"25/12/19 13:45:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.9 KiB, free 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:16.953529Z","level":"info","event":"25/12/19 13:45:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 38.1 KiB, free 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:16.959864Z","level":"info","event":"25/12/19 13:45:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 12d706d757b9:38241 (size: 38.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:16.971489Z","level":"info","event":"25/12/19 13:45:16 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:17.016583Z","level":"info","event":"25/12/19 13:45:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:17.019458Z","level":"info","event":"25/12/19 13:45:17 INFO YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:17.094359Z","level":"info","event":"25/12/19 13:45:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (datanode-nodemanager-2, executor 1, partition 0, PROCESS_LOCAL, 9195 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:17.591906Z","level":"info","event":"25/12/19 13:45:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on datanode-nodemanager-2:36751 (size: 38.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:20.444226Z","level":"info","event":"25/12/19 13:45:20 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3376 ms on datanode-nodemanager-2 (executor 1) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:20.453718Z","level":"info","event":"25/12/19 13:45:20 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:20.475879Z","level":"info","event":"25/12/19 13:45:20 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 3.819 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:20.486188Z","level":"info","event":"25/12/19 13:45:20 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:20.486465Z","level":"info","event":"25/12/19 13:45:20 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:20.492115Z","level":"info","event":"25/12/19 13:45:20 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 3.977670 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:20.624790Z","level":"info","event":"25/12/19 13:45:20 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:50440) with ID 2,  ResourceProfileId 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:20.757960Z","level":"info","event":"25/12/19 13:45:20 INFO BlockManagerMasterEndpoint: Registering block manager datanode-nodemanager-1:35737 with 434.4 MiB RAM, BlockManagerId(2, datanode-nodemanager-1, 35737, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:21.499957Z","level":"info","event":"25/12/19 13:45:21 INFO BlockManagerInfo: Removed broadcast_0_piece0 on datanode-nodemanager-2:36751 in memory (size: 38.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:21.506411Z","level":"info","event":"25/12/19 13:45:21 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 12d706d757b9:38241 in memory (size: 38.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:27.919651Z","level":"info","event":"25/12/19 13:45:27 INFO DataSourceStrategy: Pruning directories with: isnotnull(ingest_date#21),(ingest_date#21 >= 2025-09-01),(ingest_date#21 < 2025-09-08)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:27.929633Z","level":"info","event":"25/12/19 13:45:27 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:27.932517Z","level":"info","event":"25/12/19 13:45:27 INFO FileSourceStrategy: Post-Scan Filters: (size(from_json(ArrayType(StructType(StructField(topic,StringType,true)),true), topics_json#19, Some(GMT)).topic, true) > 0),isnotnull(from_json(ArrayType(StructType(StructField(topic,StringType,true)),true), topics_json#19, Some(GMT)).topic)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:29.751665Z","level":"info","event":"25/12/19 13:45:29 INFO CodeGenerator: Code generated in 912.44925 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:29.822407Z","level":"info","event":"25/12/19 13:45:29 INFO CodeGenerator: Code generated in 47.17376 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:29.872678Z","level":"info","event":"25/12/19 13:45:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 202.6 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:29.904540Z","level":"info","event":"25/12/19 13:45:29 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:29.904929Z","level":"info","event":"25/12/19 13:45:29 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 12d706d757b9:38241 (size: 35.9 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:29.912538Z","level":"info","event":"25/12/19 13:45:29 INFO SparkContext: Created broadcast 1 from showString at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:29.967613Z","level":"info","event":"25/12/19 13:45:29 INFO InMemoryFileIndex: Selected 7 partitions out of 16, pruned 56.25% partitions.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:30.006840Z","level":"info","event":"25/12/19 13:45:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:30.324000Z","level":"info","event":"25/12/19 13:45:30 INFO DAGScheduler: Registering RDD 8 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:30.336668Z","level":"info","event":"25/12/19 13:45:30 INFO DAGScheduler: Got map stage job 1 (showString at NativeMethodAccessorImpl.java:0) with 14 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:30.337982Z","level":"info","event":"25/12/19 13:45:30 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (showString at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:30.338676Z","level":"info","event":"25/12/19 13:45:30 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:30.343261Z","level":"info","event":"25/12/19 13:45:30 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:30.348607Z","level":"info","event":"25/12/19 13:45:30 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:30.425748Z","level":"info","event":"25/12/19 13:45:30 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 46.5 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:30.449048Z","level":"info","event":"25/12/19 13:45:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.3 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:30.450823Z","level":"info","event":"25/12/19 13:45:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 12d706d757b9:38241 (size: 20.3 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:30.451830Z","level":"info","event":"25/12/19 13:45:30 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:30.456005Z","level":"info","event":"25/12/19 13:45:30 INFO DAGScheduler: Submitting 14 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:30.456187Z","level":"info","event":"25/12/19 13:45:30 INFO YarnScheduler: Adding task set 1.0 with 14 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:30.500055Z","level":"info","event":"25/12/19 13:45:30 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (datanode-nodemanager-1, executor 2, partition 0, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:30.501140Z","level":"info","event":"25/12/19 13:45:30 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (datanode-nodemanager-2, executor 1, partition 1, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:30.619041Z","level":"info","event":"25/12/19 13:45:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on datanode-nodemanager-2:36751 (size: 20.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:31.046591Z","level":"info","event":"25/12/19 13:45:31 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on datanode-nodemanager-1:35737 (size: 20.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:33.166663Z","level":"info","event":"25/12/19 13:45:33 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on datanode-nodemanager-2:36751 (size: 35.9 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:34.266692Z","level":"info","event":"25/12/19 13:45:34 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on datanode-nodemanager-1:35737 (size: 35.9 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:36.069476Z","level":"info","event":"25/12/19 13:45:36 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (datanode-nodemanager-2, executor 1, partition 2, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:36.079347Z","level":"info","event":"25/12/19 13:45:36 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 5578 ms on datanode-nodemanager-2 (executor 1) (1/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:37.215562Z","level":"info","event":"25/12/19 13:45:37 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (datanode-nodemanager-2, executor 1, partition 3, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:37.216827Z","level":"info","event":"25/12/19 13:45:37 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 1152 ms on datanode-nodemanager-2 (executor 1) (2/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:38.212844Z","level":"info","event":"25/12/19 13:45:38 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (datanode-nodemanager-2, executor 1, partition 4, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:38.215863Z","level":"info","event":"25/12/19 13:45:38 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 1000 ms on datanode-nodemanager-2 (executor 1) (3/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:39.154416Z","level":"info","event":"25/12/19 13:45:39 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (datanode-nodemanager-2, executor 1, partition 5, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:39.157722Z","level":"info","event":"25/12/19 13:45:39 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 946 ms on datanode-nodemanager-2 (executor 1) (4/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:39.331813Z","level":"info","event":"25/12/19 13:45:39 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (datanode-nodemanager-1, executor 2, partition 6, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:39.332107Z","level":"info","event":"25/12/19 13:45:39 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 8849 ms on datanode-nodemanager-1 (executor 2) (5/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:40.047841Z","level":"info","event":"25/12/19 13:45:40 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (datanode-nodemanager-2, executor 1, partition 7, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:40.048090Z","level":"info","event":"25/12/19 13:45:40 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 894 ms on datanode-nodemanager-2 (executor 1) (6/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:40.358721Z","level":"info","event":"25/12/19 13:45:40 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9) (datanode-nodemanager-1, executor 2, partition 8, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:40.360207Z","level":"info","event":"25/12/19 13:45:40 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 1041 ms on datanode-nodemanager-1 (executor 2) (7/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:40.925884Z","level":"info","event":"25/12/19 13:45:40 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 10) (datanode-nodemanager-2, executor 1, partition 9, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:40.926129Z","level":"info","event":"25/12/19 13:45:40 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 880 ms on datanode-nodemanager-2 (executor 1) (8/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:41.305002Z","level":"info","event":"25/12/19 13:45:41 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 11) (datanode-nodemanager-1, executor 2, partition 10, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:41.306121Z","level":"info","event":"25/12/19 13:45:41 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 948 ms on datanode-nodemanager-1 (executor 2) (9/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:41.818426Z","level":"info","event":"25/12/19 13:45:41 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 12) (datanode-nodemanager-2, executor 1, partition 11, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:41.832633Z","level":"info","event":"25/12/19 13:45:41 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 10) in 898 ms on datanode-nodemanager-2 (executor 1) (10/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:42.464685Z","level":"info","event":"25/12/19 13:45:42 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 13) (datanode-nodemanager-1, executor 2, partition 12, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:42.470459Z","level":"info","event":"25/12/19 13:45:42 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 11) in 1164 ms on datanode-nodemanager-1 (executor 2) (11/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:42.643007Z","level":"info","event":"25/12/19 13:45:42 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 14) (datanode-nodemanager-2, executor 1, partition 13, NODE_LOCAL, 13862 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:42.643954Z","level":"info","event":"25/12/19 13:45:42 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 12) in 826 ms on datanode-nodemanager-2 (executor 1) (12/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.116556Z","level":"info","event":"25/12/19 13:45:43 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 13) in 654 ms on datanode-nodemanager-1 (executor 2) (13/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.123359Z","level":"info","event":"25/12/19 13:45:43 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 14) in 481 ms on datanode-nodemanager-2 (executor 1) (14/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.123599Z","level":"info","event":"25/12/19 13:45:43 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.128562Z","level":"info","event":"25/12/19 13:45:43 INFO DAGScheduler: ShuffleMapStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 12.754 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.128774Z","level":"info","event":"25/12/19 13:45:43 INFO DAGScheduler: looking for newly runnable stages","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.130146Z","level":"info","event":"25/12/19 13:45:43 INFO DAGScheduler: running: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.131153Z","level":"info","event":"25/12/19 13:45:43 INFO DAGScheduler: waiting: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.131742Z","level":"info","event":"25/12/19 13:45:43 INFO DAGScheduler: failed: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.187186Z","level":"info","event":"25/12/19 13:45:43 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.360842Z","level":"info","event":"25/12/19 13:45:43 INFO CodeGenerator: Code generated in 109.026639 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.459526Z","level":"info","event":"25/12/19 13:45:43 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.467178Z","level":"info","event":"25/12/19 13:45:43 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.467806Z","level":"info","event":"25/12/19 13:45:43 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.467953Z","level":"info","event":"25/12/19 13:45:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.470719Z","level":"info","event":"25/12/19 13:45:43 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.476741Z","level":"info","event":"25/12/19 13:45:43 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.512211Z","level":"info","event":"25/12/19 13:45:43 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 48.9 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.542455Z","level":"info","event":"25/12/19 13:45:43 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 21.7 KiB, free 434.0 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.543882Z","level":"info","event":"25/12/19 13:45:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 12d706d757b9:38241 (size: 21.7 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.547138Z","level":"info","event":"25/12/19 13:45:43 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.547317Z","level":"info","event":"25/12/19 13:45:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.547380Z","level":"info","event":"25/12/19 13:45:43 INFO YarnScheduler: Adding task set 3.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.560191Z","level":"info","event":"25/12/19 13:45:43 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 12d706d757b9:38241 in memory (size: 20.3 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.561951Z","level":"info","event":"25/12/19 13:45:43 INFO BlockManagerInfo: Removed broadcast_2_piece0 on datanode-nodemanager-2:36751 in memory (size: 20.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.587943Z","level":"info","event":"25/12/19 13:45:43 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 15) (datanode-nodemanager-2, executor 1, partition 0, NODE_LOCAL, 9010 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.589616Z","level":"info","event":"25/12/19 13:45:43 INFO BlockManagerInfo: Removed broadcast_2_piece0 on datanode-nodemanager-1:35737 in memory (size: 20.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.680558Z","level":"info","event":"25/12/19 13:45:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on datanode-nodemanager-2:36751 (size: 21.7 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:43.855318Z","level":"info","event":"25/12/19 13:45:43 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.3:50658","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:44.315546Z","level":"info","event":"25/12/19 13:45:44 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 15) in 741 ms on datanode-nodemanager-2 (executor 1) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:44.315853Z","level":"info","event":"25/12/19 13:45:44 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:44.317596Z","level":"info","event":"25/12/19 13:45:44 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.813 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:44.318449Z","level":"info","event":"25/12/19 13:45:44 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:44.318623Z","level":"info","event":"25/12/19 13:45:44 INFO YarnScheduler: Killing all running tasks in stage 3: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:44.319001Z","level":"info","event":"25/12/19 13:45:44 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.858254 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:45.128766Z","level":"info","event":"25/12/19 13:45:45 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 12d706d757b9:38241 in memory (size: 21.7 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:45.133429Z","level":"info","event":"25/12/19 13:45:45 INFO BlockManagerInfo: Removed broadcast_3_piece0 on datanode-nodemanager-2:36751 in memory (size: 21.7 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:46.978847Z","level":"info","event":"25/12/19 13:45:46 INFO CodeGenerator: Code generated in 28.54188 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:47.014424Z","level":"info","event":"+--------------------+--------------------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:47.014605Z","level":"info","event":"|            topic_id|          topic_name|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:47.014670Z","level":"info","event":"+--------------------+--------------------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:47.014725Z","level":"info","event":"| 7262183755669310342|                 IPO|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:47.014777Z","level":"info","event":"| 2398001789366731595|    Economy - Fiscal|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:47.014825Z","level":"info","event":"|-8706132297326582471|     Economy - Macro|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:47.014871Z","level":"info","event":"|-1496748679391501014|             Finance|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:47.014916Z","level":"info","event":"| -674034787253727345|            Earnings|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:47.014961Z","level":"info","event":"| 6330205094660838234|Real Estate & Con...|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:47.015009Z","level":"info","event":"| 2354747098425553887|  Retail & Wholesale|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:47.015058Z","level":"info","event":"| 2704355848778060929|Energy & Transpor...|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:47.015105Z","level":"info","event":"|  224347739280556027|          Technology|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:47.015156Z","level":"info","event":"| -448291436371370118|   Financial Markets|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:47.015203Z","level":"info","event":"| 5998011884746003305|  Economy - Monetary|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:47.015251Z","level":"info","event":"| -900445728666717830|Mergers & Acquisi...|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:47.015298Z","level":"info","event":"| 8225716003774577002|       Manufacturing|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:47.015342Z","level":"info","event":"| 8636157333299584077|       Life Sciences|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:47.015388Z","level":"info","event":"| 6135255710989356306|          Blockchain|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:47.015434Z","level":"info","event":"+--------------------+--------------------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:47.015478Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.471846Z","level":"info","event":"25/12/19 13:45:48 INFO DataSourceStrategy: Pruning directories with: isnotnull(ingest_date#21),(ingest_date#21 >= 2025-09-01),(ingest_date#21 < 2025-09-08)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.473473Z","level":"info","event":"25/12/19 13:45:48 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.474261Z","level":"info","event":"25/12/19 13:45:48 INFO FileSourceStrategy: Post-Scan Filters: (size(from_json(ArrayType(StructType(StructField(topic,StringType,true)),true), topics_json#19, Some(GMT)).topic, true) > 0),isnotnull(from_json(ArrayType(StructType(StructField(topic,StringType,true)),true), topics_json#19, Some(GMT)).topic)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.538406Z","level":"info","event":"25/12/19 13:45:48 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 202.6 KiB, free 434.0 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.564474Z","level":"info","event":"25/12/19 13:45:48 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.566410Z","level":"info","event":"25/12/19 13:45:48 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 12d706d757b9:38241 (size: 35.9 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.567859Z","level":"info","event":"25/12/19 13:45:48 INFO SparkContext: Created broadcast 4 from jdbc at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.570356Z","level":"info","event":"25/12/19 13:45:48 INFO InMemoryFileIndex: Selected 7 partitions out of 16, pruned 56.25% partitions.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.572980Z","level":"info","event":"25/12/19 13:45:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.634473Z","level":"info","event":"25/12/19 13:45:48 INFO DAGScheduler: Registering RDD 18 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.634773Z","level":"info","event":"25/12/19 13:45:48 INFO DAGScheduler: Got map stage job 3 (jdbc at NativeMethodAccessorImpl.java:0) with 14 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.635507Z","level":"info","event":"25/12/19 13:45:48 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (jdbc at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.635565Z","level":"info","event":"25/12/19 13:45:48 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.656960Z","level":"info","event":"25/12/19 13:45:48 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.657532Z","level":"info","event":"25/12/19 13:45:48 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 12d706d757b9:38241 in memory (size: 35.9 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.659752Z","level":"info","event":"25/12/19 13:45:48 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[18] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.667751Z","level":"info","event":"25/12/19 13:45:48 INFO BlockManagerInfo: Removed broadcast_1_piece0 on datanode-nodemanager-1:35737 in memory (size: 35.9 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.680698Z","level":"info","event":"25/12/19 13:45:48 INFO BlockManagerInfo: Removed broadcast_1_piece0 on datanode-nodemanager-2:36751 in memory (size: 35.9 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.691811Z","level":"info","event":"25/12/19 13:45:48 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 46.5 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.722036Z","level":"info","event":"25/12/19 13:45:48 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 20.3 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.723244Z","level":"info","event":"25/12/19 13:45:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 12d706d757b9:38241 (size: 20.3 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.724993Z","level":"info","event":"25/12/19 13:45:48 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.726732Z","level":"info","event":"25/12/19 13:45:48 INFO DAGScheduler: Submitting 14 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[18] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.728714Z","level":"info","event":"25/12/19 13:45:48 INFO YarnScheduler: Adding task set 4.0 with 14 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.737178Z","level":"info","event":"25/12/19 13:45:48 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 16) (datanode-nodemanager-2, executor 1, partition 0, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.738543Z","level":"info","event":"25/12/19 13:45:48 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 17) (datanode-nodemanager-1, executor 2, partition 1, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.775741Z","level":"info","event":"25/12/19 13:45:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on datanode-nodemanager-2:36751 (size: 20.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.812374Z","level":"info","event":"25/12/19 13:45:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on datanode-nodemanager-1:35737 (size: 20.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.858097Z","level":"info","event":"25/12/19 13:45:48 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on datanode-nodemanager-2:36751 (size: 35.9 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:48.961020Z","level":"info","event":"25/12/19 13:45:48 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on datanode-nodemanager-1:35737 (size: 35.9 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:49.864152Z","level":"info","event":"25/12/19 13:45:49 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 18) (datanode-nodemanager-2, executor 1, partition 2, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:49.865809Z","level":"info","event":"25/12/19 13:45:49 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 16) in 1129 ms on datanode-nodemanager-2 (executor 1) (1/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:49.975263Z","level":"info","event":"25/12/19 13:45:49 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 19) (datanode-nodemanager-1, executor 2, partition 3, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:49.975442Z","level":"info","event":"25/12/19 13:45:49 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 17) in 1234 ms on datanode-nodemanager-1 (executor 2) (2/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:50.472661Z","level":"info","event":"25/12/19 13:45:50 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 20) (datanode-nodemanager-2, executor 1, partition 4, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:50.474344Z","level":"info","event":"25/12/19 13:45:50 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 18) in 612 ms on datanode-nodemanager-2 (executor 1) (3/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:50.619432Z","level":"info","event":"25/12/19 13:45:50 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 21) (datanode-nodemanager-1, executor 2, partition 5, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:50.622436Z","level":"info","event":"25/12/19 13:45:50 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 19) in 653 ms on datanode-nodemanager-1 (executor 2) (4/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:51.114381Z","level":"info","event":"25/12/19 13:45:51 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 22) (datanode-nodemanager-2, executor 1, partition 6, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:51.116903Z","level":"info","event":"25/12/19 13:45:51 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 20) in 645 ms on datanode-nodemanager-2 (executor 1) (5/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:51.363817Z","level":"info","event":"25/12/19 13:45:51 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 23) (datanode-nodemanager-1, executor 2, partition 7, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:51.365710Z","level":"info","event":"25/12/19 13:45:51 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 21) in 747 ms on datanode-nodemanager-1 (executor 2) (6/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:51.722389Z","level":"info","event":"25/12/19 13:45:51 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 24) (datanode-nodemanager-2, executor 1, partition 8, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:51.727361Z","level":"info","event":"25/12/19 13:45:51 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 22) in 611 ms on datanode-nodemanager-2 (executor 1) (7/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:52.040252Z","level":"info","event":"25/12/19 13:45:52 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 25) (datanode-nodemanager-1, executor 2, partition 9, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:52.042760Z","level":"info","event":"25/12/19 13:45:52 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 23) in 679 ms on datanode-nodemanager-1 (executor 2) (8/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:52.358853Z","level":"info","event":"25/12/19 13:45:52 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 26) (datanode-nodemanager-2, executor 1, partition 10, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:52.359570Z","level":"info","event":"25/12/19 13:45:52 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 24) in 636 ms on datanode-nodemanager-2 (executor 1) (9/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:52.869769Z","level":"info","event":"25/12/19 13:45:52 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 27) (datanode-nodemanager-1, executor 2, partition 11, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:52.872515Z","level":"info","event":"25/12/19 13:45:52 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 25) in 832 ms on datanode-nodemanager-1 (executor 2) (10/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:53.175996Z","level":"info","event":"25/12/19 13:45:53 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 28) (datanode-nodemanager-2, executor 1, partition 12, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:53.179911Z","level":"info","event":"25/12/19 13:45:53 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 26) in 820 ms on datanode-nodemanager-2 (executor 1) (11/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:53.552547Z","level":"info","event":"25/12/19 13:45:53 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 29) (datanode-nodemanager-1, executor 2, partition 13, NODE_LOCAL, 13862 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:53.554043Z","level":"info","event":"25/12/19 13:45:53 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 27) in 685 ms on datanode-nodemanager-1 (executor 2) (12/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:53.752898Z","level":"info","event":"25/12/19 13:45:53 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 28) in 575 ms on datanode-nodemanager-2 (executor 1) (13/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:54.062090Z","level":"info","event":"25/12/19 13:45:54 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 29) in 510 ms on datanode-nodemanager-1 (executor 2) (14/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:54.062317Z","level":"info","event":"25/12/19 13:45:54 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:54.063067Z","level":"info","event":"25/12/19 13:45:54 INFO DAGScheduler: ShuffleMapStage 4 (jdbc at NativeMethodAccessorImpl.java:0) finished in 5.388 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:54.063194Z","level":"info","event":"25/12/19 13:45:54 INFO DAGScheduler: looking for newly runnable stages","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:54.063259Z","level":"info","event":"25/12/19 13:45:54 INFO DAGScheduler: running: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:54.063318Z","level":"info","event":"25/12/19 13:45:54 INFO DAGScheduler: waiting: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:54.063378Z","level":"info","event":"25/12/19 13:45:54 INFO DAGScheduler: failed: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:54.069411Z","level":"info","event":"25/12/19 13:45:54 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:54.147335Z","level":"info","event":"25/12/19 13:45:54 INFO CodeGenerator: Code generated in 61.375143 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:54.276139Z","level":"info","event":"25/12/19 13:45:54 INFO SparkContext: Starting job: jdbc at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:54.278288Z","level":"info","event":"25/12/19 13:45:54 INFO DAGScheduler: Got job 4 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:54.278419Z","level":"info","event":"25/12/19 13:45:54 INFO DAGScheduler: Final stage: ResultStage 6 (jdbc at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:54.278458Z","level":"info","event":"25/12/19 13:45:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:54.282256Z","level":"info","event":"25/12/19 13:45:54 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:54.284194Z","level":"info","event":"25/12/19 13:45:54 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[23] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:54.307657Z","level":"info","event":"25/12/19 13:45:54 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 56.5 KiB, free 434.0 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:54.321867Z","level":"info","event":"25/12/19 13:45:54 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 25.0 KiB, free 434.0 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:54.328758Z","level":"info","event":"25/12/19 13:45:54 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 12d706d757b9:38241 (size: 25.0 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:54.330664Z","level":"info","event":"25/12/19 13:45:54 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 12d706d757b9:38241 in memory (size: 20.3 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:54.336172Z","level":"info","event":"25/12/19 13:45:54 INFO BlockManagerInfo: Removed broadcast_5_piece0 on datanode-nodemanager-1:35737 in memory (size: 20.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:54.336343Z","level":"info","event":"25/12/19 13:45:54 INFO BlockManagerInfo: Removed broadcast_5_piece0 on datanode-nodemanager-2:36751 in memory (size: 20.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:54.336419Z","level":"info","event":"25/12/19 13:45:54 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:54.337387Z","level":"info","event":"25/12/19 13:45:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[23] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:54.339430Z","level":"info","event":"25/12/19 13:45:54 INFO YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:54.339577Z","level":"info","event":"25/12/19 13:45:54 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 30) (datanode-nodemanager-1, executor 2, partition 0, NODE_LOCAL, 9010 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:54.407493Z","level":"info","event":"25/12/19 13:45:54 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on datanode-nodemanager-1:35737 (size: 25.0 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:45:54.851025Z","level":"info","event":"25/12/19 13:45:54 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.18.0.4:50440","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.101605Z","level":"info","event":"25/12/19 13:46:01 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 30) in 6764 ms on datanode-nodemanager-1 (executor 2) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.101904Z","level":"info","event":"25/12/19 13:46:01 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.105977Z","level":"info","event":"25/12/19 13:46:01 INFO DAGScheduler: ResultStage 6 (jdbc at NativeMethodAccessorImpl.java:0) finished in 6.815 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.107051Z","level":"info","event":"25/12/19 13:46:01 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.107243Z","level":"info","event":"25/12/19 13:46:01 INFO YarnScheduler: Killing all running tasks in stage 6: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.110305Z","level":"info","event":"25/12/19 13:46:01 INFO DAGScheduler: Job 4 finished: jdbc at NativeMethodAccessorImpl.java:0, took 3.479860 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.450413Z","level":"info","event":"25/12/19 13:46:01 INFO DataSourceStrategy: Pruning directories with: isnotnull(ingest_date#21),(ingest_date#21 >= 2025-09-01),(ingest_date#21 < 2025-09-08)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.452675Z","level":"info","event":"25/12/19 13:46:01 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.452992Z","level":"info","event":"25/12/19 13:46:01 INFO FileSourceStrategy: Post-Scan Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.581776Z","level":"info","event":"25/12/19 13:46:01 INFO CodeGenerator: Code generated in 73.896235 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.591307Z","level":"info","event":"25/12/19 13:46:01 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 202.5 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.622459Z","level":"info","event":"25/12/19 13:46:01 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 12d706d757b9:38241 in memory (size: 25.0 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.630919Z","level":"info","event":"25/12/19 13:46:01 INFO BlockManagerInfo: Removed broadcast_6_piece0 on datanode-nodemanager-1:35737 in memory (size: 25.0 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.642840Z","level":"info","event":"25/12/19 13:46:01 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 35.8 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.647180Z","level":"info","event":"25/12/19 13:46:01 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 12d706d757b9:38241 (size: 35.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.652234Z","level":"info","event":"25/12/19 13:46:01 INFO SparkContext: Created broadcast 7 from showString at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.655232Z","level":"info","event":"25/12/19 13:46:01 INFO InMemoryFileIndex: Selected 7 partitions out of 16, pruned 56.25% partitions.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.659364Z","level":"info","event":"25/12/19 13:46:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.729778Z","level":"info","event":"25/12/19 13:46:01 INFO DAGScheduler: Registering RDD 27 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 2","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.730474Z","level":"info","event":"25/12/19 13:46:01 INFO DAGScheduler: Got map stage job 5 (showString at NativeMethodAccessorImpl.java:0) with 14 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.730681Z","level":"info","event":"25/12/19 13:46:01 INFO DAGScheduler: Final stage: ShuffleMapStage 7 (showString at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.730753Z","level":"info","event":"25/12/19 13:46:01 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.743069Z","level":"info","event":"25/12/19 13:46:01 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.751839Z","level":"info","event":"25/12/19 13:46:01 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[27] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.767948Z","level":"info","event":"25/12/19 13:46:01 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 37.1 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.772322Z","level":"info","event":"25/12/19 13:46:01 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.772721Z","level":"info","event":"25/12/19 13:46:01 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 12d706d757b9:38241 (size: 16.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.783692Z","level":"info","event":"25/12/19 13:46:01 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.783941Z","level":"info","event":"25/12/19 13:46:01 INFO DAGScheduler: Submitting 14 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[27] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.784070Z","level":"info","event":"25/12/19 13:46:01 INFO YarnScheduler: Adding task set 7.0 with 14 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.804406Z","level":"info","event":"25/12/19 13:46:01 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 31) (datanode-nodemanager-1, executor 2, partition 0, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.804665Z","level":"info","event":"25/12/19 13:46:01 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 32) (datanode-nodemanager-2, executor 1, partition 1, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.831981Z","level":"info","event":"25/12/19 13:46:01 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on datanode-nodemanager-1:35737 (size: 16.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:01.850663Z","level":"info","event":"25/12/19 13:46:01 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on datanode-nodemanager-2:36751 (size: 16.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:02.026851Z","level":"info","event":"25/12/19 13:46:02 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on datanode-nodemanager-2:36751 (size: 35.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:02.081910Z","level":"info","event":"25/12/19 13:46:02 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on datanode-nodemanager-1:35737 (size: 35.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:02.731952Z","level":"info","event":"25/12/19 13:46:02 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 33) (datanode-nodemanager-2, executor 1, partition 2, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:02.737475Z","level":"info","event":"25/12/19 13:46:02 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 32) in 942 ms on datanode-nodemanager-2 (executor 1) (1/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:02.868796Z","level":"info","event":"25/12/19 13:46:02 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 34) (datanode-nodemanager-1, executor 2, partition 3, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:02.878700Z","level":"info","event":"25/12/19 13:46:02 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 31) in 1080 ms on datanode-nodemanager-1 (executor 2) (2/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:03.167560Z","level":"info","event":"25/12/19 13:46:03 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 35) (datanode-nodemanager-2, executor 1, partition 4, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:03.169160Z","level":"info","event":"25/12/19 13:46:03 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 33) in 439 ms on datanode-nodemanager-2 (executor 1) (3/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:03.337718Z","level":"info","event":"25/12/19 13:46:03 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 36) (datanode-nodemanager-1, executor 2, partition 5, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:03.339076Z","level":"info","event":"25/12/19 13:46:03 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 34) in 471 ms on datanode-nodemanager-1 (executor 2) (4/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:03.636150Z","level":"info","event":"25/12/19 13:46:03 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 37) (datanode-nodemanager-2, executor 1, partition 6, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:03.638122Z","level":"info","event":"25/12/19 13:46:03 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 35) in 470 ms on datanode-nodemanager-2 (executor 1) (5/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:03.809088Z","level":"info","event":"25/12/19 13:46:03 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 38) (datanode-nodemanager-1, executor 2, partition 7, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:03.809281Z","level":"info","event":"25/12/19 13:46:03 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 36) in 472 ms on datanode-nodemanager-1 (executor 2) (6/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:04.123159Z","level":"info","event":"25/12/19 13:46:04 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 39) (datanode-nodemanager-2, executor 1, partition 8, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:04.124336Z","level":"info","event":"25/12/19 13:46:04 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 37) in 488 ms on datanode-nodemanager-2 (executor 1) (7/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:04.348973Z","level":"info","event":"25/12/19 13:46:04 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 40) (datanode-nodemanager-1, executor 2, partition 9, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:04.349856Z","level":"info","event":"25/12/19 13:46:04 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 38) in 544 ms on datanode-nodemanager-1 (executor 2) (8/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:04.571427Z","level":"info","event":"25/12/19 13:46:04 INFO TaskSetManager: Starting task 10.0 in stage 7.0 (TID 41) (datanode-nodemanager-2, executor 1, partition 10, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:04.574106Z","level":"info","event":"25/12/19 13:46:04 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 39) in 450 ms on datanode-nodemanager-2 (executor 1) (9/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:04.788047Z","level":"info","event":"25/12/19 13:46:04 INFO TaskSetManager: Starting task 11.0 in stage 7.0 (TID 42) (datanode-nodemanager-1, executor 2, partition 11, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:04.789551Z","level":"info","event":"25/12/19 13:46:04 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 40) in 440 ms on datanode-nodemanager-1 (executor 2) (10/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.016993Z","level":"info","event":"25/12/19 13:46:05 INFO TaskSetManager: Starting task 12.0 in stage 7.0 (TID 43) (datanode-nodemanager-2, executor 1, partition 12, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.018090Z","level":"info","event":"25/12/19 13:46:05 INFO TaskSetManager: Finished task 10.0 in stage 7.0 (TID 41) in 447 ms on datanode-nodemanager-2 (executor 1) (11/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.191615Z","level":"info","event":"25/12/19 13:46:05 INFO TaskSetManager: Starting task 13.0 in stage 7.0 (TID 44) (datanode-nodemanager-1, executor 2, partition 13, NODE_LOCAL, 13862 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.193988Z","level":"info","event":"25/12/19 13:46:05 INFO TaskSetManager: Finished task 11.0 in stage 7.0 (TID 42) in 407 ms on datanode-nodemanager-1 (executor 2) (12/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.367894Z","level":"info","event":"25/12/19 13:46:05 INFO TaskSetManager: Finished task 12.0 in stage 7.0 (TID 43) in 352 ms on datanode-nodemanager-2 (executor 1) (13/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.548286Z","level":"info","event":"25/12/19 13:46:05 INFO TaskSetManager: Finished task 13.0 in stage 7.0 (TID 44) in 357 ms on datanode-nodemanager-1 (executor 2) (14/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.548537Z","level":"info","event":"25/12/19 13:46:05 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.549672Z","level":"info","event":"25/12/19 13:46:05 INFO DAGScheduler: ShuffleMapStage 7 (showString at NativeMethodAccessorImpl.java:0) finished in 3.794 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.549848Z","level":"info","event":"25/12/19 13:46:05 INFO DAGScheduler: looking for newly runnable stages","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.549925Z","level":"info","event":"25/12/19 13:46:05 INFO DAGScheduler: running: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.549980Z","level":"info","event":"25/12/19 13:46:05 INFO DAGScheduler: waiting: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.550034Z","level":"info","event":"25/12/19 13:46:05 INFO DAGScheduler: failed: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.556082Z","level":"info","event":"25/12/19 13:46:05 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.659061Z","level":"info","event":"25/12/19 13:46:05 INFO CodeGenerator: Code generated in 78.245445 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.706480Z","level":"info","event":"25/12/19 13:46:05 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.708830Z","level":"info","event":"25/12/19 13:46:05 INFO DAGScheduler: Got job 6 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.709073Z","level":"info","event":"25/12/19 13:46:05 INFO DAGScheduler: Final stage: ResultStage 9 (showString at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.709163Z","level":"info","event":"25/12/19 13:46:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.711555Z","level":"info","event":"25/12/19 13:46:05 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.713663Z","level":"info","event":"25/12/19 13:46:05 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[30] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.725047Z","level":"info","event":"25/12/19 13:46:05 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 47.5 KiB, free 433.8 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.742989Z","level":"info","event":"25/12/19 13:46:05 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 20.8 KiB, free 433.8 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.745884Z","level":"info","event":"25/12/19 13:46:05 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 12d706d757b9:38241 (size: 20.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.754491Z","level":"info","event":"25/12/19 13:46:05 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 12d706d757b9:38241 in memory (size: 16.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.755892Z","level":"info","event":"25/12/19 13:46:05 INFO BlockManagerInfo: Removed broadcast_8_piece0 on datanode-nodemanager-2:36751 in memory (size: 16.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.756694Z","level":"info","event":"25/12/19 13:46:05 INFO BlockManagerInfo: Removed broadcast_8_piece0 on datanode-nodemanager-1:35737 in memory (size: 16.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.760770Z","level":"info","event":"25/12/19 13:46:05 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.761006Z","level":"info","event":"25/12/19 13:46:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[30] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.763080Z","level":"info","event":"25/12/19 13:46:05 INFO YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.770735Z","level":"info","event":"25/12/19 13:46:05 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 45) (datanode-nodemanager-1, executor 2, partition 0, NODE_LOCAL, 9010 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.808857Z","level":"info","event":"25/12/19 13:46:05 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on datanode-nodemanager-1:35737 (size: 20.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:05.905251Z","level":"info","event":"25/12/19 13:46:05 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.18.0.4:50440","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.105452Z","level":"info","event":"25/12/19 13:46:06 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 45) in 336 ms on datanode-nodemanager-1 (executor 2) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.105816Z","level":"info","event":"25/12/19 13:46:06 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.114353Z","level":"info","event":"25/12/19 13:46:06 INFO DAGScheduler: ResultStage 9 (showString at NativeMethodAccessorImpl.java:0) finished in 0.389 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.114573Z","level":"info","event":"25/12/19 13:46:06 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.114642Z","level":"info","event":"25/12/19 13:46:06 INFO YarnScheduler: Killing all running tasks in stage 9: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.114698Z","level":"info","event":"25/12/19 13:46:06 INFO DAGScheduler: Job 6 finished: showString at NativeMethodAccessorImpl.java:0, took 0.403743 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.173306Z","level":"info","event":"25/12/19 13:46:06 INFO CodeGenerator: Code generated in 34.297719 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.180394Z","level":"info","event":"+--------------------+----------+----------------+----------+------------+---------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.180570Z","level":"info","event":"|             time_id| time_date|time_day_of_week|time_month|time_quarter|time_year|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.180630Z","level":"info","event":"+--------------------+----------+----------------+----------+------------+---------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.180679Z","level":"info","event":"| 6367420500199088588|2025-09-04|               5|         9|           3|     2025|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.180725Z","level":"info","event":"| -222147908674804431|2025-09-07|               1|         9|           3|     2025|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.187048Z","level":"info","event":"|-7039825263568189942|2025-09-05|               6|         9|           3|     2025|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.187234Z","level":"info","event":"|-8675524983303160171|2025-09-03|               4|         9|           3|     2025|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.187298Z","level":"info","event":"| 2869404304239412763|2025-09-02|               3|         9|           3|     2025|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.187345Z","level":"info","event":"|-1861558615695025446|2025-09-01|               2|         9|           3|     2025|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.187388Z","level":"info","event":"|-5757472051673998136|2025-09-06|               7|         9|           3|     2025|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.187430Z","level":"info","event":"+--------------------+----------+----------------+----------+------------+---------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.187472Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.449910Z","level":"info","event":"25/12/19 13:46:06 INFO DataSourceStrategy: Pruning directories with: isnotnull(ingest_date#21),(ingest_date#21 >= 2025-09-01),(ingest_date#21 < 2025-09-08)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.464083Z","level":"info","event":"25/12/19 13:46:06 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.464499Z","level":"info","event":"25/12/19 13:46:06 INFO FileSourceStrategy: Post-Scan Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.526567Z","level":"info","event":"25/12/19 13:46:06 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 202.5 KiB, free 433.7 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.567487Z","level":"info","event":"25/12/19 13:46:06 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 35.8 KiB, free 433.6 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.567717Z","level":"info","event":"25/12/19 13:46:06 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 12d706d757b9:38241 (size: 35.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.569352Z","level":"info","event":"25/12/19 13:46:06 INFO SparkContext: Created broadcast 10 from jdbc at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.571412Z","level":"info","event":"25/12/19 13:46:06 INFO InMemoryFileIndex: Selected 7 partitions out of 16, pruned 56.25% partitions.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.574446Z","level":"info","event":"25/12/19 13:46:06 INFO BlockManagerInfo: Removed broadcast_7_piece0 on datanode-nodemanager-2:36751 in memory (size: 35.8 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.581731Z","level":"info","event":"25/12/19 13:46:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.582171Z","level":"info","event":"25/12/19 13:46:06 INFO BlockManagerInfo: Removed broadcast_7_piece0 on datanode-nodemanager-1:35737 in memory (size: 35.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.588271Z","level":"info","event":"25/12/19 13:46:06 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 12d706d757b9:38241 in memory (size: 35.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.605378Z","level":"info","event":"25/12/19 13:46:06 INFO DAGScheduler: Registering RDD 34 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 3","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.607295Z","level":"info","event":"25/12/19 13:46:06 INFO DAGScheduler: Got map stage job 7 (jdbc at NativeMethodAccessorImpl.java:0) with 14 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.608307Z","level":"info","event":"25/12/19 13:46:06 INFO DAGScheduler: Final stage: ShuffleMapStage 10 (jdbc at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.608950Z","level":"info","event":"25/12/19 13:46:06 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.622426Z","level":"info","event":"25/12/19 13:46:06 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.626871Z","level":"info","event":"25/12/19 13:46:06 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[34] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.640757Z","level":"info","event":"25/12/19 13:46:06 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 37.1 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.644584Z","level":"info","event":"25/12/19 13:46:06 INFO BlockManagerInfo: Removed broadcast_9_piece0 on datanode-nodemanager-1:35737 in memory (size: 20.8 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.645707Z","level":"info","event":"25/12/19 13:46:06 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 12d706d757b9:38241 in memory (size: 20.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.650953Z","level":"info","event":"25/12/19 13:46:06 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 433.8 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.657298Z","level":"info","event":"25/12/19 13:46:06 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 12d706d757b9:38241 (size: 16.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.663393Z","level":"info","event":"25/12/19 13:46:06 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.663582Z","level":"info","event":"25/12/19 13:46:06 INFO DAGScheduler: Submitting 14 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[34] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.663659Z","level":"info","event":"25/12/19 13:46:06 INFO YarnScheduler: Adding task set 10.0 with 14 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.672122Z","level":"info","event":"25/12/19 13:46:06 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 46) (datanode-nodemanager-2, executor 1, partition 0, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.672594Z","level":"info","event":"25/12/19 13:46:06 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 47) (datanode-nodemanager-1, executor 2, partition 1, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.776152Z","level":"info","event":"25/12/19 13:46:06 INFO BlockManagerInfo: Removed broadcast_4_piece0 on datanode-nodemanager-1:35737 in memory (size: 35.9 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.776370Z","level":"info","event":"25/12/19 13:46:06 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 12d706d757b9:38241 in memory (size: 35.9 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.776438Z","level":"info","event":"25/12/19 13:46:06 INFO BlockManagerInfo: Removed broadcast_4_piece0 on datanode-nodemanager-2:36751 in memory (size: 35.9 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.776486Z","level":"info","event":"25/12/19 13:46:06 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on datanode-nodemanager-2:36751 (size: 16.6 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.790538Z","level":"info","event":"25/12/19 13:46:06 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on datanode-nodemanager-1:35737 (size: 16.6 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.830265Z","level":"info","event":"25/12/19 13:46:06 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on datanode-nodemanager-2:36751 (size: 35.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:06.890189Z","level":"info","event":"25/12/19 13:46:06 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on datanode-nodemanager-1:35737 (size: 35.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:07.322021Z","level":"info","event":"25/12/19 13:46:07 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 46) in 651 ms on datanode-nodemanager-2 (executor 1) (1/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:07.330190Z","level":"info","event":"25/12/19 13:46:07 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 48) (datanode-nodemanager-2, executor 1, partition 2, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:07.453900Z","level":"info","event":"25/12/19 13:46:07 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 49) (datanode-nodemanager-1, executor 2, partition 3, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:07.455217Z","level":"info","event":"25/12/19 13:46:07 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 47) in 784 ms on datanode-nodemanager-1 (executor 2) (2/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:07.793826Z","level":"info","event":"25/12/19 13:46:07 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 50) (datanode-nodemanager-2, executor 1, partition 4, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:07.795126Z","level":"info","event":"25/12/19 13:46:07 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 48) in 465 ms on datanode-nodemanager-2 (executor 1) (3/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:07.915936Z","level":"info","event":"25/12/19 13:46:07 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 51) (datanode-nodemanager-1, executor 2, partition 5, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:07.916349Z","level":"info","event":"25/12/19 13:46:07 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 49) in 461 ms on datanode-nodemanager-1 (executor 2) (4/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:08.366817Z","level":"info","event":"25/12/19 13:46:08 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 52) (datanode-nodemanager-2, executor 1, partition 6, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:08.367125Z","level":"info","event":"25/12/19 13:46:08 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 50) in 574 ms on datanode-nodemanager-2 (executor 1) (5/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:08.606621Z","level":"info","event":"25/12/19 13:46:08 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 53) (datanode-nodemanager-1, executor 2, partition 7, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:08.607240Z","level":"info","event":"25/12/19 13:46:08 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 51) in 702 ms on datanode-nodemanager-1 (executor 2) (6/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:08.951454Z","level":"info","event":"25/12/19 13:46:08 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 54) (datanode-nodemanager-2, executor 1, partition 8, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:08.952795Z","level":"info","event":"25/12/19 13:46:08 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 52) in 587 ms on datanode-nodemanager-2 (executor 1) (7/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:09.190249Z","level":"info","event":"25/12/19 13:46:09 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 55) (datanode-nodemanager-1, executor 2, partition 9, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:09.191434Z","level":"info","event":"25/12/19 13:46:09 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 53) in 587 ms on datanode-nodemanager-1 (executor 2) (8/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:09.437972Z","level":"info","event":"25/12/19 13:46:09 INFO TaskSetManager: Starting task 10.0 in stage 10.0 (TID 56) (datanode-nodemanager-2, executor 1, partition 10, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:09.438893Z","level":"info","event":"25/12/19 13:46:09 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 54) in 488 ms on datanode-nodemanager-2 (executor 1) (9/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:09.609345Z","level":"info","event":"25/12/19 13:46:09 INFO TaskSetManager: Starting task 11.0 in stage 10.0 (TID 57) (datanode-nodemanager-1, executor 2, partition 11, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:09.611565Z","level":"info","event":"25/12/19 13:46:09 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 55) in 421 ms on datanode-nodemanager-1 (executor 2) (10/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:09.838079Z","level":"info","event":"25/12/19 13:46:09 INFO TaskSetManager: Starting task 12.0 in stage 10.0 (TID 58) (datanode-nodemanager-2, executor 1, partition 12, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:09.839194Z","level":"info","event":"25/12/19 13:46:09 INFO TaskSetManager: Finished task 10.0 in stage 10.0 (TID 56) in 401 ms on datanode-nodemanager-2 (executor 1) (11/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:10.158183Z","level":"info","event":"25/12/19 13:46:10 INFO TaskSetManager: Starting task 13.0 in stage 10.0 (TID 59) (datanode-nodemanager-1, executor 2, partition 13, NODE_LOCAL, 13862 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:10.160246Z","level":"info","event":"25/12/19 13:46:10 INFO TaskSetManager: Finished task 11.0 in stage 10.0 (TID 57) in 551 ms on datanode-nodemanager-1 (executor 2) (12/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:10.353509Z","level":"info","event":"25/12/19 13:46:10 INFO TaskSetManager: Finished task 12.0 in stage 10.0 (TID 58) in 515 ms on datanode-nodemanager-2 (executor 1) (13/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:10.671587Z","level":"info","event":"25/12/19 13:46:10 INFO TaskSetManager: Finished task 13.0 in stage 10.0 (TID 59) in 514 ms on datanode-nodemanager-1 (executor 2) (14/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:10.671798Z","level":"info","event":"25/12/19 13:46:10 INFO YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:10.672276Z","level":"info","event":"25/12/19 13:46:10 INFO DAGScheduler: ShuffleMapStage 10 (jdbc at NativeMethodAccessorImpl.java:0) finished in 4.041 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:10.672405Z","level":"info","event":"25/12/19 13:46:10 INFO DAGScheduler: looking for newly runnable stages","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:10.672514Z","level":"info","event":"25/12/19 13:46:10 INFO DAGScheduler: running: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:10.672596Z","level":"info","event":"25/12/19 13:46:10 INFO DAGScheduler: waiting: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:10.672652Z","level":"info","event":"25/12/19 13:46:10 INFO DAGScheduler: failed: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:10.678497Z","level":"info","event":"25/12/19 13:46:10 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:10.853007Z","level":"info","event":"25/12/19 13:46:10 INFO CodeGenerator: Code generated in 138.392368 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:10.914169Z","level":"info","event":"25/12/19 13:46:10 INFO BlockManagerInfo: Removed broadcast_11_piece0 on datanode-nodemanager-2:36751 in memory (size: 16.6 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:10.917615Z","level":"info","event":"25/12/19 13:46:10 INFO BlockManagerInfo: Removed broadcast_11_piece0 on datanode-nodemanager-1:35737 in memory (size: 16.6 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:10.921876Z","level":"info","event":"25/12/19 13:46:10 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 12d706d757b9:38241 in memory (size: 16.6 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:10.940674Z","level":"info","event":"25/12/19 13:46:10 INFO SparkContext: Starting job: jdbc at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:10.943051Z","level":"info","event":"25/12/19 13:46:10 INFO DAGScheduler: Got job 8 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:10.943307Z","level":"info","event":"25/12/19 13:46:10 INFO DAGScheduler: Final stage: ResultStage 12 (jdbc at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:10.943427Z","level":"info","event":"25/12/19 13:46:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:10.946086Z","level":"info","event":"25/12/19 13:46:10 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:10.956135Z","level":"info","event":"25/12/19 13:46:10 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[39] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:10.994032Z","level":"info","event":"25/12/19 13:46:10 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 54.3 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:11.018966Z","level":"info","event":"25/12/19 13:46:11 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 23.8 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:11.022146Z","level":"info","event":"25/12/19 13:46:11 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 12d706d757b9:38241 (size: 23.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:11.027303Z","level":"info","event":"25/12/19 13:46:11 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:11.033567Z","level":"info","event":"25/12/19 13:46:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[39] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:11.033827Z","level":"info","event":"25/12/19 13:46:11 INFO YarnScheduler: Adding task set 12.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:11.044138Z","level":"info","event":"25/12/19 13:46:11 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 60) (datanode-nodemanager-2, executor 1, partition 0, NODE_LOCAL, 9010 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:11.098672Z","level":"info","event":"25/12/19 13:46:11 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on datanode-nodemanager-2:36751 (size: 23.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:11.399081Z","level":"info","event":"25/12/19 13:46:11 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.18.0.3:50658","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.071978Z","level":"info","event":"25/12/19 13:46:13 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 60) in 2029 ms on datanode-nodemanager-2 (executor 1) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.072578Z","level":"info","event":"25/12/19 13:46:13 INFO YarnScheduler: Removed TaskSet 12.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.073297Z","level":"info","event":"25/12/19 13:46:13 INFO DAGScheduler: ResultStage 12 (jdbc at NativeMethodAccessorImpl.java:0) finished in 2.113 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.073742Z","level":"info","event":"25/12/19 13:46:13 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.073868Z","level":"info","event":"25/12/19 13:46:13 INFO YarnScheduler: Killing all running tasks in stage 12: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.074749Z","level":"info","event":"25/12/19 13:46:13 INFO DAGScheduler: Job 8 finished: jdbc at NativeMethodAccessorImpl.java:0, took 2.133511 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.320527Z","level":"info","event":"25/12/19 13:46:13 INFO DataSourceStrategy: Pruning directories with: isnotnull(ingest_date#21),(ingest_date#21 >= 2025-09-01),(ingest_date#21 < 2025-09-08)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.321658Z","level":"info","event":"25/12/19 13:46:13 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.321871Z","level":"info","event":"25/12/19 13:46:13 INFO FileSourceStrategy: Post-Scan Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.482194Z","level":"info","event":"25/12/19 13:46:13 INFO CodeGenerator: Code generated in 59.590809 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.492141Z","level":"info","event":"25/12/19 13:46:13 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 203.3 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.517742Z","level":"info","event":"25/12/19 13:46:13 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.522326Z","level":"info","event":"25/12/19 13:46:13 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 12d706d757b9:38241 (size: 36.1 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.522981Z","level":"info","event":"25/12/19 13:46:13 INFO BlockManagerInfo: Removed broadcast_10_piece0 on datanode-nodemanager-1:35737 in memory (size: 35.8 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.529644Z","level":"info","event":"25/12/19 13:46:13 INFO BlockManagerInfo: Removed broadcast_10_piece0 on datanode-nodemanager-2:36751 in memory (size: 35.8 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.530504Z","level":"info","event":"25/12/19 13:46:13 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 12d706d757b9:38241 in memory (size: 35.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.530630Z","level":"info","event":"25/12/19 13:46:13 INFO SparkContext: Created broadcast 13 from showString at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.535359Z","level":"info","event":"25/12/19 13:46:13 INFO InMemoryFileIndex: Selected 7 partitions out of 16, pruned 56.25% partitions.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.544101Z","level":"info","event":"25/12/19 13:46:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.566811Z","level":"info","event":"25/12/19 13:46:13 INFO BlockManagerInfo: Removed broadcast_12_piece0 on datanode-nodemanager-2:36751 in memory (size: 23.8 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.573922Z","level":"info","event":"25/12/19 13:46:13 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 12d706d757b9:38241 in memory (size: 23.8 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.593422Z","level":"info","event":"25/12/19 13:46:13 INFO DAGScheduler: Registering RDD 44 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 4","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.593966Z","level":"info","event":"25/12/19 13:46:13 INFO DAGScheduler: Got map stage job 9 (showString at NativeMethodAccessorImpl.java:0) with 14 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.594086Z","level":"info","event":"25/12/19 13:46:13 INFO DAGScheduler: Final stage: ShuffleMapStage 13 (showString at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.594144Z","level":"info","event":"25/12/19 13:46:13 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.632556Z","level":"info","event":"25/12/19 13:46:13 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.642468Z","level":"info","event":"25/12/19 13:46:13 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[44] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.703008Z","level":"info","event":"25/12/19 13:46:13 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 40.4 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.725459Z","level":"info","event":"25/12/19 13:46:13 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 17.1 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.727664Z","level":"info","event":"25/12/19 13:46:13 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 12d706d757b9:38241 (size: 17.1 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.730415Z","level":"info","event":"25/12/19 13:46:13 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.730606Z","level":"info","event":"25/12/19 13:46:13 INFO DAGScheduler: Submitting 14 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[44] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.730826Z","level":"info","event":"25/12/19 13:46:13 INFO YarnScheduler: Adding task set 13.0 with 14 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.734420Z","level":"info","event":"25/12/19 13:46:13 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 61) (datanode-nodemanager-2, executor 1, partition 0, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.735626Z","level":"info","event":"25/12/19 13:46:13 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 62) (datanode-nodemanager-1, executor 2, partition 1, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.775774Z","level":"info","event":"25/12/19 13:46:13 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on datanode-nodemanager-1:35737 (size: 17.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:13.808482Z","level":"info","event":"25/12/19 13:46:13 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on datanode-nodemanager-2:36751 (size: 17.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:14.098958Z","level":"info","event":"25/12/19 13:46:14 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on datanode-nodemanager-1:35737 (size: 36.1 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:14.143575Z","level":"info","event":"25/12/19 13:46:14 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on datanode-nodemanager-2:36751 (size: 36.1 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:15.282158Z","level":"info","event":"25/12/19 13:46:15 INFO TaskSetManager: Starting task 2.0 in stage 13.0 (TID 63) (datanode-nodemanager-2, executor 1, partition 2, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:15.282383Z","level":"info","event":"25/12/19 13:46:15 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 61) in 1547 ms on datanode-nodemanager-2 (executor 1) (1/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:15.306772Z","level":"info","event":"25/12/19 13:46:15 INFO TaskSetManager: Starting task 3.0 in stage 13.0 (TID 64) (datanode-nodemanager-1, executor 2, partition 3, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:15.309866Z","level":"info","event":"25/12/19 13:46:15 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 62) in 1575 ms on datanode-nodemanager-1 (executor 2) (2/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:15.799153Z","level":"info","event":"25/12/19 13:46:15 INFO TaskSetManager: Starting task 4.0 in stage 13.0 (TID 65) (datanode-nodemanager-1, executor 2, partition 4, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:15.799357Z","level":"info","event":"25/12/19 13:46:15 INFO TaskSetManager: Finished task 3.0 in stage 13.0 (TID 64) in 494 ms on datanode-nodemanager-1 (executor 2) (3/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:15.801705Z","level":"info","event":"25/12/19 13:46:15 INFO TaskSetManager: Starting task 5.0 in stage 13.0 (TID 66) (datanode-nodemanager-2, executor 1, partition 5, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:15.803759Z","level":"info","event":"25/12/19 13:46:15 INFO TaskSetManager: Finished task 2.0 in stage 13.0 (TID 63) in 526 ms on datanode-nodemanager-2 (executor 1) (4/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:16.269575Z","level":"info","event":"25/12/19 13:46:16 INFO TaskSetManager: Starting task 6.0 in stage 13.0 (TID 67) (datanode-nodemanager-2, executor 1, partition 6, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:16.270612Z","level":"info","event":"25/12/19 13:46:16 INFO TaskSetManager: Finished task 5.0 in stage 13.0 (TID 66) in 469 ms on datanode-nodemanager-2 (executor 1) (5/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:16.291882Z","level":"info","event":"25/12/19 13:46:16 INFO TaskSetManager: Starting task 7.0 in stage 13.0 (TID 68) (datanode-nodemanager-1, executor 2, partition 7, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:16.293056Z","level":"info","event":"25/12/19 13:46:16 INFO TaskSetManager: Finished task 4.0 in stage 13.0 (TID 65) in 495 ms on datanode-nodemanager-1 (executor 2) (6/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:16.824145Z","level":"info","event":"25/12/19 13:46:16 INFO TaskSetManager: Starting task 8.0 in stage 13.0 (TID 69) (datanode-nodemanager-2, executor 1, partition 8, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:16.824350Z","level":"info","event":"25/12/19 13:46:16 INFO TaskSetManager: Finished task 6.0 in stage 13.0 (TID 67) in 555 ms on datanode-nodemanager-2 (executor 1) (7/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:16.835641Z","level":"info","event":"25/12/19 13:46:16 INFO TaskSetManager: Starting task 9.0 in stage 13.0 (TID 70) (datanode-nodemanager-1, executor 2, partition 9, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:16.848856Z","level":"info","event":"25/12/19 13:46:16 INFO TaskSetManager: Finished task 7.0 in stage 13.0 (TID 68) in 546 ms on datanode-nodemanager-1 (executor 2) (8/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:17.611320Z","level":"info","event":"25/12/19 13:46:17 INFO TaskSetManager: Starting task 10.0 in stage 13.0 (TID 71) (datanode-nodemanager-2, executor 1, partition 10, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:17.612085Z","level":"info","event":"25/12/19 13:46:17 INFO TaskSetManager: Finished task 8.0 in stage 13.0 (TID 69) in 789 ms on datanode-nodemanager-2 (executor 1) (9/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:17.647491Z","level":"info","event":"25/12/19 13:46:17 INFO TaskSetManager: Starting task 11.0 in stage 13.0 (TID 72) (datanode-nodemanager-1, executor 2, partition 11, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:17.647836Z","level":"info","event":"25/12/19 13:46:17 INFO TaskSetManager: Finished task 9.0 in stage 13.0 (TID 70) in 813 ms on datanode-nodemanager-1 (executor 2) (10/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.134882Z","level":"info","event":"25/12/19 13:46:18 INFO TaskSetManager: Starting task 12.0 in stage 13.0 (TID 73) (datanode-nodemanager-1, executor 2, partition 12, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.138213Z","level":"info","event":"25/12/19 13:46:18 INFO TaskSetManager: Finished task 11.0 in stage 13.0 (TID 72) in 490 ms on datanode-nodemanager-1 (executor 2) (11/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.165729Z","level":"info","event":"25/12/19 13:46:18 INFO TaskSetManager: Starting task 13.0 in stage 13.0 (TID 74) (datanode-nodemanager-2, executor 1, partition 13, NODE_LOCAL, 13862 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.173268Z","level":"info","event":"25/12/19 13:46:18 INFO TaskSetManager: Finished task 10.0 in stage 13.0 (TID 71) in 562 ms on datanode-nodemanager-2 (executor 1) (12/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.503623Z","level":"info","event":"25/12/19 13:46:18 INFO TaskSetManager: Finished task 13.0 in stage 13.0 (TID 74) in 339 ms on datanode-nodemanager-2 (executor 1) (13/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.519404Z","level":"info","event":"25/12/19 13:46:18 INFO TaskSetManager: Finished task 12.0 in stage 13.0 (TID 73) in 386 ms on datanode-nodemanager-1 (executor 2) (14/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.519663Z","level":"info","event":"25/12/19 13:46:18 INFO YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.520566Z","level":"info","event":"25/12/19 13:46:18 INFO DAGScheduler: ShuffleMapStage 13 (showString at NativeMethodAccessorImpl.java:0) finished in 4.878 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.520730Z","level":"info","event":"25/12/19 13:46:18 INFO DAGScheduler: looking for newly runnable stages","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.520795Z","level":"info","event":"25/12/19 13:46:18 INFO DAGScheduler: running: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.520853Z","level":"info","event":"25/12/19 13:46:18 INFO DAGScheduler: waiting: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.520902Z","level":"info","event":"25/12/19 13:46:18 INFO DAGScheduler: failed: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.527384Z","level":"info","event":"25/12/19 13:46:18 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.565762Z","level":"info","event":"25/12/19 13:46:18 INFO CodeGenerator: Code generated in 21.511536 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.601381Z","level":"info","event":"25/12/19 13:46:18 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.603228Z","level":"info","event":"25/12/19 13:46:18 INFO DAGScheduler: Got job 10 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.603472Z","level":"info","event":"25/12/19 13:46:18 INFO DAGScheduler: Final stage: ResultStage 15 (showString at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.603543Z","level":"info","event":"25/12/19 13:46:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.612230Z","level":"info","event":"25/12/19 13:46:18 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.614155Z","level":"info","event":"25/12/19 13:46:18 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[48] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.653835Z","level":"info","event":"25/12/19 13:46:18 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 46.5 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.684876Z","level":"info","event":"25/12/19 13:46:18 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 434.0 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.685102Z","level":"info","event":"25/12/19 13:46:18 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 12d706d757b9:38241 (size: 19.9 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.687891Z","level":"info","event":"25/12/19 13:46:18 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.688160Z","level":"info","event":"25/12/19 13:46:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[48] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.688241Z","level":"info","event":"25/12/19 13:46:18 INFO YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.700196Z","level":"info","event":"25/12/19 13:46:18 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 75) (datanode-nodemanager-1, executor 2, partition 0, NODE_LOCAL, 9010 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.700385Z","level":"info","event":"25/12/19 13:46:18 INFO BlockManagerInfo: Removed broadcast_14_piece0 on datanode-nodemanager-2:36751 in memory (size: 17.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.704795Z","level":"info","event":"25/12/19 13:46:18 INFO BlockManagerInfo: Removed broadcast_14_piece0 on datanode-nodemanager-1:35737 in memory (size: 17.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.715116Z","level":"info","event":"25/12/19 13:46:18 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 12d706d757b9:38241 in memory (size: 17.1 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.730670Z","level":"info","event":"25/12/19 13:46:18 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on datanode-nodemanager-1:35737 (size: 19.9 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:18.744870Z","level":"info","event":"25/12/19 13:46:18 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 172.18.0.4:50440","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.166190Z","level":"info","event":"25/12/19 13:46:19 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 75) in 474 ms on datanode-nodemanager-1 (executor 2) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.166511Z","level":"info","event":"25/12/19 13:46:19 INFO YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.172739Z","level":"info","event":"25/12/19 13:46:19 INFO DAGScheduler: ResultStage 15 (showString at NativeMethodAccessorImpl.java:0) finished in 0.540 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.172944Z","level":"info","event":"25/12/19 13:46:19 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.173002Z","level":"info","event":"25/12/19 13:46:19 INFO YarnScheduler: Killing all running tasks in stage 15: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.176172Z","level":"info","event":"25/12/19 13:46:19 INFO DAGScheduler: Job 10 finished: showString at NativeMethodAccessorImpl.java:0, took 0.573820 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.217728Z","level":"info","event":"25/12/19 13:46:19 INFO CodeGenerator: Code generated in 30.19139 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.228074Z","level":"info","event":"+--------------------+--------------------+------------------+--------------------+--------------------+---------------------------+----------------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.228419Z","level":"info","event":"|             news_id|        news_time_id|news_overall_score|          news_title|        news_summary|news_category_within_source|     news_source|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.228521Z","level":"info","event":"+--------------------+--------------------+------------------+--------------------+--------------------+---------------------------+----------------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.228568Z","level":"info","event":"|-9220512935994728299| 6367420500199088588|          0.186512|Cerence AI Files ...|BURLINGTON, Mass....|                       News|        Benzinga|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.228607Z","level":"info","event":"|-9215974233538529989| 2869404304239412763|          0.197429|Macquarie Asset M...|Dow has received ...|                       News|        Benzinga|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.228645Z","level":"info","event":"|-9215253552835217895|-8675524983303160171|           0.30086|Brady Corporation...|MILWAUKEE, Sept. ...|                       News|        Benzinga|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.228686Z","level":"info","event":"|-9214964412323901468|-1861558615695025446|          0.401813|Independent Bank ...|Dividends are one...|                           |Zacks Commentary|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.228724Z","level":"info","event":"|-9213791324328842980|-5757472051673998136|          0.247479|4 Brilliant Stock...|The AI arms race ...|                        n/a|     Motley Fool|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.228761Z","level":"info","event":"|-9211392508655787880|-7039825263568189942|          0.026307|91% of Jensen Hua...|Nvidia uses firm ...|                           |     Motley Fool|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.228802Z","level":"info","event":"|-9206036968928510820| 2869404304239412763|          0.166119|After Plunging 14...|Torrid Holdings (...|                        n/a|Zacks Commentary|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.229919Z","level":"info","event":"|-9205659208143189960|-8675524983303160171|         -0.081615|Alto Neuroscience...|Investors can con...|                       News|        Benzinga|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.230183Z","level":"info","event":"|-9202385578066064236| -222147908674804431|          0.436498|Welnax NeuroRelie...|New York, Sept. 0...|                           |        Benzinga|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.230385Z","level":"info","event":"|-9191398791713679453|-8675524983303160171|          0.396383|NCS Multistage  (...|NCS Multistage (N...|                        n/a|Zacks Commentary|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.230515Z","level":"info","event":"+--------------------+--------------------+------------------+--------------------+--------------------+---------------------------+----------------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.230583Z","level":"info","event":"only showing top 10 rows","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.230715Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.475434Z","level":"info","event":"25/12/19 13:46:19 INFO DataSourceStrategy: Pruning directories with: isnotnull(ingest_date#21),(ingest_date#21 >= 2025-09-01),(ingest_date#21 < 2025-09-08)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.476777Z","level":"info","event":"25/12/19 13:46:19 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.477293Z","level":"info","event":"25/12/19 13:46:19 INFO FileSourceStrategy: Post-Scan Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.552482Z","level":"info","event":"25/12/19 13:46:19 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 203.3 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.580918Z","level":"info","event":"25/12/19 13:46:19 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 12d706d757b9:38241 in memory (size: 19.9 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.581174Z","level":"info","event":"25/12/19 13:46:19 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.582290Z","level":"info","event":"25/12/19 13:46:19 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 12d706d757b9:38241 (size: 36.1 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.583246Z","level":"info","event":"25/12/19 13:46:19 INFO BlockManagerInfo: Removed broadcast_15_piece0 on datanode-nodemanager-1:35737 in memory (size: 19.9 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.583580Z","level":"info","event":"25/12/19 13:46:19 INFO SparkContext: Created broadcast 16 from jdbc at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.586164Z","level":"info","event":"25/12/19 13:46:19 INFO InMemoryFileIndex: Selected 7 partitions out of 16, pruned 56.25% partitions.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.587684Z","level":"info","event":"25/12/19 13:46:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.597255Z","level":"info","event":"25/12/19 13:46:19 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 12d706d757b9:38241 in memory (size: 36.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.597873Z","level":"info","event":"25/12/19 13:46:19 INFO BlockManagerInfo: Removed broadcast_13_piece0 on datanode-nodemanager-1:35737 in memory (size: 36.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.612908Z","level":"info","event":"25/12/19 13:46:19 INFO BlockManagerInfo: Removed broadcast_13_piece0 on datanode-nodemanager-2:36751 in memory (size: 36.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.646236Z","level":"info","event":"25/12/19 13:46:19 INFO DAGScheduler: Registering RDD 53 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 5","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.646475Z","level":"info","event":"25/12/19 13:46:19 INFO DAGScheduler: Got map stage job 11 (jdbc at NativeMethodAccessorImpl.java:0) with 14 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.646569Z","level":"info","event":"25/12/19 13:46:19 INFO DAGScheduler: Final stage: ShuffleMapStage 16 (jdbc at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.646625Z","level":"info","event":"25/12/19 13:46:19 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.646685Z","level":"info","event":"25/12/19 13:46:19 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.656429Z","level":"info","event":"25/12/19 13:46:19 INFO DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[53] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.728283Z","level":"info","event":"25/12/19 13:46:19 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 40.4 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.731238Z","level":"info","event":"25/12/19 13:46:19 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.733306Z","level":"info","event":"25/12/19 13:46:19 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 12d706d757b9:38241 (size: 17.0 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.737039Z","level":"info","event":"25/12/19 13:46:19 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.737364Z","level":"info","event":"25/12/19 13:46:19 INFO DAGScheduler: Submitting 14 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[53] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.737614Z","level":"info","event":"25/12/19 13:46:19 INFO YarnScheduler: Adding task set 16.0 with 14 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.744628Z","level":"info","event":"25/12/19 13:46:19 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 76) (datanode-nodemanager-2, executor 1, partition 0, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.745406Z","level":"info","event":"25/12/19 13:46:19 INFO TaskSetManager: Starting task 1.0 in stage 16.0 (TID 77) (datanode-nodemanager-1, executor 2, partition 1, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.773719Z","level":"info","event":"25/12/19 13:46:19 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on datanode-nodemanager-2:36751 (size: 17.0 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.785122Z","level":"info","event":"25/12/19 13:46:19 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on datanode-nodemanager-1:35737 (size: 17.0 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.816671Z","level":"info","event":"25/12/19 13:46:19 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on datanode-nodemanager-2:36751 (size: 36.1 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:19.838074Z","level":"info","event":"25/12/19 13:46:19 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on datanode-nodemanager-1:35737 (size: 36.1 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:20.770132Z","level":"info","event":"25/12/19 13:46:20 INFO TaskSetManager: Starting task 2.0 in stage 16.0 (TID 78) (datanode-nodemanager-2, executor 1, partition 2, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:20.771493Z","level":"info","event":"25/12/19 13:46:20 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 76) in 1028 ms on datanode-nodemanager-2 (executor 1) (1/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:20.797146Z","level":"info","event":"25/12/19 13:46:20 INFO TaskSetManager: Starting task 3.0 in stage 16.0 (TID 79) (datanode-nodemanager-1, executor 2, partition 3, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:20.797500Z","level":"info","event":"25/12/19 13:46:20 INFO TaskSetManager: Finished task 1.0 in stage 16.0 (TID 77) in 1052 ms on datanode-nodemanager-1 (executor 2) (2/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:21.317403Z","level":"info","event":"25/12/19 13:46:21 INFO TaskSetManager: Starting task 4.0 in stage 16.0 (TID 80) (datanode-nodemanager-2, executor 1, partition 4, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:21.318146Z","level":"info","event":"25/12/19 13:46:21 INFO TaskSetManager: Finished task 2.0 in stage 16.0 (TID 78) in 557 ms on datanode-nodemanager-2 (executor 1) (3/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:21.320141Z","level":"info","event":"25/12/19 13:46:21 INFO TaskSetManager: Starting task 5.0 in stage 16.0 (TID 81) (datanode-nodemanager-1, executor 2, partition 5, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:21.321403Z","level":"info","event":"25/12/19 13:46:21 INFO TaskSetManager: Finished task 3.0 in stage 16.0 (TID 79) in 526 ms on datanode-nodemanager-1 (executor 2) (4/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:21.788205Z","level":"info","event":"25/12/19 13:46:21 INFO TaskSetManager: Starting task 6.0 in stage 16.0 (TID 82) (datanode-nodemanager-2, executor 1, partition 6, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:21.790621Z","level":"info","event":"25/12/19 13:46:21 INFO TaskSetManager: Finished task 4.0 in stage 16.0 (TID 80) in 474 ms on datanode-nodemanager-2 (executor 1) (5/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:21.807342Z","level":"info","event":"25/12/19 13:46:21 INFO TaskSetManager: Starting task 7.0 in stage 16.0 (TID 83) (datanode-nodemanager-1, executor 2, partition 7, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:21.809929Z","level":"info","event":"25/12/19 13:46:21 INFO TaskSetManager: Finished task 5.0 in stage 16.0 (TID 81) in 490 ms on datanode-nodemanager-1 (executor 2) (6/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:22.242141Z","level":"info","event":"25/12/19 13:46:22 INFO TaskSetManager: Starting task 8.0 in stage 16.0 (TID 84) (datanode-nodemanager-2, executor 1, partition 8, NODE_LOCAL, 14324 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:22.242963Z","level":"info","event":"25/12/19 13:46:22 INFO TaskSetManager: Finished task 6.0 in stage 16.0 (TID 82) in 455 ms on datanode-nodemanager-2 (executor 1) (7/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:22.278954Z","level":"info","event":"25/12/19 13:46:22 INFO TaskSetManager: Starting task 9.0 in stage 16.0 (TID 85) (datanode-nodemanager-1, executor 2, partition 9, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:22.282799Z","level":"info","event":"25/12/19 13:46:22 INFO TaskSetManager: Finished task 7.0 in stage 16.0 (TID 83) in 471 ms on datanode-nodemanager-1 (executor 2) (8/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:22.682453Z","level":"info","event":"25/12/19 13:46:22 INFO TaskSetManager: Starting task 10.0 in stage 16.0 (TID 86) (datanode-nodemanager-2, executor 1, partition 10, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:22.683649Z","level":"info","event":"25/12/19 13:46:22 INFO TaskSetManager: Finished task 8.0 in stage 16.0 (TID 84) in 442 ms on datanode-nodemanager-2 (executor 1) (9/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:22.716900Z","level":"info","event":"25/12/19 13:46:22 INFO TaskSetManager: Starting task 11.0 in stage 16.0 (TID 87) (datanode-nodemanager-1, executor 2, partition 11, NODE_LOCAL, 14345 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:22.717212Z","level":"info","event":"25/12/19 13:46:22 INFO TaskSetManager: Finished task 9.0 in stage 16.0 (TID 85) in 441 ms on datanode-nodemanager-1 (executor 2) (10/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.089784Z","level":"info","event":"25/12/19 13:46:23 INFO TaskSetManager: Starting task 12.0 in stage 16.0 (TID 88) (datanode-nodemanager-2, executor 1, partition 12, NODE_LOCAL, 14303 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.090417Z","level":"info","event":"25/12/19 13:46:23 INFO TaskSetManager: Finished task 10.0 in stage 16.0 (TID 86) in 409 ms on datanode-nodemanager-2 (executor 1) (11/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.125918Z","level":"info","event":"25/12/19 13:46:23 INFO TaskSetManager: Starting task 13.0 in stage 16.0 (TID 89) (datanode-nodemanager-1, executor 2, partition 13, NODE_LOCAL, 13862 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.126500Z","level":"info","event":"25/12/19 13:46:23 INFO TaskSetManager: Finished task 11.0 in stage 16.0 (TID 87) in 412 ms on datanode-nodemanager-1 (executor 2) (12/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.514285Z","level":"info","event":"25/12/19 13:46:23 INFO TaskSetManager: Finished task 12.0 in stage 16.0 (TID 88) in 424 ms on datanode-nodemanager-2 (executor 1) (13/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.544779Z","level":"info","event":"25/12/19 13:46:23 INFO TaskSetManager: Finished task 13.0 in stage 16.0 (TID 89) in 417 ms on datanode-nodemanager-1 (executor 2) (14/14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.545052Z","level":"info","event":"25/12/19 13:46:23 INFO YarnScheduler: Removed TaskSet 16.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.545205Z","level":"info","event":"25/12/19 13:46:23 INFO DAGScheduler: ShuffleMapStage 16 (jdbc at NativeMethodAccessorImpl.java:0) finished in 3.862 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.545277Z","level":"info","event":"25/12/19 13:46:23 INFO DAGScheduler: looking for newly runnable stages","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.545334Z","level":"info","event":"25/12/19 13:46:23 INFO DAGScheduler: running: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.547071Z","level":"info","event":"25/12/19 13:46:23 INFO DAGScheduler: waiting: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.547229Z","level":"info","event":"25/12/19 13:46:23 INFO DAGScheduler: failed: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.555510Z","level":"info","event":"25/12/19 13:46:23 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.678377Z","level":"info","event":"25/12/19 13:46:23 INFO SparkContext: Starting job: jdbc at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.680062Z","level":"info","event":"25/12/19 13:46:23 INFO DAGScheduler: Got job 12 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.680289Z","level":"info","event":"25/12/19 13:46:23 INFO DAGScheduler: Final stage: ResultStage 18 (jdbc at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.680461Z","level":"info","event":"25/12/19 13:46:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.680518Z","level":"info","event":"25/12/19 13:46:23 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.682034Z","level":"info","event":"25/12/19 13:46:23 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[59] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.709271Z","level":"info","event":"25/12/19 13:46:23 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 54.7 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.721913Z","level":"info","event":"25/12/19 13:46:23 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 23.6 KiB, free 434.0 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.723387Z","level":"info","event":"25/12/19 13:46:23 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 12d706d757b9:38241 (size: 23.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.727385Z","level":"info","event":"25/12/19 13:46:23 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1611","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.727632Z","level":"info","event":"25/12/19 13:46:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[59] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.727716Z","level":"info","event":"25/12/19 13:46:23 INFO YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.728994Z","level":"info","event":"25/12/19 13:46:23 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 90) (datanode-nodemanager-2, executor 1, partition 0, NODE_LOCAL, 9010 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.732800Z","level":"info","event":"25/12/19 13:46:23 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 12d706d757b9:38241 in memory (size: 17.0 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.734906Z","level":"info","event":"25/12/19 13:46:23 INFO BlockManagerInfo: Removed broadcast_17_piece0 on datanode-nodemanager-2:36751 in memory (size: 17.0 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.743310Z","level":"info","event":"25/12/19 13:46:23 INFO BlockManagerInfo: Removed broadcast_17_piece0 on datanode-nodemanager-1:35737 in memory (size: 17.0 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.755155Z","level":"info","event":"25/12/19 13:46:23 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on datanode-nodemanager-2:36751 (size: 23.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:23.782421Z","level":"info","event":"25/12/19 13:46:23 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 172.18.0.3:50658","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:26.521408Z","level":"info","event":"25/12/19 13:46:26 WARN TaskSetManager: Lost task 0.0 in stage 18.0 (TID 90) (datanode-nodemanager-2 executor 1): java.sql.BatchUpdateException: Violation of PRIMARY KEY constraint 'PK__dim_news__4C27CCD80BD94A5D'. Cannot insert duplicate key in object 'dbo.dim_news'. The duplicate key value is (-9220512935994728299).","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:26.521705Z","level":"info","event":"at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2187)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:26.521899Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:26.521981Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:26.522033Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:26.522133Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:26.522208Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:26.522258Z","level":"info","event":"at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:26.522306Z","level":"info","event":"at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:26.522356Z","level":"info","event":"at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:26.522406Z","level":"info","event":"at org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:26.522498Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:26.522608Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:26.522669Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:26.522717Z","level":"info","event":"at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:26.522767Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:26.522815Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:26.522872Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:26.523070Z","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:834)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:26.523157Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:26.526437Z","level":"info","event":"25/12/19 13:46:26 INFO TaskSetManager: Starting task 0.1 in stage 18.0 (TID 91) (datanode-nodemanager-1, executor 2, partition 0, NODE_LOCAL, 9010 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:26.546243Z","level":"info","event":"25/12/19 13:46:26 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on datanode-nodemanager-1:35737 (size: 23.6 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:26.564163Z","level":"info","event":"25/12/19 13:46:26 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 172.18.0.4:50440","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:27.424533Z","level":"info","event":"25/12/19 13:46:27 INFO TaskSetManager: Lost task 0.1 in stage 18.0 (TID 91) on datanode-nodemanager-1, executor 2: java.sql.BatchUpdateException (Violation of PRIMARY KEY constraint 'PK__dim_news__4C27CCD80BD94A5D'. Cannot insert duplicate key in object 'dbo.dim_news'. The duplicate key value is (-9220512935994728299).) [duplicate 1]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:27.427756Z","level":"info","event":"25/12/19 13:46:27 INFO TaskSetManager: Starting task 0.2 in stage 18.0 (TID 92) (datanode-nodemanager-2, executor 1, partition 0, NODE_LOCAL, 9010 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.007554Z","level":"info","event":"25/12/19 13:46:28 INFO TaskSetManager: Lost task 0.2 in stage 18.0 (TID 92) on datanode-nodemanager-2, executor 1: java.sql.BatchUpdateException (Violation of PRIMARY KEY constraint 'PK__dim_news__4C27CCD80BD94A5D'. Cannot insert duplicate key in object 'dbo.dim_news'. The duplicate key value is (-9220512935994728299).) [duplicate 2]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.010295Z","level":"info","event":"25/12/19 13:46:28 INFO TaskSetManager: Starting task 0.3 in stage 18.0 (TID 93) (datanode-nodemanager-1, executor 2, partition 0, NODE_LOCAL, 9010 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.560821Z","level":"info","event":"25/12/19 13:46:28 INFO TaskSetManager: Lost task 0.3 in stage 18.0 (TID 93) on datanode-nodemanager-1, executor 2: java.sql.BatchUpdateException (Violation of PRIMARY KEY constraint 'PK__dim_news__4C27CCD80BD94A5D'. Cannot insert duplicate key in object 'dbo.dim_news'. The duplicate key value is (-9220512935994728299).) [duplicate 3]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.562916Z","level":"info","event":"25/12/19 13:46:28 ERROR TaskSetManager: Task 0 in stage 18.0 failed 4 times; aborting job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.565886Z","level":"info","event":"25/12/19 13:46:28 INFO YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.569558Z","level":"info","event":"25/12/19 13:46:28 INFO YarnScheduler: Cancelling stage 18","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.569728Z","level":"info","event":"25/12/19 13:46:28 INFO YarnScheduler: Killing all running tasks in stage 18: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 18.0 failed 4 times, most recent failure: Lost task 0.3 in stage 18.0 (TID 93) (datanode-nodemanager-1 executor 2): java.sql.BatchUpdateException: Violation of PRIMARY KEY constraint 'PK__dim_news__4C27CCD80BD94A5D'. Cannot insert duplicate key in object 'dbo.dim_news'. The duplicate key value is (-9220512935994728299).","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.569775Z","level":"info","event":"at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2187)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.569803Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.569829Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.569854Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.569876Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.569900Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.569923Z","level":"info","event":"at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.569945Z","level":"info","event":"at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.569966Z","level":"info","event":"at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.569988Z","level":"info","event":"at org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.570011Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.570034Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.570058Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.570080Z","level":"info","event":"at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.570103Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.570124Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.570146Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.570170Z","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:834)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.570206Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.570243Z","level":"info","event":"Driver stacktrace:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.570960Z","level":"info","event":"25/12/19 13:46:28 INFO DAGScheduler: ResultStage 18 (jdbc at NativeMethodAccessorImpl.java:0) failed in 4.879 s due to Job aborted due to stage failure: Task 0 in stage 18.0 failed 4 times, most recent failure: Lost task 0.3 in stage 18.0 (TID 93) (datanode-nodemanager-1 executor 2): java.sql.BatchUpdateException: Violation of PRIMARY KEY constraint 'PK__dim_news__4C27CCD80BD94A5D'. Cannot insert duplicate key in object 'dbo.dim_news'. The duplicate key value is (-9220512935994728299).","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.571109Z","level":"info","event":"at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2187)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.571166Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.571209Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.571255Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.571300Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.571345Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.571391Z","level":"info","event":"at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.571470Z","level":"info","event":"at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.571548Z","level":"info","event":"at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.571609Z","level":"info","event":"at org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.571662Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.571711Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.571784Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.571827Z","level":"info","event":"at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.571855Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.571901Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.571930Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.571954Z","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:834)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.572051Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.572101Z","level":"info","event":"Driver stacktrace:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:28.575208Z","level":"info","event":"25/12/19 13:46:28 INFO DAGScheduler: Job 12 failed: jdbc at NativeMethodAccessorImpl.java:0, took 4.896506 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.028968Z","level":"info","event":"Traceback (most recent call last):","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.029145Z","level":"info","event":"File \"/workspace/airflow/spark-jobs/update_news_data.py\", line 73, in <module>","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.033077Z","level":"info","event":"dim_news.write.jdbc(","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.033286Z","level":"info","event":"File \"/workspace/airflow/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 1984, in jdbc","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.040704Z","level":"info","event":"File \"/workspace/airflow/.venv/lib/python3.12/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.046655Z","level":"info","event":"File \"/workspace/airflow/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.052449Z","level":"info","event":"File \"/workspace/airflow/.venv/lib/python3.12/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.075248Z","level":"info","event":"py4j.protocol.Py4JJavaError25/12/19 13:46:29 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 12d706d757b9:38241 in memory (size: 36.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.076721Z","level":"info","event":"25/12/19 13:46:29 INFO BlockManagerInfo: Removed broadcast_16_piece0 on datanode-nodemanager-1:35737 in memory (size: 36.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.078315Z","level":"info","event":"25/12/19 13:46:29 INFO BlockManagerInfo: Removed broadcast_16_piece0 on datanode-nodemanager-2:36751 in memory (size: 36.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.085369Z","level":"info","event":"25/12/19 13:46:29 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 12d706d757b9:38241 in memory (size: 23.6 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.087717Z","level":"info","event":"25/12/19 13:46:29 INFO BlockManagerInfo: Removed broadcast_18_piece0 on datanode-nodemanager-2:36751 in memory (size: 23.6 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.088903Z","level":"info","event":"25/12/19 13:46:29 INFO BlockManagerInfo: Removed broadcast_18_piece0 on datanode-nodemanager-1:35737 in memory (size: 23.6 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.112455Z","level":"info","event":": An error occurred while calling o124.jdbc.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.112659Z","level":"info","event":": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 4 times, most recent failure: Lost task 0.3 in stage 18.0 (TID 93) (datanode-nodemanager-1 executor 2): java.sql.BatchUpdateException: Violation of PRIMARY KEY constraint 'PK__dim_news__4C27CCD80BD94A5D'. Cannot insert duplicate key in object 'dbo.dim_news'. The duplicate key value is (-9220512935994728299).","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.112769Z","level":"info","event":"at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2187)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.112821Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.112869Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.112915Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.112959Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.113006Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.113051Z","level":"info","event":"at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.113098Z","level":"info","event":"at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.113139Z","level":"info","event":"at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.113184Z","level":"info","event":"at org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.113245Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.113293Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.113340Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.113387Z","level":"info","event":"at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.113431Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.113474Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.113518Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.113559Z","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:834)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.113604Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.113650Z","level":"info","event":"Driver stacktrace:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.113698Z","level":"info","event":"at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2898)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.113744Z","level":"info","event":"at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2834)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.113789Z","level":"info","event":"at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2833)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.113838Z","level":"info","event":"at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.113885Z","level":"info","event":"at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.113930Z","level":"info","event":"at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.113977Z","level":"info","event":"at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2833)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.114024Z","level":"info","event":"at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1253)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.114071Z","level":"info","event":"at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1253)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.114152Z","level":"info","event":"at scala.Option.foreach(Option.scala:407)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.114206Z","level":"info","event":"at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1253)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.114263Z","level":"info","event":"at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3102)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.114317Z","level":"info","event":"at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3036)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.114370Z","level":"info","event":"at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3025)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.114420Z","level":"info","event":"at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.114461Z","level":"info","event":"at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:995)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.114504Z","level":"info","event":"at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.114543Z","level":"info","event":"at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.114582Z","level":"info","event":"at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.114626Z","level":"info","event":"at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.114671Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.114712Z","level":"info","event":"at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.114752Z","level":"info","event":"at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.114788Z","level":"info","event":"at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.114828Z","level":"info","event":"at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.114865Z","level":"info","event":"at org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3516)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.114900Z","level":"info","event":"at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.114932Z","level":"info","event":"at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4310)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.114966Z","level":"info","event":"at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.115002Z","level":"info","event":"at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.115037Z","level":"info","event":"at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.115078Z","level":"info","event":"at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.115120Z","level":"info","event":"at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.115158Z","level":"info","event":"at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4308)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.115195Z","level":"info","event":"at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3516)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.115232Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:903)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.115270Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.115306Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.115342Z","level":"info","event":"at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.115389Z","level":"info","event":"at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.115426Z","level":"info","event":"at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.115465Z","level":"info","event":"at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.115493Z","level":"info","event":"at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.115515Z","level":"info","event":"at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.115537Z","level":"info","event":"at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.115559Z","level":"info","event":"at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.115591Z","level":"info","event":"at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.115665Z","level":"info","event":"at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.115730Z","level":"info","event":"at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.115773Z","level":"info","event":"at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.115831Z","level":"info","event":"at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.115879Z","level":"info","event":"at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.115932Z","level":"info","event":"at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.115989Z","level":"info","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.116852Z","level":"info","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.116901Z","level":"info","event":"at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.116951Z","level":"info","event":"at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.116997Z","level":"info","event":"at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.117042Z","level":"info","event":"at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.117090Z","level":"info","event":"at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.117142Z","level":"info","event":"at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.117194Z","level":"info","event":"at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.117237Z","level":"info","event":"at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.117278Z","level":"info","event":"at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.117318Z","level":"info","event":"at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.117363Z","level":"info","event":"at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.117405Z","level":"info","event":"at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:766)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.117445Z","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.117549Z","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.117604Z","level":"info","event":"at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.117656Z","level":"info","event":"at java.base/java.lang.reflect.Method.invoke(Method.java:566)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.117763Z","level":"info","event":"at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.117826Z","level":"info","event":"at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.117868Z","level":"info","event":"at py4j.Gateway.invoke(Gateway.java:282)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.117911Z","level":"info","event":"at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.117953Z","level":"info","event":"at py4j.commands.CallCommand.execute(CallCommand.java:79)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.117996Z","level":"info","event":"at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.118036Z","level":"info","event":"at py4j.ClientServerConnection.run(ClientServerConnection.java:106)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.118084Z","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:829)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.118129Z","level":"info","event":"Caused by: java.sql.BatchUpdateException: Violation of PRIMARY KEY constraint 'PK__dim_news__4C27CCD80BD94A5D'. Cannot insert duplicate key in object 'dbo.dim_news'. The duplicate key value is (-9220512935994728299).","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.118172Z","level":"info","event":"at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2187)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.118215Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.118257Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.118296Z","level":"info","event":"at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.118337Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.118379Z","level":"info","event":"at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.118421Z","level":"info","event":"at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.118468Z","level":"info","event":"at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.118509Z","level":"info","event":"at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.118548Z","level":"info","event":"at org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.118589Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.118637Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.118681Z","level":"info","event":"at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.118722Z","level":"info","event":"at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.118771Z","level":"info","event":"at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.118818Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.118864Z","level":"info","event":"at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.118901Z","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:834)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.118938Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.228267Z","level":"info","event":"25/12/19 13:46:29 INFO SparkContext: Invoking stop() from shutdown hook","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.228838Z","level":"info","event":"25/12/19 13:46:29 INFO SparkContext: SparkContext is stopping with exitCode 0.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.258253Z","level":"info","event":"25/12/19 13:46:29 INFO SparkUI: Stopped Spark web UI at http://12d706d757b9:4040","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.276596Z","level":"info","event":"25/12/19 13:46:29 INFO YarnClientSchedulerBackend: Interrupting monitor thread","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.302366Z","level":"info","event":"25/12/19 13:46:29 INFO YarnClientSchedulerBackend: Shutting down all executors","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.303582Z","level":"info","event":"25/12/19 13:46:29 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.315582Z","level":"info","event":"25/12/19 13:46:29 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.347160Z","level":"info","event":"25/12/19 13:46:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.414098Z","level":"info","event":"25/12/19 13:46:29 INFO MemoryStore: MemoryStore cleared","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.414449Z","level":"info","event":"25/12/19 13:46:29 INFO BlockManager: BlockManager stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.436105Z","level":"info","event":"25/12/19 13:46:29 INFO BlockManagerMaster: BlockManagerMaster stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.463411Z","level":"info","event":"25/12/19 13:46:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.518311Z","level":"info","event":"25/12/19 13:46:29 INFO SparkContext: Successfully stopped SparkContext","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.518527Z","level":"info","event":"25/12/19 13:46:29 INFO ShutdownHookManager: Shutdown hook called","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.518592Z","level":"info","event":"25/12/19 13:46:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-f763b0e9-1ff5-42a7-92b7-d4624ff74534","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.532401Z","level":"info","event":"25/12/19 13:46:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-1a89fce6-45d5-4b11-b100-68fc71097f04/pyspark-d976f152-e6ce-4f96-953b-7f4a18e67d41","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.539443Z","level":"info","event":"25/12/19 13:46:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-1a89fce6-45d5-4b11-b100-68fc71097f04","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-12-19T13:46:29.770360Z","level":"error","event":"Task failed with exception","logger":"task","filename":"task_runner.py","lineno":980,"error_detail":[{"exc_type":"AirflowException","exc_value":"Cannot execute: spark-submit --master yarn --jars /workspace/airflow/connector/mssql-jdbc-12.2.0.jre11.jar --name arrow-spark --verbose --deploy-mode client /workspace/airflow/spark-jobs/update_news_data.py 2025-09-07 2025-09-14. Error code is: 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/workspace/airflow/.venv/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":928,"name":"run"},{"filename":"/workspace/airflow/.venv/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1315,"name":"_execute_task"},{"filename":"/workspace/airflow/.venv/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":416,"name":"wrapper"},{"filename":"/workspace/airflow/.venv/lib/python3.12/site-packages/airflow/providers/apache/spark/operators/spark_submit.py","lineno":197,"name":"execute"},{"filename":"/workspace/airflow/.venv/lib/python3.12/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py","lineno":566,"name":"submit"}],"is_group":false,"exceptions":[]}]}
